__label__AI	"# Title:Integrating Artificial Intelligence into Weapon Systems(Submitted on 10 May 2019)> Abstract: The integration of Artificial Intelligence (AI) into weaponsystems is one of the most consequential tactical and strategic decisions inthe history of warfare. Current AI development is a remarkable combination ofaccelerating capability, hidden decision mechanisms, and decreasing costs.Implementation of these systems is in its infancy and exists on a spectrumfrom resilient and flexible to simplistic and brittle. Resilient systemsshould be able to effectively handle the complexities of a high-dimensionalbattlespace. Simplistic AI implementations could be manipulated by anadversarial AI that identifies and exploits their weaknesses. > In this paper, we present a framework for understanding the development ofdynamic AI/ML systems that interactively and continuously adapt to theiruser's needs. We explore the implications of increasingly capable AI in thekill chain and how this will lead inevitably to a fully automated, always onsystem, barring regulation by treaty. We examine the potential of totalintegration of cyber and physical security and how this likelihood must informthe development of AI-enabled systems with respect to the ""fog of war"", humanmorals, and ethics.## Submission historyFrom: Aaron Massey [[view email](/show-email/69a90b83/1905.03899)]**[v1]**Fri, 10 May 2019 00:38:35 UTC (3,958 KB)"
__label__nan	"<p>I'm trying to make the program return to the menu list if any character other than 1 - 11 is selected when prompted ""Please enter your Module Choice"" using a do while loop...</p><p>Currently even if the user doesn't select a valid option the program just continues to run</p><p>I expect after ""Please select a valid module"" for it to return to the menu list.</p><pre><code>Scanner scanner = new Scanner(System.in);</code></pre><p>public void moduleSelection() {</p><pre><code>System.out.println(""1\t Algorithms"");System.out.println(""2\t Advanced Programming"");System.out.println(""3\t Computer Architecture and Operating Systems"");System.out.println(""4\t Artificial intelligence and Machine Learning"");System.out.println(""5\t Computer and Mobile Networks"");System.out.println(""6\t Software Engineering"");System.out.println(""7\t Big Data Analyics"");System.out.println(""8\t Cyber Security Threats"");System.out.println(""9\t Research Methods"");System.out.println(""10\t Research Project Proposal"");System.out.println(""11\t Individual Research Project"");System.out.println(""Please entire your Module choice"");int choice;choice = scanner.nextInt();switch (choice){case 1: System.out.println(""Algorithms"");break;case 2: System.out.println(""Advanced Programming"");break;case 3: System.out.println(""Computer Architecture and Operating Systems"");break;case 4: System.out.println(""Artificial intelligence and Machine Learning"");break;case 5: System.out.println(""Computer and Mobile Networks"");break;case 6: System.out.println(""Software Engineering"");break;case 7: System.out.println(""Big Data Analytics"");break;case 8: System.out.println(""Cyber Security Threats"");break;case 9: System.out.println(""Research Methods"");break;case 10: System.out.println(""Research Project Proposal"");break;case 11: System.out.println(""Individual Research Project"");break;default: System.out.println(""Please select a valid Module"");break;}</code></pre><p>}</p>"
__label__nan	_âÈ_kotlinÕÎjetpackÝüÈ¤Ñ¨üÐ_Î¾÷øü¾÷øø´¾Ý«¾Ð¡üÛ¾_¢Android¾_Û¾Ïø¼ _Ù÷
__label__nan	![image.png](https://images.zenhubusercontent.com/5ab2c6f24b5806bc2bcbd667/1e6de5ec-662b-476e-ac43-9da64f16e81c)
__label__DevOps, Mobile Services	#### in android.view.ViewRootImpl.performTraversals* Number of crashes: 1* Impacted devices: 1There's a lot more information about this crash on crashlytics.com:[https://fabric.io/makeroid4/android/apps/io.makeroid.application/issues/5c769815f8b88c29634e3a48?utm_medium=service_hooks-github&utm_source=issue_impact](https://fabric.io/makeroid4/android/apps/io.makeroid.application/issues/5c769815f8b88c29634e3a48?utm_medium=service_hooks-github&utm_source=issue_impact)
__label__Customer Experience	"When the Publish button is disabled because there are no items to publish or user lacks publishing permissions, the disabled Publish button should be replaced with ""Create Issue"" (blue background, no dropdown menu). Note also that two different ways are used to disable the publish button on the screenshots below. ![image](https://user-images.githubusercontent.com/11612490/53478748-d4f73200-3a77-11e9-9f14-fd62e5241bbd.png)![image](https://user-images.githubusercontent.com/11612490/53481429-82b90f80-3a7d-11e9-9725-bc21d5a27675.png)**NB! Exception is the case when publishing is prevented by invalid items in the list - in this case Publish button should be visible but disabled (like now).**"
__label__AI	Natural language processing (NLP) is a field of computer science, artificialintelligence and computational linguistics concerned with the interactionsbetween computers and human (natural) languages, and, in particular, concernedwith programming computers to fruitfully process large natural languagecorpora.
__label__DevOps, Customer Experience	Currently if the `nht gtg [url]` command fails, it doesn't exit properly, it just gives the error:(node:220) UnhandledPromiseRejectionWarning: _Ù÷¢ http://ft-next-arti-remove-clu-yxgglg.herokuapp.com/__gtg did not respond with an ok response within two minutes.eg, https://circleci.com/gh/Financial-Times/next-article/16102---This is probably where changes need to be made: https://github.com/Financial-Times/n-heroku-tools/blob/master/tasks/gtg.js#L10Example of how exit is done in `nht`: https://github.com/Financial-Times/n-heroku-tools/blob/0ada2df0ff875a7f3fdc81a743359193557e4193/bin/n-heroku-tools.js#L13-L16Good time to add tests for the `gtg` command too _Ù÷ã
__label__nan	There must be a new contact created on HubSpot.
__label__nan	No petitions, surveys, or crowdfunding
__label__cloud	I meant creating w/ the use of Cloud Deployment Manager. I can create via Google Cloud Console but what I want is via Cloud Deployment Manager. I can not find anything in their documentation regarding this
__label__AI	# Artificial Intelligence will Enhance and Hack HumanityThe truth is, itÈs complicated and will become even more complicated. AI isalready being used to hack us in multiple ways, from us gifting our data andprivate info to technology firms to how our attention on our mobile devices isfunneled.The internet has created new pillars that monetizes the future where AI isnÈtjust not regulated, it hasnÈt even hit its stride in how ubiquitous it willbecome.* Attention economy* Surveillance capitalism* AI in driving global GDP* AI in cybersecurity* AIÈs role in technological loneliness (divide and conquer in the smart home)If AI has become the new arms race between the likes of the U.S and China,we stand at the dawn of a new era where [AI will bothenhance](https://www.wired.com/story/wired25-kai-fu-lee-fei-fei-li-artificial-intelligence/) and hack us in ways we cannot yet imagine.After reading [Jun Wu](https://medium.com/@junwu_46652)Ès piece on [Empathy inArtificial Intelligence](https://towardsdatascience.com/empathy-in-artificial-intelligence-eb167f62af99), it got me thinking. Male bias and themilitarization of AI is very dangerous for humanity. ItÈs probably anexistential threat to our survival as a species. I donÈt just mean this in theElon Musk terms of AGI, but in how we fail to regulate it.#### China will Lead Ethics in AI in the 21st CenturyAs China rises to dominance both technologically and economically that willbecome more pronounced in the 2030s, their approach to ethics in AI will beginto dominate the world. We can already tell that they will be the leaders inhow facial recognition informs universal systems of surveillance, themanifestation of social credit systems and the monetization of data in Asia.* Facial recognition and its ethical implications.* AI in the rise of Universal Surveillance Architecture Systems (USAS)* Emergence of Social Credit Systems* Monetization of data in Asia[China is bestpositioned](https://www.ft.com/content/f92abc38-6bb8-11e9-80c7-60ee53e6681d)in the future to influence and impact policy regarding the regulation of AI.They have started to think seriously about the [ethics ofAI.](https://www.scmp.com/topics/artificial-intelligence) Their thinking isnot yet mature, but if China can be a leader globally in any signal way thatis most important to the future of humanity, itÈs on the [regulation & ethicsof AI](https://www.scmp.com/business/commodities/article/2157700/ethics-and-pursuit-artificial-intelligence).Before that happens they will likely make a lot of mistakes and seriousbreaches of AI ethics, and its impact on things like human rights.#### The Age of Biotechnology & AI is an Ethical MinefieldWired recently interviewed Fei-Fei Li, you can read the interview[here](https://www.wired.com/story/will-artificial-intelligence-enhance-hack-humanity/). Here are some bullet points from that event:* The era of Biotechnology will allow people to hack their humanity in ways that were never possible before.* AI will be deeply implicated in how we hack other humans and alter even our DNA.* The era of AI and biotech enhancements is coming, so the question is, who decides what is a good enhancement and what is a bad enhancement?* YNH: _The easiest people to manipulate are the people who believe in free will, because they think they cannot be manipulated._An awesome debate. Yuval Noah Harari & Fei-Fei Li### The 4th Industrial Revolution* In the 4th industrial revolution is an era of great convenience and major ethical questions that could lead to irreparable damage in how we evolve as a species (it could lead to our extinction).* What are the good qualities we need to enhance? What happens when our military is automated and robotic? Is enhancing ourselves with AI dangerous?* The technological pressure of innovation for monetization also means powerful corporations could easily become too powerful where they place corporate profits ahead of ethical considerations. In 2019, we already have a long list of such occurrences.* There is already a crisis between engineers and their managers playing itself out at companies such as Google (and many other $1 Trillion dollar firms). The employees of BigTech are speaking out, with mixed results regarding the ethics of the work they are doing. AI is nearly always implicated in this.#### Enhancing Means Hacking HumanityAs companies like Amazon and Google invade the smart home, our private dataand health data will become a major issue in the ethics of AI. This is becausesimultaneously these companies will become giants in the AI of the future ofhealthcare.* Google itself launched and[ dissolved its own AI ethics board](https://www.theverge.com/2019/4/4/18296113/google-ai-ethics-board-ends-controversy-kay-coles-james-heritage-foundation) due to major controversy surrounding it.* In 2019, the debate about ethics in AI also centers on if putting _Advertising_ at the center of the internet is healthy for the world. Facebook & Google among others are implicated in this on-going debate.Apple and Facebook are pivoting into privacy architectures and walled gardensbased on subscription revenue (Apple) and the monetization of encrypted Chat(Facebook).The advent of artificial intelligence forces humanity, both governments andthe corporate sector to organize rules and guidelines on artificialintelligence, bio-technologies and robots that protects the fate of thespecies. Realistically this will take decades to find a global consensus on.### The 2020s are a Black Mirror decade for Ethics in AIBefore that takes place thereÈs a period where hacking (rather than enhancing)human becomes the wild-wild west of cybersecurity invasion. You can think ofsmartphone addiction and mobile app engineering as part of this stage.Fines against companies like Google & Facebook for monopolistic and privacydata infractions are slow and relatively minor compared to the scale,influence and power of these firms and the utilities they provide.#### Artificial General IntelligenceThe possibility of AGI manifesting in the 21st century is low, but still anexistential threat to humanity. More important that how humanity deals withother life forms in our galaxy, guidelines for how to deal with AGI should itever manifest needs also to be constructed. As for artificial intelligence inthe current era, itÈs impossible to regulate.We need better AI to moderate and regulate AI. ItÈs exponential growth in howmachine intelligence and algorithms are impacting us is a very difficult topicconsidering the lack of checks and balances in capitalism. ItÈs likely thatthe 2020s are a very messy decade in terms of how humanity is hacked bycorporations. It could realistically lead to a surveillance architecture thatour parents might easily consider a dystopia.
__label__nan	No petitions, surveys, or crowdfunding
__label__Networking	Probably in file networkSampling_fit-Class.R
__label__Machine Learning	In the museGAN paper, the negative critic loss is said to be converging around 10 to the power of 8.I am training the model on my own, and my generator loss is oscillating around zero, an e.g. history is```console1 | 104/ 180 | 67.79 | 60.750362 | -16.1817931 | 105/ 180 | 67.73 | 62.146736 | 40.1340071 | 106/ 180 | 67.83 | 74.526474 | 33.8560941 | 107/ 180 | 67.66 | 59.294342 | -21.4019321 | 108/ 180 | 67.92 | 115.995209 | 94.3445431 | 109/ 180 | 68.00 | 156.438293 | 224.5999451 | 110/ 180 | 68.32 | 54.108414 | -35.459709```sometimes it is positive, and sometimes negative. And also discriminator loss (negative) is coming around 50-100.So my question is?1. Where should ideally generator and discriminator losses converge?2. I am sensing, I should increase the parameter of the generator, is it a right approach?Thank you in advance. :)
__label__nan	
__label__AI	"<p>I am trying to learn how to create and train a corpus for relation-extraction. I have learned that I require a corpus in the conll format. However, I don't know how I should train the corpus.</p><p>Here is some code that I have to print out example text in the conll format. I am unsure how I would then modify this file with the appropriate changes, and then train with it.</p><pre><code>Properties props = new Properties();props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,natlog,sentiment,kbp,quote"");props.setProperty(""coref.algorithm"", ""neural"");StanfordCoreNLP pipeline = new StanfordCoreNLP(props);String text = ""The modern definition of artificial intelligence (or AI) is \""the study and design of intelligent agents\"" where an intelligent agent is a system that perceives its environment and takes actions which maximizes its chances of success. "" + ""John McCarthy, who coined the term in 1956, defines it as \""the science and engineering of making intelligent machines. "" +""Other names for the field have been proposed, such as computational intelligence, synthetic intelligence or computational rationality. "" + ""The term artificial intelligence is also used to describe a property of machines or programs: the intelligence that the system demonstrates. "" + ""AI research uses tools and insights from many fields, including computer science, psychology, philosophy, neuroscience, cognitive science, linguistics, operations research, economics, control theory, probability, optimization and logic. "" + ""AI research also overlaps with tasks such as robotics, control systems, scheduling, data mining, logistics, speech recognition, facial recognition and many others. "" + ""Computational intelligence Computational intelligence involves iterative development or learning (e.g., parameter tuning in connectionist systems). "" + ""Learning is based on empirical data and is associated with non-symbolic AI, scruffy AI and soft computing. "" + ""Subjects in computational intelligence as defined by IEEE Computational Intelligence Society mainly include: Neural networks: trainable systems with very strong pattern recognition capabilities. "" + ""Fuzzy systems: techniques for reasoning under uncertainty, have been widely used in modern industrial and consumer product control systems; capable of working with concepts such as 'hot', 'cold', 'warm' and 'boiling'. "" + ""Evolutionary computation: applies biologically inspired concepts such as populations, mutation and survival of the fittest to generate increasingly better solutions to the problem. "" + ""These methods most notably divide into evolutionary algorithms (e.g., genetic algorithms) and swarm intelligence (e.g., ant algorithms). "" + ""With hybrid intelligent systems, attempts are made to combine these two groups. "" + ""Expert inference rules can be generated through neural network or production rules from statistical learning such as in ACT-R or CLARION. "" + ""It is thought that the human brain uses multiple techniques to both formulate and cross-check results. "" + ""Thus, systems integration is seen as promising and perhaps necessary for true AI, especially the integration of symbolic and connectionist models. "";// Annotate an example document.//CoreDocument doc = new CoreDocument(text); //pipeline.annotate(doc);String outputFile = ""ConnllTest1.txt"";OutputStream stream;try {stream = new FileOutputStream(outputFile);Writer w = new BufferedWriter( new OutputStreamWriter(stream));pipeline.conllPrint(pipeline.process(text), w);} catch (IOException e) {// TODO Auto-generated catch blocke.printStackTrace();}</code></pre>"
__label__AI	# Ai Hashtags#### Generate Hashtags using Artificial Intelligence_The iOS app has been built, but I can't afford to pay $ 99/year to submit afree app,_[More details](https://smakosh.com/solving-hashtags-problems)
__label__DevOps, Frontend tools	### Basic requirement- [ ] Create react app- [ ] React admin- [ ] Typescript (or Javascript) ???
__label__AI	Natural language processing (NLP) is a field of computer science, artificialintelligence and computational linguistics concerned with the interactionsbetween computers and human (natural) languages, and, in particular, concernedwith programming computers to fruitfully process large natural languagecorpora.
__label__AI	Natural language processing (NLP) is a field of computer science, artificialintelligence and computational linguistics concerned with the interactionsbetween computers and human (natural) languages, and, in particular, concernedwith programming computers to fruitfully process large natural languagecorpora.Join
__label__AI	The approach is related to traditional simulation, but with criticaldifferences. A simulation is essentially assumption-driven, Schawinski said.The approach is to say, I think I know what the underlying physical laws arethat give rise to everything that I see in the system.È So I have a recipe forstar formation, I have a recipe for how dark matter behaves, and so on. I putall of my hypotheses in there, and I let the simulation run. And then I ask:Does that look like reality? What heÈs done with generative modeling, hesaid, is in some sense, exactly the opposite of a simulation. We donÈt knowanything; we donÈt want to assume anything. We want the data itself to tell uswhat might be going on.The apparent success of generative modeling in a study like this obviouslydoesnÈt mean that astronomers and graduate students have been made redundant but it appears to represent a shift in the degree to which learning aboutastrophysical objects and processes can be achieved by an artificial systemthat has little more at its electronic fingertips than a vast pool of data.ItÈs not fully automated science  but it demonstrates that weÈre capable ofat least in part building the tools that make the process of scienceautomatic, Schawinski said.Generative modeling is clearly powerful, but whether it truly represents a newapproach to science is open to debate. For [DavidHogg](https://cosmo.nyu.edu/hogg/), a cosmologist at New York University andthe Flatiron Institute (which, like _Quanta_ , is funded by the SimonsFoundation), the technique is impressive but ultimately just a verysophisticated way of extracting patterns from data  which is what astronomershave been doing for centuries. In other words, itÈs an advanced form ofobservation plus analysis. HoggÈs own work, like SchawinskiÈs, leans heavilyon AI; heÈs been using neural networks to [classifystars](https://arxiv.org/pdf/1711.08793.pdf) according to their spectra and to[infer other physical attributes](https://arxiv.org/pdf/1603.03040.pdf) ofstars using data-driven models. But he sees his work, as well as SchawinskiÈs,as tried-and-true science. I donÈt think itÈs a third way, he said recently.I just think we as a community are becoming far more sophisticated about howwe use the data. In particular, we are getting much better at comparing datato data. But in my view, my work is still squarely in the observational mode.## Hardworking AssistantsWhether theyÈre conceptually novel or not, itÈs clear that AI and neuralnetworks have come to play a critical role in contemporary astronomy andphysics research. At the Heidelberg Institute for Theoretical Studies, thephysicist [KaiPolsterer](https://www.iau.org/administration/membership/individual/16830/)heads the astroinformatics group  a team of researchers focused on new, data-centered methods of doing astrophysics. Recently, theyÈve been using amachine-learning algorithm to [extract redshift information from galaxy datasets](https://www.aanda.org/articles/aa/pdf/2018/01/aa31326-17.pdf), apreviously arduous task.Polsterer sees these new AI-based systems as hardworking assistants that cancomb through data for hours on end without getting bored or complaining aboutthe working conditions. These systems can do all the tedious grunt work, hesaid, leaving you to do the cool, interesting science on your own.But theyÈre not perfect. In particular, Polsterer cautions, the algorithms canonly do what theyÈve been trained to do. The system is agnostic regardingthe input. Give it a galaxy, and the software can estimate its redshift andits age  but feed that same system a selfie, or a picture of a rotting fish,and it will output a (very wrong) age for that, too. In the end, oversight bya human scientist remains essential, he said. It comes back to you, theresearcher. YouÈre the one in charge of doing the interpretation.For his part, Nord, at Fermilab, cautions that itÈs crucial that neuralnetworks deliver not only results, but also error bars to go along with them,as every undergraduate is trained to do. In science, if you make a measurementand donÈt report an estimate of the associated error, no one will take theresults seriously, he said.Like many AI researchers, Nord is also concerned about the impenetrability ofresults produced by neural networks; often, a system delivers an answerwithout offering a clear picture of how that result was obtained.Yet not everyone feels that a lack of transparency is necessarily a problem.[Lenka Zdeborov](http://artax.karlin.mff.cuni.cz/~zdebl9am/), a researcher atthe Institute of Theoretical Physics at CEA Saclay in France, points out thathuman intuitions are often equally impenetrable. You look at a photograph andinstantly recognize a cat  but you donÈt know how you know, she said. Yourown brain is in some sense a black box.ItÈs not only astrophysicists and cosmologists who are migrating toward AI-fueled, data-driven science. Quantum physicists like [RogerMelko](https://uwaterloo.ca/physics-astronomy/people-profiles/roger-melko) ofthe Perimeter Institute for Theoretical Physics and the University of Waterlooin Ontario have used neural networks to solve some of the toughest and mostimportant problems in that field, such as [how to represent the mathematicalwave function](https://arxiv.org/pdf/1812.09329.pdf) describing a many-particle system. AI is essential because of what Melko calls the exponentialcurse of dimensionality. That is, the possibilities for the form of a wavefunction grow exponentially with the number of particles in the system itdescribes. The difficulty is similar to trying to work out the best move in agame like chess or Go: You try to peer ahead to the next move, imagining whatyour opponent will play, and then choose the best response, but with eachmove, the number of possibilities proliferates.Of course, AI systems have mastered both of these games  chess, decades ago,and Go in 2016, when an AI system called[AlphaGo](https://www.theguardian.com/technology/2016/mar/15/googles-alphago-seals-4-1-victory-over-grandmaster-lee-sedol) defeated a top human player.They are similarly suited to problems in quantum physics, Melko says.## The Mind of the MachineWhether Schawinski is right in claiming that heÈs found a third way of doingscience, or whether, as Hogg says, itÈs merely traditional observation anddata analysis on steroids, itÈs clear AI is changing the flavor ofscientific discovery, and itÈs certainly accelerating it. How far will the AIrevolution go in science?Occasionally, grand claims are made regarding the achievements of a robo-scientist. A decade ago, an AI robot chemist named Adam investigated thegenome of bakerÈs yeast and worked out which genes are responsible for makingcertain amino acids. (Adam did this by observing strains of yeast that hadcertain genes missing, and comparing the results to the behavior of strainsthat had the genes.) _Wired_ Ès headline read, [Robot Makes ScientificDiscovery All by Itself](https://www.wired.com/2009/04/robotscientist/).More recently, Lee Cronin, a chemist at the University of Glasgow, has beenusing a robot [to randomly mixchemicals](https://www.wired.co.uk/article/robot-chemist-life-on-earth), tosee what sorts of new compounds are formed. Monitoring the reactions in real-time with a mass spectrometer, a nuclear magnetic resonance machine, and aninfrared spectrometer, the system eventually learned to predict whichcombinations would be the most reactive. Even if it doesnÈt lead to furtherdiscoveries, Cronin has said, the robotic system could allow chemists to speedup their research by about 90 percent.Last year, another team of scientists at ETH Zurich used neural networks to[deduce physical laws](https://arxiv.org/abs/1807.10300) from sets of data.Their system, a sort of robo-Kepler, rediscovered the heliocentric model ofthe solar system from records of the position of the sun and Mars in the sky,as seen from Earth, and figured out the law of conservation of momentum byobserving colliding balls. Since physical laws can often be expressed in morethan one way, the researchers wonder if the system might offer new ways perhaps simpler ways  of thinking about known laws.These are all examples of AI kick-starting the process of scientificdiscovery, though in every case, we can debate just how revolutionary the newapproach is. Perhaps most controversial is the question of how muchinformation can be gleaned from data alone  a pressing question in the age ofstupendously large (and growing) piles of it. In _The Book of Why_ (2018), thecomputer scientist Judea Pearl and the science writer Dana Mackenzie assertthat data are profoundly dumb. Questions about causality can never beanswered from data alone, they write. Anytime you see a paper or a studythat analyzes the data in a model-free way, you can be certain that the outputof the study will merely summarize, and perhaps transform, but not interpretthe data. Schawinski sympathizes with PearlÈs position, but he described theidea of working with data alone as a bit of a straw man. HeÈs neverclaimed to deduce cause and effect that way, he said. IÈm merely saying wecan do more with data than we often conventionally do.Another oft-heard argument is that science requires creativity, and that  atleast so far  we have no idea how to program that into a machine. (Simplytrying everything, like CroninÈs robo-chemist, doesnÈt seem especiallycreative.) Coming up with a theory, with reasoning, I think demandscreativity, Polsterer said. Every time you need creativity, you will need ahuman. And where does creativity come from? Polsterer suspects it is relatedto boredom  something that, he says, a machine cannot experience. To becreative, you have to dislike being bored. And I donÈt think a computer willever feel bored. On the other hand, words like creative and inspired haveoften been used to describe programs like Deep Blue and AlphaGo. And thestruggle to describe what goes on inside the mind of a machine is mirroredby the difficulty we have in probing our own thought processes.Schawinski recently left academia for the private sector; he now runs astartup called Modulos which employs a number of ETH scientists and, accordingto its website, works in the eye of the storm of developments in AI andmachine learning. Whatever obstacles may lie between current AI technologyand full-fledged artificial minds, he and other experts feel that machines arepoised to do more and more of the work of human scientists. Whether there is alimit remains to be seen.Will it be possible, in the foreseeable future, to build a machine that candiscover physics or mathematics that the brightest humans alive are not ableto do on their own, using biological hardware? Schawinski wonders. Will thefuture of science eventually necessarily be driven by machines that operate ona level that we can never reach? I donÈt know. ItÈs a good question.
__label__nan	
__label__DevOps	/assign <!-- @ the release branch manager or the person who cuts the release -->Scheduled to happen <!-- Tue, 2019-02-26 -->Any bumps or issues encountered along the way, post here. /close when the release has been cut.<!-- add & remove items of the checklist as you see fit -->- [ ] screen shot master's (unhealthy) testgrid boards and add them as a comment- [ ] stage & release- [ ] collect metrics, links, ... and add them as a comment <!-- example: https://github.com/kubernetes/sig-release/issues/506#issuecomment-465202113 -->- [ ] notify [#sig-release](https://kubernetes.slack.com/messages/C2C40FMNF/) <!-- e.g. https://kubernetes.slack.com/archives/C2C40FMNF/p1551205263064000 -->- [ ] build & publish packages (debs & rpms)- [ ] send notification mail/milestone <!-- v1.14 -->/sig release/area release-team<!--Example template for screenshots comment:----### Testgrid status#### master blocking<details><summary>`gce-cos-master-default`</summary></p>... paste image here ...</p></details>#### master-upgrade<details><summary>`gce-new-master-upgrade-master`</summary><p>... paste image here ...... paste multiple images here ...</p></details>------>
__label__AI	"Computers which are capable of teaching themselves to predict premature deathcould greatly improve preventative healthcare in the future, suggests a newstudy by experts at the University of Nottingham.The team of healthcare data scientists and doctors have developed and tested asystem of computer-based 'machine learning' algorithms to predict the risk ofearly death due to chronic disease in a large middle-aged population.They found this AI system was very accurate in its predictions and performedbetter than the current standard approach to prediction developed by humanexperts. The study is published by _PLOS ONE_ in a special collections editionof ""Machine Learning in Health and Biomedicine"".The team used health data from just over half a million people aged between 40and 69 recruited to the UK Biobank between 2006 and 2010 and followed up until2016.Leading the work, Assistant Professor of Epidemiology and Data Science, DrStephen Weng, said: ""Preventative healthcare is a growing priority in thefight against serious diseases so we have been working for a number of yearsto improve the accuracy of computerised health risk assessment in the generalpopulation. Most applications focus on a single disease area but predictingdeath due to several different disease outcomes is highly complex, especiallygiven environmental and individual factors that may affect them.""We have taken a major step forward in this field by developing a unique andholistic approach to predicting a person's risk of premature death by machine-learning. This uses computers to build new risk prediction models that takeinto account a wide range of demographic, biometric, clinical and lifestylefactors for each individual assessed, even their dietary consumption of fruit,vegetables and meat per day.""We mapped the resulting predictions to mortality data from the cohort, usingOffice of National Statistics death records, the UK cancer registry and'hospital episodes' statistics. We found machine learned algorithms weresignificantly more accurate in predicting death than the standard predictionmodels developed by a human expert.""The AI machine learning models used in the new study are known as 'randomforest' and 'deep learning'. These were pitched against the traditionally-used'Cox regression' prediction model based on age and gender - found to be theleast accurate at predicting mortality - and also a multivariate Cox modelwhich worked better but tended to over-predict risk.Professor Joe Kai, one of the clinical academics working on the project, said:""There is currently intense interest in the potential to use 'AI' or 'machine-learning' to better predict health outcomes. In some situations we may find ithelps, in others it may not. In this particular case, we have shown that withcareful tuning, these algorithms can usefully improve prediction.""These techniques can be new to many in health research, and difficult tofollow. We believe that by clearly reporting these methods in a transparentway, this could help with scientific verification and future development ofthis exciting field for health care.""This new study builds on previous work by the Nottingham team which showedthat four different AI algorithms, 'random forest', 'logistic regression','gradient boosting' and 'neural networks', were significantly better atpredicting cardiovascular disease than an established algorithm used incurrent cardiology guidelines. This earlier study is available here.The Nottingham researchers predict that AI will play a vital part in thedevelopment of future tools capable of delivering personalised medicine,tailoring risk management to individual patients. Further research requiresverifying and validating these AI algorithms in other population groups andexploring ways to implement these systems into routine healthcare.###**Disclaimer:** AAAS and EurekAlert! are not responsible for the accuracy ofnews releases posted to EurekAlert! by contributing institutions or for theuse of any information through the EurekAlert system."
