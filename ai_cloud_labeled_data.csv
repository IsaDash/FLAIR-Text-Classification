,_id,_index,_score,tags,category label,text,category (from classifier),confidence,title,_type0,t3_buuoi2,conversations_reddit,17.658812,,AI,"Natural language processing (NLP) is a field of computer science, artificialintelligence and computational linguistics concerned with the interactionsbetween computers and human (natural) languages, and, in particular, concernedwith programming computers to fruitfully process large natural languagecorpora.",Artificial Intelligence,1,Ask AI: Is Bob Dylan an Author or a Songwriter?,standard1,t3_bxz7xt,conversations_reddit,17.658812,,AI,"Natural language processing (NLP) is a field of computer science, artificialintelligence and computational linguistics concerned with the interactionsbetween computers and human (natural) languages, and, in particular, concernedwith programming computers to fruitfully process large natural languagecorpora.",Artificial Intelligence,1,Understanding the parsed sentence,standard2,https://news.ycombinator.com/item?id=19416359,conversations_hackernewsnew,17.657885,[],,"%PDF-1.5 %¥Ë_¥Ë_¥Ë_¥Ë_ 366 0 obj <> endobj 384 0 obj<>/Filter/FlateDecode/ID[<20FF5D460D546D4B863BC23C3FC01C80>]/Index[36635]/Info 365 0 R/Length 89/Prev 206688/Root 367 0 R/Size 401/Type/XRef/W[1 21]>>stream h¥Ë_bbd``b`¥Ë_ ¥Ë_¥Ë_S ¥Ë_`¥Ë_ ¥Ë_*¥Ë_¥Ë_¥Ë_-¥Ë_bÎÅ_&¥Ë_ $V$X~ .¥Ë_,¥Ë_2¥Ë_¥Ë_2¥Ë_:¥Ë_X ¥Ë_`d¥Ë_2¥Ë_¥Ë_¥Ë_r¥Ë_?C¥Ë_'¥Ë_¥Ë_¥Ë_< endstream endobj startxref 0 %%EOF 400 0 obj <>stream h¥Ë_b```f``¥Ë_¥Ë_¥Ë_¥Ë_¥Ë_¥Ë_,¥Ë_ ¥Ë_¥Ë_@1V ¥Ë_$¥Ë_$¥Ë_¥Ë_¥Ë_era¥Ë_¥Ë_¥Ë_¥Ë_X¥Ë_¥Ë_ = mkKK¥Ë_d¥Ë_Nv¥Ë_t¥Ë_¥Ë_q¥Ë_¥Ë_¥Ë_¥Ë_X__q#âÍm&J¥Ë_n\¥Ë_I Åó\¥Ë_¥Ë_¥Ë_¥Ë_¥Ë_¥Ë_¥Ë_6¥Ë_&¥Ë_00¥Ë_¥Ë_v¥Ë_0¥Ë_Q¥Ë_¥Ë_|¥Ë_)v¥Ë_.g¥Ë_5¥Ë_¥Ë_¥Ë_¥Ë_¥Ë_—Ÿm¥Ë_k¥Ë_¥Ë_(Ü„ì¥Ë_ ¥Ë_:¥Ë_9¥Ë_6v ¥Ë_¥Ë_¥Ë_’È ¥Ë_¥Ë_¥Ë_¥Ë_¥Ë_4¥Ë_ ¥Ë_¥Ë_¥Ë_¥Ë_¥Ë_¥Ë_¥Ë_¥Ë_ ¥Ë_ ¥Ë_9¥Ë_endstream endobj 367 0 obj <",Artificial Intelligence,0.574,How artificial intelligence works [pdf],standard3,https://news.ycombinator.com/item?id=19359970,conversations_hackernewsnew,17.657885,[],AI,"The approach is related to traditional simulation, but with criticaldifferences. A simulation is äóìessentially assumption-driven,äóù Schawinski said.äóìThe approach is to say, äóÖI think I know what the underlying physical laws arethat give rise to everything that I see in the system.äó» So I have a recipe forstar formation, I have a recipe for how dark matter behaves, and so on. I putall of my hypotheses in there, and I let the simulation run. And then I ask:Does that look like reality?äóù What heäó»s done with generative modeling, hesaid, is äóìin some sense, exactly the opposite of a simulation. We donäó»t knowanything; we donäó»t want to assume anything. We want the data itself to tell uswhat might be going on.äóùThe apparent success of generative modeling in a study like this obviouslydoesnäó»t mean that astronomers and graduate students have been made redundant äóîbut it appears to represent a shift in the degree to which learning aboutastrophysical objects and processes can be achieved by an artificial systemthat has little more at its electronic fingertips than a vast pool of data.äóìItäó»s not fully automated science äóî but it demonstrates that weäó»re capable ofat least in part building the tools that make the process of scienceautomatic,äóù Schawinski said.Generative modeling is clearly powerful, but whether it truly represents a newapproach to science is open to debate. For [DavidHogg](https://cosmo.nyu.edu/hogg/), a cosmologist at New York University andthe Flatiron Institute (which, like _Quanta_ , is funded by the SimonsFoundation), the technique is impressive but ultimately just a verysophisticated way of extracting patterns from data äóî which is what astronomershave been doing for centuries. In other words, itäó»s an advanced form ofobservation plus analysis. Hoggäó»s own work, like Schawinskiäó»s, leans heavilyon AI; heäó»s been using neural networks to [classifystars](https://arxiv.org/pdf/1711.08793.pdf) according to their spectra and to[infer other physical attributes](https://arxiv.org/pdf/1603.03040.pdf) ofstars using data-driven models. But he sees his work, as well as Schawinskiäó»s,as tried-and-true science. äóìI donäó»t think itäó»s a third way,äóù he said recently.äóìI just think we as a community are becoming far more sophisticated about howwe use the data. In particular, we are getting much better at comparing datato data. But in my view, my work is still squarely in the observational mode.äóù## Hardworking AssistantsWhether theyäó»re conceptually novel or not, itäó»s clear that AI and neuralnetworks have come to play a critical role in contemporary astronomy andphysics research. At the Heidelberg Institute for Theoretical Studies, thephysicist [KaiPolsterer](https://www.iau.org/administration/membership/individual/16830/)heads the astroinformatics group äóî a team of researchers focused on new, data-centered methods of doing astrophysics. Recently, theyäó»ve been using amachine-learning algorithm to [extract redshift information from galaxy datasets](https://www.aanda.org/articles/aa/pdf/2018/01/aa31326-17.pdf), apreviously arduous task.Polsterer sees these new AI-based systems as äóìhardworking assistantsäóù that cancomb through data for hours on end without getting bored or complaining aboutthe working conditions. These systems can do all the tedious grunt work, hesaid, leaving you äóìto do the cool, interesting science on your own.äóùBut theyäó»re not perfect. In particular, Polsterer cautions, the algorithms canonly do what theyäó»ve been trained to do. The system is äóìagnosticäóù regardingthe input. Give it a galaxy, and the software can estimate its redshift andits age äóî but feed that same system a selfie, or a picture of a rotting fish,and it will output a (very wrong) age for that, too. In the end, oversight bya human scientist remains essential, he said. äóìIt comes back to you, theresearcher. Youäó»re the one in charge of doing the interpretation.äóùFor his part, Nord, at Fermilab, cautions that itäó»s crucial that neuralnetworks deliver not only results, but also error bars to go along with them,as every undergraduate is trained to do. In science, if you make a measurementand donäó»t report an estimate of the associated error, no one will take theresults seriously, he said.Like many AI researchers, Nord is also concerned about the impenetrability ofresults produced by neural networks; often, a system delivers an answerwithout offering a clear picture of how that result was obtained.Yet not everyone feels that a lack of transparency is necessarily a problem.[Lenka Zdeborovíç](http://artax.karlin.mff.cuni.cz/~zdebl9am/), a researcher atthe Institute of Theoretical Physics at CEA Saclay in France, points out thathuman intuitions are often equally impenetrable. You look at a photograph andinstantly recognize a cat äóî äóìbut you donäó»t know how you know,äóù she said. äóìYourown brain is in some sense a black box.äóùItäó»s not only astrophysicists and cosmologists who are migrating toward AI-fueled, data-driven science. Quantum physicists like [RogerMelko](https://uwaterloo.ca/physics-astronomy/people-profiles/roger-melko) ofthe Perimeter Institute for Theoretical Physics and the University of Waterlooin Ontario have used neural networks to solve some of the toughest and mostimportant problems in that field, such as [how to represent the mathematicaläóìwave functionäóù](https://arxiv.org/pdf/1812.09329.pdf) describing a many-particle system. AI is essential because of what Melko calls äóìthe exponentialcurse of dimensionality.äóù That is, the possibilities for the form of a wavefunction grow exponentially with the number of particles in the system itdescribes. The difficulty is similar to trying to work out the best move in agame like chess or Go: You try to peer ahead to the next move, imagining whatyour opponent will play, and then choose the best response, but with eachmove, the number of possibilities proliferates.Of course, AI systems have mastered both of these games äóî chess, decades ago,and Go in 2016, when an AI system called[AlphaGo](https://www.theguardian.com/technology/2016/mar/15/googles-alphago-seals-4-1-victory-over-grandmaster-lee-sedol) defeated a top human player.They are similarly suited to problems in quantum physics, Melko says.## The Mind of the MachineWhether Schawinski is right in claiming that heäó»s found a äóìthird wayäóù of doingscience, or whether, as Hogg says, itäó»s merely traditional observation anddata analysis äóìon steroids,äóù itäó»s clear AI is changing the flavor ofscientific discovery, and itäó»s certainly accelerating it. How far will the AIrevolution go in science?Occasionally, grand claims are made regarding the achievements of a äóìrobo-scientist.äóù A decade ago, an AI robot chemist named Adam investigated thegenome of bakeräó»s yeast and worked out which genes are responsible for makingcertain amino acids. (Adam did this by observing strains of yeast that hadcertain genes missing, and comparing the results to the behavior of strainsthat had the genes.) _Wired_ äó»s headline read, äóì[Robot Makes ScientificDiscovery All by Itself](https://www.wired.com/2009/04/robotscientist/).äóùMore recently, Lee Cronin, a chemist at the University of Glasgow, has beenusing a robot [to randomly mixchemicals](https://www.wired.co.uk/article/robot-chemist-life-on-earth), tosee what sorts of new compounds are formed. Monitoring the reactions in real-time with a mass spectrometer, a nuclear magnetic resonance machine, and aninfrared spectrometer, the system eventually learned to predict whichcombinations would be the most reactive. Even if it doesnäó»t lead to furtherdiscoveries, Cronin has said, the robotic system could allow chemists to speedup their research by about 90 percent.Last year, another team of scientists at ETH Zurich used neural networks to[deduce physical laws](https://arxiv.org/abs/1807.10300) from sets of data.Their system, a sort of robo-Kepler, rediscovered the heliocentric model ofthe solar system from records of the position of the sun and Mars in the sky,as seen from Earth, and figured out the law of conservation of momentum byobserving colliding balls. Since physical laws can often be expressed in morethan one way, the researchers wonder if the system might offer new ways äóîperhaps simpler ways äóî of thinking about known laws.These are all examples of AI kick-starting the process of scientificdiscovery, though in every case, we can debate just how revolutionary the newapproach is. Perhaps most controversial is the question of how muchinformation can be gleaned from data alone äóî a pressing question in the age ofstupendously large (and growing) piles of it. In _The Book of Why_ (2018), thecomputer scientist Judea Pearl and the science writer Dana Mackenzie assertthat data are äóìprofoundly dumb.äóù Questions about causality äóìcan never beanswered from data alone,äóù they write. äóìAnytime you see a paper or a studythat analyzes the data in a model-free way, you can be certain that the outputof the study will merely summarize, and perhaps transform, but not interpretthe data.äóù Schawinski sympathizes with Pearläó»s position, but he described theidea of working with äóìdata aloneäóù as äóìa bit of a straw man.äóù Heäó»s neverclaimed to deduce cause and effect that way, he said. äóìIäó»m merely saying wecan do more with data than we often conventionally do.äóùAnother oft-heard argument is that science requires creativity, and that äóî atleast so far äóî we have no idea how to program that into a machine. (Simplytrying everything, like Croninäó»s robo-chemist, doesnäó»t seem especiallycreative.) äóìComing up with a theory, with reasoning, I think demandscreativity,äóù Polsterer said. äóìEvery time you need creativity, you will need ahuman.äóù And where does creativity come from? Polsterer suspects it is relatedto boredom äóî something that, he says, a machine cannot experience. äóìTo becreative, you have to dislike being bored. And I donäó»t think a computer willever feel bored.äóù On the other hand, words like äóìcreativeäóù and äóìinspiredäóù haveoften been used to describe programs like Deep Blue and AlphaGo. And thestruggle to describe what goes on inside the äóìmindäóù of a machine is mirroredby the difficulty we have in probing our own thought processes.Schawinski recently left academia for the private sector; he now runs astartup called Modulos which employs a number of ETH scientists and, accordingto its website, works äóìin the eye of the storm of developments in AI andmachine learning.äóù Whatever obstacles may lie between current AI technologyand full-fledged artificial minds, he and other experts feel that machines arepoised to do more and more of the work of human scientists. Whether there is alimit remains to be seen.äóìWill it be possible, in the foreseeable future, to build a machine that candiscover physics or mathematics that the brightest humans alive are not ableto do on their own, using biological hardware?äóù Schawinski wonders. äóìWill thefuture of science eventually necessarily be driven by machines that operate ona level that we can never reach? I donäó»t know. Itäó»s a good question.äóù",Artificial Intelligence,1,Artificial Intelligence Is Changing Science,standard4,https://news.ycombinator.com/item?id=19787525,conversations_hackernewsnew,17.657885,[],AI,"[![](https://www.microsoft.com/en-us/research/uploads/prod/2019/04/Visceral-Machines-2Site_04_2019_1400x788-1024x576.png)](https://www.microsoft.com/en-us/research/uploads/prod/2019/04/Visceral-Machines-2Site_04_2019_1400x788.png)Recent successes in machine intelligence hinge on core computation ability toefficiently search through billions of possibilities in order to makedecisions. Sequences of decisions, if successful, often suggest that perhapscomputation is catching up toäóñor even surpassingäóñhuman intelligence. Humanintelligence, on the other hand, is highly generalizable, adaptive, robust andexhibits characteristics that the current state-of-the-art machineintelligence systems simply are not yet capable of producing. For example,humans are able to plan significantly far in advance based on the anticipatedoutcomes, even in the presence of many unknown variables. Human intelligenceshines in scenarios in which other humans and living beings are involved andconsistently demonstrates reasoning and meta-reasoning abilities. Humanintelligence is also sympathetic, empathetic, kind, nurturingandäóñimportantlyäóñable to relinquish and redefine the goals of a mission for thebenefit of a greater good. While almost all the work in machine intelligencefocuses on äóìhowäóù, the hallmark of human-intelligence is the ability to askäóìwhatäóù and äóìwhyäóù.Our hypothesis is that emotional intelligence is key to unlocking emergence ofmachines that are not only more general, robust and efficient, but that alsoare aligned with the values of humanity. The affective mechanisms in humansallow us to accomplish tasks that are far too difficult to program or teachcurrent machines. For example, our sympathetic and parasympathetic responsesallow us to stay safe and to be aware of danger. Our ability to recognizeaffect in others and imagine ourselves in their situations makes us far moreeffective in taking appropriate decisions and navigating in the complex world.Drives and affect such as hunger, curiosity, surprise, and joy enable us toregulate our own behavior and also determine the sets of goals that we wish toachieve. And finally, our ability to express our own internal state is anexcellent way to signal to others and possibly influence their decisionmaking.Consequently, [it has beenhypothesized](https://mitpress.mit.edu/books/affective-computing) thatbuilding such an emotional intelligence into a computational framework atminimum would require the following capabilities:* Recognizing othersäó» emotions* Responding to othersäó» emotions* Expressing emotions* Regulating and utilizing emotions in decision makingHistorically, the research in building emotionally intelligent machines hasprimarily taken the human-machine collaboration point of view and mostlyfocused on the first three capabilities. For example, [the earliestwork](https://mitpress.mit.edu/books/affective-computing) on affectrecognition started almost three decades ago, where physiological sensors,cameras, microphones, and so on were used to detect a host of affectiveresponses. While there is much debate about how consistently and universallypeople express emotions on their faces and other physiological signals, andwhether these really reflect how they feel inside, [researchers havesuccessfully built algorithms to identify useful signals in the noisy world ofhuman expressions as well as demonstrated that these signals are consistentwith socio-culturalnorms](http://alumni.media.mit.edu/~djmcduff/assets/publications/McDuff_2017_SAS_Abstract.pdf).Ability to take appropriate actions based on the internal cognitive state of ahuman is imperative for an emotionally intelligent agent.[Applications](https://ieeexplore.ieee.org/document/1532370) such as[automatic tutoring systems](https://www.microsoft.com/en-us/research/uploads/prod/2016/12/ACM2005.pdf), mental and physical healthsupport, and applications for improving productivity lie at the forefront ofwhat is being pursued. The recent line of work on sequential decision making,such as contextual bandits, is slowly making gains in this rich area. [Our ownwork](https://www.microsoft.com/en-us/research/uploads/prod/2016/10/foodandmood_final.pdf), for example, showshow a system sensitive to affective aspects of managing a diet could helpsubjects make good decisions.Expression of affect has been at the forefront of computing for many decadesnow. Even simple signals (for example, light, color, sound) have the abilityto convey and provoke rich emotion. In äóì[Neural TTS Stylization withAdversarial and Collaborative Games](https://www.microsoft.com/en-us/research/publication/neural-tts-stylization-with-adversarial-and-collaborative-games/),äóù (co-authored with Shuang Ma and [YaleSong](https://www.microsoft.com/en-us/research/people/yalesong/)) to bepresented at the Seventh International Conference on LearningRepresentationsäóî[ICLR 2019](https://iclr.cc/), we propose a new machinelearning approach to synthesizing realistic human sounding speech that isexpressive. This architecture challenges the model to generate realisticsounding speech that is faithful to the textual content while maintaining aneasily controllable dial for changing the emotion expressed in an independentfashion. Our model achieves start-of-the-art results across multiple tasks,including style transfer (content and style swapping), emotion modeling, andidentity transfer (fitting a new speakeräó»s voice). An open-sourceimplementation is available with the paper.[![Figure 1-Our neural architecture uses a combination of adversarial andcollaborative approaches. The algorithm received two audio samples during eachtraining step and has to produce two samples one of which is a reconstructionof the first audio sample \(i.e., has both the content and style of sample 1\)and the second which has the content of sample 1 and the style of sample 2. Indoing so it creates an internal representation of both content and style whichare disambiguated. ](https://www.microsoft.com/en-us/research/uploads/prod/2019/04/Neural_TTS-1024x552.jpg)](https://researchdemopage.wixsite.com/tts-gan)Figure 1-Our neural architecture uses a combination of adversarial andcollaborative approaches. The algorithm received two audio samples during eachtraining step and has to produce two samples one of which is a reconstructionof the first audio sample (i.e., has both the content and style of sample 1)and the second which has the content of sample 1 and the style of sample 2. Indoing so it creates an internal representation of both content and style whichare disambiguated.While the recognition, expression and intervention aspects of artificiallyemotionally intelligent systems have been studied in-depth over the past 20years, there is a still more compelling form of intelligenceäóîa system thatutilizes the affective mechanisms effectively in order to learn better andmake choices efficiently. In the most recent line of work, we hope to explorequestions of how to build such affective mechanisms that help ourcomputational processes achieve more than what they accomplish currently.Our recent work, also appearing at ICLR 2019, explores the idea of affect-based intrinsic motivations that can aid in learning decision-makingmechanisms. Much of the recent success in artificial intelligence in solvinggames such as Go, Pac-Man, and text-based RPGs rely on reinforcement learning,where good actions are rewarded and bad actions are penalized. However, itrequires a large number of trials in such an action-reward framework for acomputational agent to learn a reasonable policy. The intuition behind ourproposal is to get inspiration from how humans and other living beingsleverage affective mechanisms to learn much more efficiently.As a human learns to navigate the world, the bodyäó»s (nervous systemäó»s)responses provide constant intrinsic feedback about the potential consequenceof action choices, for example, becoming nervous when close to a cliffäó»s edgeor when driving fast around a bend. Physiological changes are correlated withthese biological preparations to protect one-self from danger. Theanticipatory response in humans to a threatening situation is for the heartrate to increase, heart rate variability to decrease, and for blood to bediverted from the extremities and for the sweat glands to dilate. This is thebodyäó»s äóìfight or flightäóù response. Humans have evolved over millions of yearsto build up these complex systems. What if machines had similar feedbacksystems?[![Visceral Machines are a novel approach to reinforcement learning thatleverages neural networks trained on physiological signals to mimic autonomicnervous system responses. Such signals then are used as intrinsic rewardmechanisms to train agents that can learn to accomplish varioustasks.](https://www.microsoft.com/en-us/research/uploads/prod/2019/04/Figure-2a-1024x728.png)](https://www.microsoft.com/en-us/research/uploads/prod/2019/04/Figure-2a.png)Figure 2-Visceral Machines are a novel approach to reinforcement learning thatleverages neural networks trained on physiological signals to mimic autonomicnervous system responses. Such signals then are used as intrinsic rewardmechanisms to train agents that can learn to accomplish various tasks.In äóì[Visceral Machines: Risk-Aversion in Reinforcement Learning with IntrinsicPhysiological Rewards](https://www.microsoft.com/en-us/research/publication/visceral-machines-risk-aversion-in-reinforcement-learning-with-intrinsic-physiological-rewards/),äóù we propose a novel approachto reinforcement learning that leverages an intrinsic reward function trainedon human fight or flight behavior.Our hypothesis is that such reward functions can circumvent the challengesassociated with sparse and skewed rewards in reinforcement learning settingsand can help improve sample efficiency. In our case, extrinsic rewards fromevents are not necessary for the agent to learn. We test this in a simulateddriving environment and show that it can increase the speed of learning andreduce the number of collisions during the learning stage. We are excitedabout the potential of training autonomous systems that mimic the ability tofeel and respond to stimuli in an emotional way.[![An example of the physiological response \(blood volume pulse\) recordedfrom a human during a driving task. A zoomed in section of the pulse wave withframes from the view of the driver are shown. Note how the pulse wave pinchesbetween seconds 285 and 300, during this period the driver collided with awall while turning sharply to avoid another obstacle. The pinching beginsbefore the collision occurs as the driveräó»s anticipatory response isactivated. The intrinsic reward component aims to recreate statisticalproperties of the blood volume pulse wave during driving in the simulatedenvironment](https://www.microsoft.com/en-us/research/uploads/prod/2019/04/Figure-2b-1024x601.png)](https://www.microsoft.com/en-us/research/uploads/prod/2019/04/Figure-2b.png)Figure 3-An example of the physiological response (blood volume pulse)recorded from a human during a driving task. A zoomed in section of the pulsewave with frames from the view of the driver are shown. Note how the pulsewave pinches between seconds 285 and 300, during this period the drivercollided with a wall while turning sharply to avoid another obstacle. Thepinching begins before the collision occurs as the driveräó»s anticipatoryresponse is activated. The intrinsic reward component aims to recreatestatistical properties of the blood volume pulse wave during driving in thesimulated environmentA lot of computer scientists and roboticists have aspired to build agents thatresemble memorable characters in popular science fiction such as KITT andR2D2. However, rich opportunities exist for building holistic affectivecomputing mechanisms that go a step beyond and to help us build robust,efficient and non-myopic artificial intelligence. We hope that this workinspires a fresh look at how emotions can be used in artificial intelligence.We hope to see you at ICLR in New Orleans in May and look forward to sharingideas and advancing the conversation on the possibilities in the excitingresearch realm of emotionally intelligent agents.",Artificial Intelligence,1,Toward Emotionally Intelligent Artificial Intelligence,standard5,t3_b2jcu5,conversations_reddit,17.640572,,AI,"I analyzed all the posts from Stack Overflow / Stack Exchange and extractedmost frequently mentioned Artificial Intelligence & Machine Learning books.Top Mentioned AI & Machine Learning Books on Stack Overflow / Exchange<http://www.aimlbooks.com/>Let me know what you think!",Artificial Intelligence,1,[D] Top Mentioned AI & Machine Learning Books on Stack Overflow / Exchange,standard6,t3_bpso9a,conversations_reddit,17.640572,,AI,"Natural language processing (NLP) is a field of computer science, artificialintelligence and computational linguistics concerned with the interactionsbetween computers and human (natural) languages, and, in particular, concernedwith programming computers to fruitfully process large natural languagecorpora.Join",Artificial Intelligence,1,Microsoft makes Googleäó»s BERT NLP model better,standard7,https://news.ycombinator.com/item?id=19427931,conversations_hackernewsnew,17.60834,[],AI,"![](https://www.washingtonpost.com/resizer/yOJdgadrZVjeFjGOtMESntCzPQE=/1484x0/arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/6IUQCUSHIAI6TFFL2LO2HQG7KI.jpg) Fei-Fei Liand John Etchemendy are co-directors at the Stanford Institute for Human-Centered Artificial Intelligence. (Peter DaSilva for The Washington Post)PALO ALTO, Calif. äóî A Stanford University scientist coined the term artificialintelligence. Others at the university created some of the most significantapplications of it, such as the [first autonomousvehicle](https://www.wired.com/story/wired25-sebastian-thrun-sam-altman-artificial-intelligence/).But as Silicon Valley faces a reckoning over how technology is changingsociety, Stanford wants to be at the forefront of a different type ofinnovation, one that puts humans and ethics at the center of the booming fieldof AI.On Monday, the university will launch the Stanford Institute for Human-Centered Artificial Intelligence (HAI), a sprawling think tank that aims tobecome an interdisciplinary hub for policymakers, researchers and students whowill go on to build the technologies of the future. They hope they caninculcate in that next generation a more worldly and humane set of values thanthose that have characterized it so far äóî and guide politicians to make moresophisticated decisions about the challenging social questions wrought bytechnology.äóìI could not have envisioned that the discipline I was so interested in would,a decade and a half later, become one of the driving forces of the changesthat humanity will undergo,äóù said Fei-Fei Li, an AI pioneer and former Googlevice president who is one of two directors of the new Stanford institute.äóìThat realization became a tremendous sense of responsibility.äóùThe institute äóî backed by the fieldäó»s biggest leaders and industry players äóîis not the first such academic effort of its kind, but it is by far the mostambitious: It aims to raise more than $1 billion. And its advisory council isa whoäó»s who of Silicon Valley titans, including former Google executivechairman Eric Schmidt, LinkedIn co-founder Reid Hoffman, former Yahoo chiefexecutive Marissa Mayer and co-founder Jerry Yang, and the prominent investorJim Breyer. Microsoft co-founder Bill Gates will keynote its inauguralsymposium on Monday.The money raised will not only go to research grants and academic gatheringsbut also to buying data processing power and luring back some of the talentthat has fled academia for lucrative industry jobs in recent years. It will behoused in a new 200,000-square-foot building at the heart of Stanfordäó»scampus.äóìWe recognize that decisions that are made early on in the development of atechnology have huge ramifications,äóù said John Etchemendy, a philosopher andformer Stanford provost, the second director of the AI institute. äóìWe need tobe thoughtful about what those might be, and to do that we canäó»t rely simplyon technologists.äóù![](https://www.washingtonpost.com/resizer/TXJKur-cQ3m4AF9lqk_kkjfemkk=/3x2/www.washingtonpost.com/pb/resources/img/spacer.gif)The Stanford Institute for Human-Centered Artificial Intelligence will behoused in a new building at the center of the campus. (Peter DaSilva for TheWashington Post)The idea for the institute began with a conversation in 2016 between Li andEtchemendy that took place in Liäó»s driveway about a five-minute drive fromcampus.Etchemendy had recently purchased the house next door. But the casualneighborly chat quickly morphed into a weightier dialogue about the future ofsociety and what had gone wrong in the exploding field of AI. Billions ofdollars were being invested in start-ups dedicated to commercializing what hadpreviously been niche academic technologies. Companies like Facebook, Appleand Google were hiring the worldäó»s top artificial researchers äóî along withmany of their recently minted graduates äóî to work in new divisions dedicatedto robotics, self-driving cars and voice recognition for home devices.äóìThe correct answer to pretty much everything in AI is more of it,äóù saidSchmidt, the former Google chairman. äóìThis generation is much more sociallyconscious than we were, and more broadly concerned about the impact ofeverything they do, so youäó»ll see a combination of both optimism and realism.äóùIn the years following that conversation in the driveway, the dangers and illsof AI have become more apparent. Seemingly every day, new statistics emergeabout the tide of job loss wrought by the technology, from long-haul truckersto farmworkers to dermatologists. Elon Musk called AI äóìhumanityäó»s existentialthreatäóù and compared it to äóìsummoning the demon.äóùResearchers and journalists have shown how AI technologies, largely designedby white and Asian men, tend to [reproduce andamplify](https://www.wsj.com/articles/computers-are-showing-their-biases-and-tech-firms-are-concerned-1440102894) social biases in dangerous ways. Computervision technologies built into cameras have trouble recognizing the faces ofpeople of color. Voice recognition[struggles](https://www.washingtonpost.com/graphics/2018/business/alexa-does-not-understand-your-accent/?utm_term=.e6e67a48897f) to pick up English accentsthat arenäó»t mainstream. Algorithms built to predict the likelihood of paroleviolations are rife with [racialbias](https://www.propublica.org/article/bias-in-criminal-risk-scores-is-mathematically-inevitable-researchers-say).And there are political ramifications: Recommendation software designed totarget ads to interested consumers was abused by bad actors, including Russianoperatives, to amplify disinformation and false narratives in public debate.äóìThe question comes down to whether this revolution of AI äóî and of todayäó»smachine learning techniques äóî will contribute to the progression of humanity,äóùsaid Hoffman, who chairs the instituteäó»s advisory council. He calledStanfordäó»s institute a potential äóìkey leveräóù that would act as a äóìcatalyst,äóùtrusted adviser, and source of intelligence for industry, the government andthe public. (Hoffman ran into trouble last year after reports showed that hehad funded a disinformation campaign on Facebook during the Alabama election.He said he did not know his money was used in that way.)While universities in recent years have drawn criticism for raising largeamounts of money äóî Stanford is among the biggest fundraisers of all äóî the cashis particularly necessary if universities are to remain competitive in thefield of AI, said James Manyika, an advisory council member and director ofthe McKinsey Global Institute. Not only will the money be used to retaintalent, but also to fund costly data processing machines that can runartificial intelligence applications at scale.äóìThe goal is to have resources that will enable Stanford to be competitive,äóùManyika said. äóìIf you gave researchers at Stanford access to compute, thatwill slow down the brain drain quite a bit toward these corporate labs.äóù![](https://www.washingtonpost.com/resizer/TXJKur-cQ3m4AF9lqk_kkjfemkk=/3x2/www.washingtonpost.com/pb/resources/img/spacer.gif)Li is an AI pioneer and academic who also worked at Google. (Peter DaSilva forThe Washington Post)Schmidt said he had observed a äóìtipping pointäóù in the last year or so, wherecomputer science programs across the country are adding courses in AI ethicsand big companies such as Google are announcing AI principles and creatinginternal programs to attempt to take the bias out of the software they arebuilding. Schmidt said that Stanfordäó»s program would elevate and centralizethese ad hoc efforts, but also contribute to the development of the fieldoverall.One of the bigger questions HAI has yet to answer is the extent to which itwill take policy positions on some of the toughest current issues, in which Liand others involved with HAI have been directly involved. Last year, when Liwas running artificial intelligence for Google Cloud, Google became embroiledin controversy for obtaining a Pentagon contract to improve artificialintelligence that can scan video footage coming in from drones. Many Googleemployees protested the contract and some even quit.Li cautioned her colleagues against using the term AI when discussing thecontract because of the sensitivity of the topic, according to a New YorkTimes report, and confirmed by Li. Etchemendy said HAI would not take sides ordictate decisions to other organizations.Etchemendy said that 200 faculty members, from departments like law andanthropology, have already applied for funding from the think tank. Fifty-fivehave already received seed grants to research AIäó»s implications for topicsincluding medical decision-making, gender bias and refugee resettlement. Oneof the instituteäó»s biggest strengths would be its commitment to diversitywithin the profession, he said, and its recruitment of experts from fields nottraditionally associated with AI.",Artificial Intelligence,0.872,Stanford launches artificial intelligence institute,standard0,reddit_https://www.reddit.com/r/artificial/comments/atf0wr/artificial_intelligence_my_own_jarvis/,conversations_reddit,17.479641,,,"no comments yetBe the first to share what you think!",Artificial Intelligence,0.833,Artificial Intelligence - My Own Jarvis!,standard1,t3_b9exhc,conversations_reddit,17.455257,,AI,"<https://www.softtraids.com/2018/09/kirin-980-16.html>Huawei is expected to release Huawei Mate 20, on October 16. The phone will beequipped with a modern SoC Kirin 980 processor manufactured by the ChineseFoundation, an opportunity Huawei will use to highlight the possibilities ofartificial intelligence in",Artificial Intelligence,0.889,Huawei Announces KIRIN 980,standard2,t3_bc108p,conversations_reddit,17.37959,,AI,"<table> <tr><td> <a href=""https://www.reddit.com/r/MachineLearning/comments/bc108p/d_4_recommended_books_on_ai_ethics_and_philosophy/""> <img src=""https://b.thumbs.redditmedia.com/zo9dARSkvlvNVnq10JEpHnPFpOh-n8X5Db8FmiQ-e5c.jpg"" alt=""[D] 4 Recommended Books on AI Ethics and Philosophy"" title=""[D] 4 Recommended Books on AI Ethics and Philosophy"" /> </a> </td><td> <!-- SC_OFF --><div class=""md""><p>One should not only know the technology and the methods. The more artificial intelligence enters our lives, the more important ethics and philosophy become. Everyone who develops ML models bears a special challenge.</p> <p>&#x200B;</p> <p>link: <a href=""https://www.aisoma.de/4-recommended-books-on-ai-ethics-and-ai-philosophy/"">https://www.aisoma.de/4-recommended-books-on-ai-ethics-and-ai-philosophy/</a> </p> <p>&#x200B;</p> <p><a href=""https://i.redd.it/vgamjtmvhnr21.jpg"">https://i.redd.it/vgamjtmvhnr21.jpg</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=""https://www.reddit.com/user/seemingly_omniscient""> /u/seemingly_omniscient </a> <br/> <span><a href=""https://www.reddit.com/r/MachineLearning/comments/bc108p/d_4_recommended_books_on_ai_ethics_and_philosophy/"">[link]</a></span> &#32; <span><a href=""https://www.reddit.com/r/MachineLearning/comments/bc108p/d_4_recommended_books_on_ai_ethics_and_philosophy/"">[comments]</a></span> </td></tr></table>",Artificial Intelligence,1,[D] 4 Recommended Books on AI Ethics and Philosophy,standard3,t3_bq3yqz,conversations_reddit,17.37936,,,"The goal of /r/tech is to provide a space dedicated to the intelligentdiscussion of innovations and changes to technology in our ever changingworld. We focus on high quality news articles about technology and informativeand thought provoking self posts.",Artificial Intelligence,1,Scientists help artificial intelligence outsmart hackers,standard4,stackoverflow-54959340,conversations_stackoverflow,17.340616,"['python', 'machine-learning', 'nlp', 'nltk']",AI,"<p>I want to train a language model using NLTK in python but i got into several problems.first of all i don't know why my words turn into just characters as i write something like this :</p><pre><code> s = ""Natural-language processing (NLP) is an area of computer science "" \""and artificial intelligence concerned with the interactions "" \""between computers and human (natural) languages.""s = s.lower();paddedLine = pad_both_ends(word_tokenize(s),n=2);train, vocab = padded_everygram_pipeline(2, paddedLine)print(list(vocab))lm = MLE(2);lm.fit(train,vocab)</code></pre><p>and the printed vocab is something like this that is clearly not correct(i don't want to work with characters!),this is part of output.:</p><pre><code>&lt;s&gt;', '&lt;', 's', '&gt;', '&lt;/s&gt;', '&lt;s&gt;', 'n', 'a', 't', 'u', 'r', 'a', 'l', '-', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', '&lt;/s&gt;', '&lt;s&gt;', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', '&lt;/s&gt;', '&lt;s&gt;', '(', '&lt;/s&gt;', '&lt;s&gt;', 'n', 'l', 'p', '&lt;/s&gt;', '&lt;s&gt;', ')', '&lt;/s&gt;'</code></pre><p>why my input turns into characters?i did this work in another way but with no luck :</p><pre><code>paddedLine = pad_both_ends(word_tokenize(s),n=2);#train, vocab = padded_everygram_pipeline(2, tokens)#train = everygrams(paddedLine,max_len = 2);train = ngrams(paddedLine,2);vocab = Vocabulary(paddedLine,unk_cutoff = 1);print(list(train))lm = MLE(2);lm.fit(train,vocab)</code></pre><p>when i run this code my train is absolute nothing,empty! it shows me ""[]"" !!wired thing is when i comment at this line from above code:</p><pre><code>vocab = Vocabulary(paddedLine,unk_cutoff = 1);</code></pre><p>now my train data is ok and something like this that is correct :</p><pre><code>[('&lt;s&gt;', 'natural-language'), ('natural-language', 'processing'), ('processing', '('), ('(', 'nlp'), ('nlp', ')'), (')', 'is'), ('is', 'an'), ('an', 'area'), ('area', 'of'), ('of', 'computer'), ('computer', 'science'), ('science', 'and'), ('and', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'concerned'), ('concerned', 'with'), ('with', 'the'), ('the', 'interactions'), ('interactions', 'between'), ('between', 'computers'), ('computers', 'and'), ('and', 'human'), ('human', '('), ('(', 'natural'), ('natural', ')'), (')', 'languages'), ('languages', '.'), ('.', '&lt;/s&gt;')]</code></pre><p>whats wrong with it? by the way i have to say that i'm not an expert in python or NLTK and its my first experience.next question is how can i use kneser-ney smoothing or add-one smoothing on training languge model? and am i doing language model training the right way?my trainig data is simple :</p><pre><code>""Natural-language processing (NLP) is an area of computer science "" \""and artificial intelligence concerned with the interactions "" \""between computers and human (natural) languages.""</code></pre><p>thanks.</p>",Artificial Intelligence,0.966,NLTK language modeling confusion,standard5,t3_b5hega,conversations_reddit,17.274488,,AI,"<https://youtu.be/E3l_aeGjkeI>This Machine Learning tutorial will introduce you to the different areas ofMachine Learning and Artificial Intelligence within the programming languagePython. In this part of the course you will learn about the three differentlearning types (Unsupervised learning, Supervised Learning and ReinforcementLearning)",Artificial Intelligence,0.921,Python Machine Learning Tutorial Part 1 | Machine Learning For Beginners,standard6,https://news.ycombinator.com/item?id=20037520,conversations_hackernewsnew,17.243877,[],AI,"29th May 2019#Leveraging Web Development with Artificial IntelligenceWeb Development is growing dramatically and so is the use of ArtificialIntelligence. Todayäó»s web development is focused mainly on enhancing userexperience and AI is, thus, the perfect match for it.Artificial Intelligence (AI) is the ability of a computer or a machine tothink and work just like humans. AI is based on algorithms which use computingpower to solve specific problems faster and better than humans can. AI allowsthe machines to analyze, plan, learn and adapt. AI relies on the data that isprovided to it and interprets this data to create new perspectives and learnnew objectives. It then leverages this data to provide better and moreaccurate results in the future.Let us now analyze how Artificial Intelligence and Machine Learning can beused in Web Development and provide you with an exceptional advantage overyour competitors:![](https://i1.wp.com/www.atyantik.com/wp-content/uploads/2019/05/60847875_2340618609596223_6147071032143380480_n.png?fit=640%2C328&ssl=1)1. **User Experience** äóñ AI and ML can be used to enhance the experience of your website or app visitors. These visitors are often potential customers and they are more likely to purchase a product if they are provided with easy-to-use interface which enables them to find specific products faster.2. **Improved Predictive Response** äóñ traditional chatbots were boring, too predictive and sometimes even useless. But the modern AI-powered chatbots are super-intelligent. They know how to quickly and accurately respond to exact customer queries and needs making the whole experience delightful for the consumers.3. **Personalized Content** äóñ Machine Learning is able to predict the intentions of the users based on their previous activities and can hence be used to show content to the audience that is best tailored to their needs. Users are shown with music, videos, tv shows, and even products that they are most likely to interact with.4. **Voice-based Search** äóñ Voice Recognition and interpretation is one of the prodigies of Machine Learning and has revolutionized the way users interact with applications. Integrating web development allows customers to shop faster and smarter.5. **Unique Store Experience** äóî AI can be used by retailers to gain customer data and get insights on customer behaviour. This data can then be used to offer personalized and customized shopping recommendations, products, deals and much more to the users. This helps boost your sales as your customers will more likely buy products from your website rather than from a competitor.#### Case Study of Amazon and Netflix implementing AI and ML in theirRecommendation EnginesHave you ever wondered why the product you see on Amazon is exactly what youwere looking for? Well, Amazon **** has been using AI techniques to tailor theproducts and content it recommends to its customers. Their recommendationsystem uses **goods-based recommendation äóñ** which means that users arerecommended based on their previous interactions and purchases and **buddy-based recommendation äóñ** which means that users are recommended products andcontent based on what their Facebook or Instagram friends like.![Netflix Logo](https://i0.wp.com/www.atyantik.com/wp-content/uploads/2019/05/netflix.png?resize=253%2C142&ssl=1)**Netflix** , on the other hand, revamped their recommendation algorithms andhoned them for visual impressions. Their AI and ML algorithms are optimized toprovide users with the image that they are most likely to respond to. Theyfeed implicit (based on user behavior) and explicit (based on user activity)data to their Machine Learning algorithms to help it figure out the mostrelevant content for each individual user. It is constantly collecting datafrom its 100 million subscribers to make its AI and ML algorithms better andbetter every day.",Artificial Intelligence,1,Leveraging Web Development with Artificial Intelligence,standard7,https://news.ycombinator.com/item?id=19489280,conversations_hackernewsnew,17.221594,[],AI,"Bullying has always been a problem in schools. Over the last few years, itäó»sfinally getting taken seriously and given proper attention.Research has shown that when adults respond quickly, bullying behavior can bestopped over time. Unfortunately, teachers donäó»t always see the signs untilitäó»s too late. One school district in Japan is attempting to give teachers andofficials [a powerfulally](http://mainichi.jp/english/articles/20190323/p2g/00m/0na/063000c) incombating the problem.## Otsu Partners with Hitachi to Analyze 9000 Cases of BullyingThe western Japanese city of Otsu recently signed an accord with[Hitachi](http://www.hitachi.us/) to collaborate on a project. They plan touse artificial intelligence and machine learning to help detect signs andpatterns of aggression.AI will analyze 9000 cases of bullying that occurred in local elementary andjunior high schools over the past 6 years. It will look at things such asgrade, gender, timing, location, number of students involved and academicrecords.The goal of the project is to help teachers detect characteristics and earlywarning signs. Often minor issues between students can lead to biggerproblems. If signs are recognized early enough, teachers can squash bullyingbefore it occurs.äóìLocal schools are expected act firmly against (bullying) without solely beingdependent on teachersäó» experience, by having AI theoretically analyze pastdata.äóù said Otsu Mayor Naomi Koshi.### Otsu Mobilized by TragedyThe school board expects the analysis to be completed by October. Otsusuffered tragedy in 2011 when a 13 year-old boy committed suicide. After a twoyear investigation, it was determined to have been caused by bullying. Sincethen, the city has required each school to report bullying cases within 24hours.Itäó»s always good to see artificial intelligence being used for good. If AI canhelp stop even one case of bullying then the project would be a success. Welook forward to more schools using technology to combat aggression and helpstudents.* * *Check out our articles on [AI making policydecisions](https://yellrobot.com/artificial-intelligence-politics-decisions-europe-ai/) and a [gourmet coffee robot.](https://yellrobot.com/briggo-coffee-robot-coffee-haus/)",Artificial Intelligence,1,Using Artificial Intelligence to Combat Bullying,standard8,https://news.ycombinator.com/item?id=19481158,conversations_hackernewsnew,17.221594,[],AI,"The approach is related to traditional simulation, but with criticaldifferences. A simulation is äóìessentially assumption-driven,äóù Schawinski said.äóìThe approach is to say, äóÖI think I know what the underlying physical laws arethat give rise to everything that I see in the system.äó» So I have a recipe forstar formation, I have a recipe for how dark matter behaves, and so on. I putall of my hypotheses in there, and I let the simulation run. And then I ask:Does that look like reality?äóù What heäó»s done with generative modeling, hesaid, is äóìin some sense, exactly the opposite of a simulation. We donäó»t knowanything; we donäó»t want to assume anything. We want the data itself to tell uswhat might be going on.äóùThe apparent success of generative modeling in a study like this obviouslydoesnäó»t mean that astronomers and graduate students have been made redundant äóîbut it appears to represent a shift in the degree to which learning aboutastrophysical objects and processes can be achieved by an artificial systemthat has little more at its electronic fingertips than a vast pool of data.äóìItäó»s not fully automated science äóî but it demonstrates that weäó»re capable ofat least in part building the tools that make the process of scienceautomatic,äóù Schawinski said.Generative modeling is clearly powerful, but whether it truly represents a newapproach to science is open to debate. For [DavidHogg](https://cosmo.nyu.edu/hogg/), a cosmologist at New York University andthe Flatiron Institute (which, like _Quanta_ , is funded by the SimonsFoundation), the technique is impressive but ultimately just a verysophisticated way of extracting patterns from data äóî which is what astronomershave been doing for centuries. In other words, itäó»s an advanced form ofobservation plus analysis. Hoggäó»s own work, like Schawinskiäó»s, leans heavilyon AI; heäó»s been using neural networks to [classifystars](https://arxiv.org/pdf/1711.08793.pdf) according to their spectra and to[infer other physical attributes](https://arxiv.org/pdf/1603.03040.pdf) ofstars using data-driven models. But he sees his work, as well as Schawinskiäó»s,as tried-and-true science. äóìI donäó»t think itäó»s a third way,äóù he said recently.äóìI just think we as a community are becoming far more sophisticated about howwe use the data. In particular, we are getting much better at comparing datato data. But in my view, my work is still squarely in the observational mode.äóù## Hardworking AssistantsWhether theyäó»re conceptually novel or not, itäó»s clear that AI and neuralnetworks have come to play a critical role in contemporary astronomy andphysics research. At the Heidelberg Institute for Theoretical Studies, thephysicist [KaiPolsterer](https://www.iau.org/administration/membership/individual/16830/)heads the astroinformatics group äóî a team of researchers focused on new, data-centered methods of doing astrophysics. Recently, theyäó»ve been using amachine-learning algorithm to [extract redshift information from galaxy datasets](https://www.aanda.org/articles/aa/pdf/2018/01/aa31326-17.pdf), apreviously arduous task.Polsterer sees these new AI-based systems as äóìhardworking assistantsäóù that cancomb through data for hours on end without getting bored or complaining aboutthe working conditions. These systems can do all the tedious grunt work, hesaid, leaving you äóìto do the cool, interesting science on your own.äóùBut theyäó»re not perfect. In particular, Polsterer cautions, the algorithms canonly do what theyäó»ve been trained to do. The system is äóìagnosticäóù regardingthe input. Give it a galaxy, and the software can estimate its redshift andits age äóî but feed that same system a selfie, or a picture of a rotting fish,and it will output a (very wrong) age for that, too. In the end, oversight bya human scientist remains essential, he said. äóìIt comes back to you, theresearcher. Youäó»re the one in charge of doing the interpretation.äóùFor his part, Nord, at Fermilab, cautions that itäó»s crucial that neuralnetworks deliver not only results, but also error bars to go along with them,as every undergraduate is trained to do. In science, if you make a measurementand donäó»t report an estimate of the associated error, no one will take theresults seriously, he said.Like many AI researchers, Nord is also concerned about the impenetrability ofresults produced by neural networks; often, a system delivers an answerwithout offering a clear picture of how that result was obtained.Yet not everyone feels that a lack of transparency is necessarily a problem.[Lenka Zdeborovíç](http://artax.karlin.mff.cuni.cz/~zdebl9am/), a researcher atthe Institute of Theoretical Physics at CEA Saclay in France, points out thathuman intuitions are often equally impenetrable. You look at a photograph andinstantly recognize a cat äóî äóìbut you donäó»t know how you know,äóù she said. äóìYourown brain is in some sense a black box.äóùItäó»s not only astrophysicists and cosmologists who are migrating toward AI-fueled, data-driven science. Quantum physicists like [RogerMelko](https://uwaterloo.ca/physics-astronomy/people-profiles/roger-melko) ofthe Perimeter Institute for Theoretical Physics and the University of Waterlooin Ontario have used neural networks to solve some of the toughest and mostimportant problems in that field, such as [how to represent the mathematicaläóìwave functionäóù](https://arxiv.org/pdf/1812.09329.pdf) describing a many-particle system. AI is essential because of what Melko calls äóìthe exponentialcurse of dimensionality.äóù That is, the possibilities for the form of a wavefunction grow exponentially with the number of particles in the system itdescribes. The difficulty is similar to trying to work out the best move in agame like chess or Go: You try to peer ahead to the next move, imagining whatyour opponent will play, and then choose the best response, but with eachmove, the number of possibilities proliferates.Of course, AI systems have mastered both of these games äóî chess, decades ago,and Go in 2016, when an AI system called[AlphaGo](https://www.theguardian.com/technology/2016/mar/15/googles-alphago-seals-4-1-victory-over-grandmaster-lee-sedol) defeated a top human player.They are similarly suited to problems in quantum physics, Melko says.## The Mind of the MachineWhether Schawinski is right in claiming that heäó»s found a äóìthird wayäóù of doingscience, or whether, as Hogg says, itäó»s merely traditional observation anddata analysis äóìon steroids,äóù itäó»s clear AI is changing the flavor ofscientific discovery, and itäó»s certainly accelerating it. How far will the AIrevolution go in science?Occasionally, grand claims are made regarding the achievements of a äóìrobo-scientist.äóù A decade ago, an AI robot chemist named Adam investigated thegenome of bakeräó»s yeast and worked out which genes are responsible for makingcertain amino acids. (Adam did this by observing strains of yeast that hadcertain genes missing, and comparing the results to the behavior of strainsthat had the genes.) _Wired_ äó»s headline read, äóì[Robot Makes ScientificDiscovery All by Itself](https://www.wired.com/2009/04/robotscientist/).äóùMore recently, Lee Cronin, a chemist at the University of Glasgow, has beenusing a robot [to randomly mixchemicals](https://www.wired.co.uk/article/robot-chemist-life-on-earth), tosee what sorts of new compounds are formed. Monitoring the reactions in real-time with a mass spectrometer, a nuclear magnetic resonance machine, and aninfrared spectrometer, the system eventually learned to predict whichcombinations would be the most reactive. Even if it doesnäó»t lead to furtherdiscoveries, Cronin has said, the robotic system could allow chemists to speedup their research by about 90 percent.Last year, another team of scientists at ETH Zurich used neural networks to[deduce physical laws](https://arxiv.org/abs/1807.10300) from sets of data.Their system, a sort of robo-Kepler, rediscovered the heliocentric model ofthe solar system from records of the position of the sun and Mars in the sky,as seen from Earth, and figured out the law of conservation of momentum byobserving colliding balls. Since physical laws can often be expressed in morethan one way, the researchers wonder if the system might offer new ways äóîperhaps simpler ways äóî of thinking about known laws.These are all examples of AI kick-starting the process of scientificdiscovery, though in every case, we can debate just how revolutionary the newapproach is. Perhaps most controversial is the question of how muchinformation can be gleaned from data alone äóî a pressing question in the age ofstupendously large (and growing) piles of it. In _The Book of Why_ (2018), thecomputer scientist Judea Pearl and the science writer Dana Mackenzie assertthat data are äóìprofoundly dumb.äóù Questions about causality äóìcan never beanswered from data alone,äóù they write. äóìAnytime you see a paper or a studythat analyzes the data in a model-free way, you can be certain that the outputof the study will merely summarize, and perhaps transform, but not interpretthe data.äóù Schawinski sympathizes with Pearläó»s position, but he described theidea of working with äóìdata aloneäóù as äóìa bit of a straw man.äóù Heäó»s neverclaimed to deduce cause and effect that way, he said. äóìIäó»m merely saying wecan do more with data than we often conventionally do.äóùAnother oft-heard argument is that science requires creativity, and that äóî atleast so far äóî we have no idea how to program that into a machine. (Simplytrying everything, like Croninäó»s robo-chemist, doesnäó»t seem especiallycreative.) äóìComing up with a theory, with reasoning, I think demandscreativity,äóù Polsterer said. äóìEvery time you need creativity, you will need ahuman.äóù And where does creativity come from? Polsterer suspects it is relatedto boredom äóî something that, he says, a machine cannot experience. äóìTo becreative, you have to dislike being bored. And I donäó»t think a computer willever feel bored.äóù On the other hand, words like äóìcreativeäóù and äóìinspiredäóù haveoften been used to describe programs like Deep Blue and AlphaGo. And thestruggle to describe what goes on inside the äóìmindäóù of a machine is mirroredby the difficulty we have in probing our own thought processes.Schawinski recently left academia for the private sector; he now runs astartup called Modulos which employs a number of ETH scientists and, accordingto its website, works äóìin the eye of the storm of developments in AI andmachine learning.äóù Whatever obstacles may lie between current AI technologyand full-fledged artificial minds, he and other experts feel that machines arepoised to do more and more of the work of human scientists. Whether there is alimit remains to be seen.äóìWill it be possible, in the foreseeable future, to build a machine that candiscover physics or mathematics that the brightest humans alive are not ableto do on their own, using biological hardware?äóù Schawinski wonders. äóìWill thefuture of science eventually necessarily be driven by machines that operate ona level that we can never reach? I donäó»t know. Itäó»s a good question.äóù",Artificial Intelligence,1,How Artificial Intelligence Is Changing Science,standard9,reddit_https://www.reddit.com/r/artificial/comments/asnhvk/common_techniques_used_in_artificial_intelligence/,conversations_reddit,17.050856,,,"no comments yetBe the first to share what you think!",Artificial Intelligence,0.833,Common Techniques Used in Artificial Intelligence,standard10,https://news.ycombinator.com/item?id=19446714,conversations_hackernewsnew,17.025965,[],AI,"The age of artificial intelligence (AI) has arrived, and is transformingeverything from healthcare to transportation to manufacturing.America has long been the global leader in this new era of AI, and is poisedto maintain this leadership going forward. Realizing the full potential of AIfor the Nation requires the combined efforts of industry, academia, andgovernment. The Administration has been active in developing policies andimplementing strategies that accelerate AI innovation in the U.S. for thebenefit of the American people. These activities align with four main pillarsof emphasis: AI for American Innovation, AI for American Industry, AI for theAmerican Worker, and AI with American Values. This AI.gov website provides aportal for exploring these activities in more depth, and serves as a resourcefor those who want to learn more about how to take full advantage of theopportunities of AI.",Artificial Intelligence,1,Artificial Intelligence for the American People,standard11,https://news.ycombinator.com/item?id=19966355,conversations_hackernewsnew,17.025965,[],AI,"# Title:Integrating Artificial Intelligence into Weapon Systems(Submitted on 10 May 2019)> Abstract: The integration of Artificial Intelligence (AI) into weaponsystems is one of the most consequential tactical and strategic decisions inthe history of warfare. Current AI development is a remarkable combination ofaccelerating capability, hidden decision mechanisms, and decreasing costs.Implementation of these systems is in its infancy and exists on a spectrumfrom resilient and flexible to simplistic and brittle. Resilient systemsshould be able to effectively handle the complexities of a high-dimensionalbattlespace. Simplistic AI implementations could be manipulated by anadversarial AI that identifies and exploits their weaknesses. > In this paper, we present a framework for understanding the development ofdynamic AI/ML systems that interactively and continuously adapt to theiruser's needs. We explore the implications of increasingly capable AI in thekill chain and how this will lead inevitably to a fully automated, always onsystem, barring regulation by treaty. We examine the potential of totalintegration of cyber and physical security and how this likelihood must informthe development of AI-enabled systems with respect to the ""fog of war"", humanmorals, and ethics.## Submission historyFrom: Aaron Massey [[view email](/show-email/69a90b83/1905.03899)]**[v1]**Fri, 10 May 2019 00:38:35 UTC (3,958 KB)",Artificial Intelligence,1,Integrating Artificial Intelligence into Weapon Systems,standard12,reddit_https://www.reddit.com/r/eos/comments/at3oqb/effectai_takes_artificial_intelligence_to_eos/,conversations_reddit,16.959984,,,"**What is EOS?**EOS is an open-source distributed blockchain operating system with a focus onbringing decentralized applications to the masses. The vision of EOS is thateveryday users will, in the near future, be able to run dapps from mobiledevices with no specialized knowledge - just as they currently do with appsdownloaded from the App Store.**Quick Links:****Other Links:****Related Subreddit:**",Artificial Intelligence:Cloud,0.71:0.968,Effect.AI takes Artificial Intelligence to EOS Blockchain,standard13,t3_bm66x7,conversations_reddit,16.959984,,,"No petitions, surveys, or crowdfunding",No tags,,Artificial Intelligence May Not 'Hallucinate' After All,standard14,https://news.ycombinator.com/item?id=19462432,conversations_hackernewsnew,16.914877,[],AI,"The approach is related to traditional simulation, but with criticaldifferences. A simulation is äóìessentially assumption-driven,äóù Schawinski said.äóìThe approach is to say, äóÖI think I know what the underlying physical laws arethat give rise to everything that I see in the system.äó» So I have a recipe forstar formation, I have a recipe for how dark matter behaves, and so on. I putall of my hypotheses in there, and I let the simulation run. And then I ask:Does that look like reality?äóù What heäó»s done with generative modeling, hesaid, is äóìin some sense, exactly the opposite of a simulation. We donäó»t knowanything; we donäó»t want to assume anything. We want the data itself to tell uswhat might be going on.äóùThe apparent success of generative modeling in a study like this obviouslydoesnäó»t mean that astronomers and graduate students have been made redundant äóîbut it appears to represent a shift in the degree to which learning aboutastrophysical objects and processes can be achieved by an artificial systemthat has little more at its electronic fingertips than a vast pool of data.äóìItäó»s not fully automated science äóî but it demonstrates that weäó»re capable ofat least in part building the tools that make the process of scienceautomatic,äóù Schawinski said.Generative modeling is clearly powerful, but whether it truly represents a newapproach to science is open to debate. For [DavidHogg](https://cosmo.nyu.edu/hogg/), a cosmologist at New York University andthe Flatiron Institute (which, like _Quanta_ , is funded by the SimonsFoundation), the technique is impressive but ultimately just a verysophisticated way of extracting patterns from data äóî which is what astronomershave been doing for centuries. In other words, itäó»s an advanced form ofobservation plus analysis. Hoggäó»s own work, like Schawinskiäó»s, leans heavilyon AI; heäó»s been using neural networks to [classifystars](https://arxiv.org/pdf/1711.08793.pdf) according to their spectra and to[infer other physical attributes](https://arxiv.org/pdf/1603.03040.pdf) ofstars using data-driven models. But he sees his work, as well as Schawinskiäó»s,as tried-and-true science. äóìI donäó»t think itäó»s a third way,äóù he said recently.äóìI just think we as a community are becoming far more sophisticated about howwe use the data. In particular, we are getting much better at comparing datato data. But in my view, my work is still squarely in the observational mode.äóù## Hardworking AssistantsWhether theyäó»re conceptually novel or not, itäó»s clear that AI and neuralnetworks have come to play a critical role in contemporary astronomy andphysics research. At the Heidelberg Institute for Theoretical Studies, thephysicist [KaiPolsterer](https://www.iau.org/administration/membership/individual/16830/)heads the astroinformatics group äóî a team of researchers focused on new, data-centered methods of doing astrophysics. Recently, theyäó»ve been using amachine-learning algorithm to [extract redshift information from galaxy datasets](https://www.aanda.org/articles/aa/pdf/2018/01/aa31326-17.pdf), apreviously arduous task.Polsterer sees these new AI-based systems as äóìhardworking assistantsäóù that cancomb through data for hours on end without getting bored or complaining aboutthe working conditions. These systems can do all the tedious grunt work, hesaid, leaving you äóìto do the cool, interesting science on your own.äóùBut theyäó»re not perfect. In particular, Polsterer cautions, the algorithms canonly do what theyäó»ve been trained to do. The system is äóìagnosticäóù regardingthe input. Give it a galaxy, and the software can estimate its redshift andits age äóî but feed that same system a selfie, or a picture of a rotting fish,and it will output a (very wrong) age for that, too. In the end, oversight bya human scientist remains essential, he said. äóìIt comes back to you, theresearcher. Youäó»re the one in charge of doing the interpretation.äóùFor his part, Nord, at Fermilab, cautions that itäó»s crucial that neuralnetworks deliver not only results, but also error bars to go along with them,as every undergraduate is trained to do. In science, if you make a measurementand donäó»t report an estimate of the associated error, no one will take theresults seriously, he said.Like many AI researchers, Nord is also concerned about the impenetrability ofresults produced by neural networks; often, a system delivers an answerwithout offering a clear picture of how that result was obtained.Yet not everyone feels that a lack of transparency is necessarily a problem.[Lenka Zdeborovíç](http://artax.karlin.mff.cuni.cz/~zdebl9am/), a researcher atthe Institute of Theoretical Physics at CEA Saclay in France, points out thathuman intuitions are often equally impenetrable. You look at a photograph andinstantly recognize a cat äóî äóìbut you donäó»t know how you know,äóù she said. äóìYourown brain is in some sense a black box.äóùItäó»s not only astrophysicists and cosmologists who are migrating toward AI-fueled, data-driven science. Quantum physicists like [RogerMelko](https://uwaterloo.ca/physics-astronomy/people-profiles/roger-melko) ofthe Perimeter Institute for Theoretical Physics and the University of Waterlooin Ontario have used neural networks to solve some of the toughest and mostimportant problems in that field, such as [how to represent the mathematicaläóìwave functionäóù](https://arxiv.org/pdf/1812.09329.pdf) describing a many-particle system. AI is essential because of what Melko calls äóìthe exponentialcurse of dimensionality.äóù That is, the possibilities for the form of a wavefunction grow exponentially with the number of particles in the system itdescribes. The difficulty is similar to trying to work out the best move in agame like chess or Go: You try to peer ahead to the next move, imagining whatyour opponent will play, and then choose the best response, but with eachmove, the number of possibilities proliferates.Of course, AI systems have mastered both of these games äóî chess, decades ago,and Go in 2016, when an AI system called[AlphaGo](https://www.theguardian.com/technology/2016/mar/15/googles-alphago-seals-4-1-victory-over-grandmaster-lee-sedol) defeated a top human player.They are similarly suited to problems in quantum physics, Melko says.## The Mind of the MachineWhether Schawinski is right in claiming that heäó»s found a äóìthird wayäóù of doingscience, or whether, as Hogg says, itäó»s merely traditional observation anddata analysis äóìon steroids,äóù itäó»s clear AI is changing the flavor ofscientific discovery, and itäó»s certainly accelerating it. How far will the AIrevolution go in science?Occasionally, grand claims are made regarding the achievements of a äóìrobo-scientist.äóù A decade ago, an AI robot chemist named Adam investigated thegenome of bakeräó»s yeast and worked out which genes are responsible for makingcertain amino acids. (Adam did this by observing strains of yeast that hadcertain genes missing, and comparing the results to the behavior of strainsthat had the genes.) _Wired_ äó»s headline read, äóì[Robot Makes ScientificDiscovery All by Itself](https://www.wired.com/2009/04/robotscientist/).äóùMore recently, Lee Cronin, a chemist at the University of Glasgow, has beenusing a robot [to randomly mixchemicals](https://www.wired.co.uk/article/robot-chemist-life-on-earth), tosee what sorts of new compounds are formed. Monitoring the reactions in real-time with a mass spectrometer, a nuclear magnetic resonance machine, and aninfrared spectrometer, the system eventually learned to predict whichcombinations would be the most reactive. Even if it doesnäó»t lead to furtherdiscoveries, Cronin has said, the robotic system could allow chemists to speedup their research by about 90 percent.Last year, another team of scientists at ETH Zurich used neural networks to[deduce physical laws](https://arxiv.org/abs/1807.10300) from sets of data.Their system, a sort of robo-Kepler, rediscovered the heliocentric model ofthe solar system from records of the position of the sun and Mars in the sky,as seen from Earth, and figured out the law of conservation of momentum byobserving colliding balls. Since physical laws can often be expressed in morethan one way, the researchers wonder if the system might offer new ways äóîperhaps simpler ways äóî of thinking about known laws.These are all examples of AI kick-starting the process of scientificdiscovery, though in every case, we can debate just how revolutionary the newapproach is. Perhaps most controversial is the question of how muchinformation can be gleaned from data alone äóî a pressing question in the age ofstupendously large (and growing) piles of it. In _The Book of Why_ (2018), thecomputer scientist Judea Pearl and the science writer Dana Mackenzie assertthat data are äóìprofoundly dumb.äóù Questions about causality äóìcan never beanswered from data alone,äóù they write. äóìAnytime you see a paper or a studythat analyzes the data in a model-free way, you can be certain that the outputof the study will merely summarize, and perhaps transform, but not interpretthe data.äóù Schawinski sympathizes with Pearläó»s position, but he described theidea of working with äóìdata aloneäóù as äóìa bit of a straw man.äóù Heäó»s neverclaimed to deduce cause and effect that way, he said. äóìIäó»m merely saying wecan do more with data than we often conventionally do.äóùAnother oft-heard argument is that science requires creativity, and that äóî atleast so far äóî we have no idea how to program that into a machine. (Simplytrying everything, like Croninäó»s robo-chemist, doesnäó»t seem especiallycreative.) äóìComing up with a theory, with reasoning, I think demandscreativity,äóù Polsterer said. äóìEvery time you need creativity, you will need ahuman.äóù And where does creativity come from? Polsterer suspects it is relatedto boredom äóî something that, he says, a machine cannot experience. äóìTo becreative, you have to dislike being bored. And I donäó»t think a computer willever feel bored.äóù On the other hand, words like äóìcreativeäóù and äóìinspiredäóù haveoften been used to describe programs like Deep Blue and AlphaGo. And thestruggle to describe what goes on inside the äóìmindäóù of a machine is mirroredby the difficulty we have in probing our own thought processes.Schawinski recently left academia for the private sector; he now runs astartup called Modulos which employs a number of ETH scientists and, accordingto its website, works äóìin the eye of the storm of developments in AI andmachine learning.äóù Whatever obstacles may lie between current AI technologyand full-fledged artificial minds, he and other experts feel that machines arepoised to do more and more of the work of human scientists. Whether there is alimit remains to be seen.äóìWill it be possible, in the foreseeable future, to build a machine that candiscover physics or mathematics that the brightest humans alive are not ableto do on their own, using biological hardware?äóù Schawinski wonders. äóìWill thefuture of science eventually necessarily be driven by machines that operate ona level that we can never reach? I donäó»t know. Itäó»s a good question.äóù",Artificial Intelligence,1,How Artificial Intelligence Is Changing Science,standard15,https://news.ycombinator.com/item?id=19952888,conversations_hackernewsnew,16.914877,[],AI,"An artificial intelligence (AI) trained on the photos of a dog, crab, and duck(top) would be vulnerable to deception because these photos contain subtlefeatures that could be manipulated. The images on the bottom row donäó»t containthese subtle features, and are thus better for training secure AI.Ilyas, Santurkar, Tsipras, Engstrom, Tran, Madry# Scientists help artificial intelligence outsmart hackersBy [Matthew Hutson](/author/matthew-hutson)May. 14, 2019 , 12:45 PM**NEW ORLEANS, LOUISIANAäóî** A hacked message in a streamed song makes Alexasend money to a foreign entity. A self-driving car crashes after a pranksterstrategically places stickers on a stop sign so the car misinterprets it as aspeed limit sign. Fortunately these havenäó»t happened yet, but hacks like this,sometimes called [adversarialattacks](https://www.sciencemag.org/news/2018/07/turtle-or-rifle-hackers-easily-fool-ais-seeing-wrong-thing), could become commonplaceäóîunlessartificial intelligence (AI) finds a way to outsmart them. Now, researchershave found a new way to give AI a defensive edge, they reported here last weekat the International Conference on Learning Representations.The work could not only protect the public. It also helps reveal why AI,notoriously difficult to understand, falls victim to such attacks in the firstplace, says Zico Kolter, a computer scientist at Carnegie Mellon University,in Pittsburgh, Pennsylvania, who was not involved in the research. Becausesome AIs are too smart for their own good, spotting patterns in images thathumans canäó»t, they are vulnerable to those patterns and need to be trainedwith that in mind, the research suggests.To identify this vulnerability, researchers created a special set of trainingdata: images that look to us like one thing, but look to AI like anotheräóîapicture of a dog, for example, that, on close examination by a computer, hascatlike fur. Then the team mislabeled the picturesäóîcalling the dog picture animage of a cat, for exampleäóîand trained an algorithm to learn the labels. Oncethe AI had learned to see dogs with subtle cat features as cats, they testedit by asking it to recognize fresh, unmodified images. Even though the AI hadbeen trained in this odd way, it could correctly identify actual dogs, cats,and so on nearly half the time. In essence, it had learned to match the subtlefeatures with labels, whatever the obvious features.The training experiment suggests AIs use two types of features: obvious, macroones like ears and tails that people recognize, and micro ones that we canonly guess at. It further suggests adversarial attacks arenäó»t just confusingan AI with meaningless tweaks to an image. In those tweaks, the AI is smartlyseeing traces of something else. An AI might see a stop sign as a speed limitsign, for example, because something about the stickers actually makes itsubtly resemble a speed limit sign in a way that humans are too oblivious tocomprehend.Some in the AI field suspected this was the case, but itäó»s good to have aresearch paper showing it, Kolter says. Bo Li, a computer scientist at theUniversity of Illinois in Champaign who was not involved in the work, saysdistinguishing apparent from hidden features is a äóìuseful and good researchdirection,äóù but that äóìthere is still a long wayäóù to doing so efficiently.So now that researchers have a better idea of why AI makes such mistakes, canthat be used to help them outsmart adversarial attacks? Andrew Ilyas, acomputer scientist at the Massachusetts Institute of Technology (MIT) inCambridge, and one of the paperäó»s authors, says engineers could change the waythey train AI. Current methods of securing an algorithm against attacks areslow and difficult. But if you modify the training data to have only human-obvious features, any algorithm trained on it wonäó»t recognizeäóîand be fooledbyäóîadditional, perhaps subtler, features.And, indeed, when the team trained an algorithm on images without the subtlefeatures, [their image recognition software was fooled by adversarial attacksonly 50% of the time](https://arxiv.org/abs/1905.02175), the researchersreported at the conference and in a preprint paper posted online last week.That compares with a 95% rate of vulnerability when the AI was trained onimages with both obvious and subtle patterns.Overall, the findings suggest an AIäó»s vulnerabilities lie in its trainingdata, not its programming, says Dimitris Tsipras of MIT, a co-author.According to Kolter, äóìOne of the things this paper does really nicely is itdrives that point home with very clear examplesäóùäóîlike the demonstration thatapparently mislabeled training data can still make for successfultrainingäóîäóìthat make this connection very visceral.äóù",Artificial Intelligence,1,Scientists help artificial intelligence outsmart hackers,standard16,https://news.ycombinator.com/item?id=19404345,conversations_hackernewsnew,16.914877,[],AI,"The approach is related to traditional simulation, but with criticaldifferences. A simulation is äóìessentially assumption-driven,äóù Schawinski said.äóìThe approach is to say, äóÖI think I know what the underlying physical laws arethat give rise to everything that I see in the system.äó» So I have a recipe forstar formation, I have a recipe for how dark matter behaves, and so on. I putall of my hypotheses in there, and I let the simulation run. And then I ask:Does that look like reality?äóù What heäó»s done with generative modeling, hesaid, is äóìin some sense, exactly the opposite of a simulation. We donäó»t knowanything; we donäó»t want to assume anything. We want the data itself to tell uswhat might be going on.äóùThe apparent success of generative modeling in a study like this obviouslydoesnäó»t mean that astronomers and graduate students have been made redundant äóîbut it appears to represent a shift in the degree to which learning aboutastrophysical objects and processes can be achieved by an artificial systemthat has little more at its electronic fingertips than a vast pool of data.äóìItäó»s not fully automated science äóî but it demonstrates that weäó»re capable ofat least in part building the tools that make the process of scienceautomatic,äóù Schawinski said.Generative modeling is clearly powerful, but whether it truly represents a newapproach to science is open to debate. For [DavidHogg](https://cosmo.nyu.edu/hogg/), a cosmologist at New York University andthe Flatiron Institute (which, like _Quanta_ , is funded by the SimonsFoundation), the technique is impressive but ultimately just a verysophisticated way of extracting patterns from data äóî which is what astronomershave been doing for centuries. In other words, itäó»s an advanced form ofobservation plus analysis. Hoggäó»s own work, like Schawinskiäó»s, leans heavilyon AI; heäó»s been using neural networks to [classifystars](https://arxiv.org/pdf/1711.08793.pdf) according to their spectra and to[infer other physical attributes](https://arxiv.org/pdf/1603.03040.pdf) ofstars using data-driven models. But he sees his work, as well as Schawinskiäó»s,as tried-and-true science. äóìI donäó»t think itäó»s a third way,äóù he said recently.äóìI just think we as a community are becoming far more sophisticated about howwe use the data. In particular, we are getting much better at comparing datato data. But in my view, my work is still squarely in the observational mode.äóù## Hardworking AssistantsWhether theyäó»re conceptually novel or not, itäó»s clear that AI and neuralnetworks have come to play a critical role in contemporary astronomy andphysics research. At the Heidelberg Institute for Theoretical Studies, thephysicist [KaiPolsterer](https://www.iau.org/administration/membership/individual/16830/)heads the astroinformatics group äóî a team of researchers focused on new, data-centered methods of doing astrophysics. Recently, theyäó»ve been using amachine-learning algorithm to [extract redshift information from galaxy datasets](https://www.aanda.org/articles/aa/pdf/2018/01/aa31326-17.pdf), apreviously arduous task.Polsterer sees these new AI-based systems as äóìhardworking assistantsäóù that cancomb through data for hours on end without getting bored or complaining aboutthe working conditions. These systems can do all the tedious grunt work, hesaid, leaving you äóìto do the cool, interesting science on your own.äóùBut theyäó»re not perfect. In particular, Polsterer cautions, the algorithms canonly do what theyäó»ve been trained to do. The system is äóìagnosticäóù regardingthe input. Give it a galaxy, and the software can estimate its redshift andits age äóî but feed that same system a selfie, or a picture of a rotting fish,and it will output a (very wrong) age for that, too. In the end, oversight bya human scientist remains essential, he said. äóìIt comes back to you, theresearcher. Youäó»re the one in charge of doing the interpretation.äóùFor his part, Nord, at Fermilab, cautions that itäó»s crucial that neuralnetworks deliver not only results, but also error bars to go along with them,as every undergraduate is trained to do. In science, if you make a measurementand donäó»t report an estimate of the associated error, no one will take theresults seriously, he said.Like many AI researchers, Nord is also concerned about the impenetrability ofresults produced by neural networks; often, a system delivers an answerwithout offering a clear picture of how that result was obtained.Yet not everyone feels that a lack of transparency is necessarily a problem.[Lenka Zdeborovíç](http://artax.karlin.mff.cuni.cz/~zdebl9am/), a researcher atthe Institute of Theoretical Physics at CEA Saclay in France, points out thathuman intuitions are often equally impenetrable. You look at a photograph andinstantly recognize a cat äóî äóìbut you donäó»t know how you know,äóù she said. äóìYourown brain is in some sense a black box.äóùItäó»s not only astrophysicists and cosmologists who are migrating toward AI-fueled, data-driven science. Quantum physicists like [RogerMelko](https://uwaterloo.ca/physics-astronomy/people-profiles/roger-melko) ofthe Perimeter Institute for Theoretical Physics and the University of Waterlooin Ontario have used neural networks to solve some of the toughest and mostimportant problems in that field, such as [how to represent the mathematicaläóìwave functionäóù](https://arxiv.org/pdf/1812.09329.pdf) describing a many-particle system. AI is essential because of what Melko calls äóìthe exponentialcurse of dimensionality.äóù That is, the possibilities for the form of a wavefunction grow exponentially with the number of particles in the system itdescribes. The difficulty is similar to trying to work out the best move in agame like chess or Go: You try to peer ahead to the next move, imagining whatyour opponent will play, and then choose the best response, but with eachmove, the number of possibilities proliferates.Of course, AI systems have mastered both of these games äóî chess, decades ago,and Go in 2016, when an AI system called[AlphaGo](https://www.theguardian.com/technology/2016/mar/15/googles-alphago-seals-4-1-victory-over-grandmaster-lee-sedol) defeated a top human player.They are similarly suited to problems in quantum physics, Melko says.## The Mind of the MachineWhether Schawinski is right in claiming that heäó»s found a äóìthird wayäóù of doingscience, or whether, as Hogg says, itäó»s merely traditional observation anddata analysis äóìon steroids,äóù itäó»s clear AI is changing the flavor ofscientific discovery, and itäó»s certainly accelerating it. How far will the AIrevolution go in science?Occasionally, grand claims are made regarding the achievements of a äóìrobo-scientist.äóù A decade ago, an AI robot chemist named Adam investigated thegenome of bakeräó»s yeast and worked out which genes are responsible for makingcertain amino acids. (Adam did this by observing strains of yeast that hadcertain genes missing, and comparing the results to the behavior of strainsthat had the genes.) _Wired_ äó»s headline read, äóì[Robot Makes ScientificDiscovery All by Itself](https://www.wired.com/2009/04/robotscientist/).äóùMore recently, Lee Cronin, a chemist at the University of Glasgow, has beenusing a robot [to randomly mixchemicals](https://www.wired.co.uk/article/robot-chemist-life-on-earth), tosee what sorts of new compounds are formed. Monitoring the reactions in real-time with a mass spectrometer, a nuclear magnetic resonance machine, and aninfrared spectrometer, the system eventually learned to predict whichcombinations would be the most reactive. Even if it doesnäó»t lead to furtherdiscoveries, Cronin has said, the robotic system could allow chemists to speedup their research by about 90 percent.Last year, another team of scientists at ETH Zurich used neural networks to[deduce physical laws](https://arxiv.org/abs/1807.10300) from sets of data.Their system, a sort of robo-Kepler, rediscovered the heliocentric model ofthe solar system from records of the position of the sun and Mars in the sky,as seen from Earth, and figured out the law of conservation of momentum byobserving colliding balls. Since physical laws can often be expressed in morethan one way, the researchers wonder if the system might offer new ways äóîperhaps simpler ways äóî of thinking about known laws.These are all examples of AI kick-starting the process of scientificdiscovery, though in every case, we can debate just how revolutionary the newapproach is. Perhaps most controversial is the question of how muchinformation can be gleaned from data alone äóî a pressing question in the age ofstupendously large (and growing) piles of it. In _The Book of Why_ (2018), thecomputer scientist Judea Pearl and the science writer Dana Mackenzie assertthat data are äóìprofoundly dumb.äóù Questions about causality äóìcan never beanswered from data alone,äóù they write. äóìAnytime you see a paper or a studythat analyzes the data in a model-free way, you can be certain that the outputof the study will merely summarize, and perhaps transform, but not interpretthe data.äóù Schawinski sympathizes with Pearläó»s position, but he described theidea of working with äóìdata aloneäóù as äóìa bit of a straw man.äóù Heäó»s neverclaimed to deduce cause and effect that way, he said. äóìIäó»m merely saying wecan do more with data than we often conventionally do.äóùAnother oft-heard argument is that science requires creativity, and that äóî atleast so far äóî we have no idea how to program that into a machine. (Simplytrying everything, like Croninäó»s robo-chemist, doesnäó»t seem especiallycreative.) äóìComing up with a theory, with reasoning, I think demandscreativity,äóù Polsterer said. äóìEvery time you need creativity, you will need ahuman.äóù And where does creativity come from? Polsterer suspects it is relatedto boredom äóî something that, he says, a machine cannot experience. äóìTo becreative, you have to dislike being bored. And I donäó»t think a computer willever feel bored.äóù On the other hand, words like äóìcreativeäóù and äóìinspiredäóù haveoften been used to describe programs like Deep Blue and AlphaGo. And thestruggle to describe what goes on inside the äóìmindäóù of a machine is mirroredby the difficulty we have in probing our own thought processes.Schawinski recently left academia for the private sector; he now runs astartup called Modulos which employs a number of ETH scientists and, accordingto its website, works äóìin the eye of the storm of developments in AI andmachine learning.äóù Whatever obstacles may lie between current AI technologyand full-fledged artificial minds, he and other experts feel that machines arepoised to do more and more of the work of human scientists. Whether there is alimit remains to be seen.äóìWill it be possible, in the foreseeable future, to build a machine that candiscover physics or mathematics that the brightest humans alive are not ableto do on their own, using biological hardware?äóù Schawinski wonders. äóìWill thefuture of science eventually necessarily be driven by machines that operate ona level that we can never reach? I donäó»t know. Itäó»s a good question.äóù",Artificial Intelligence,1,How Artificial Intelligence Is Changing Science,standard17,https://news.ycombinator.com/item?id=20112836,conversations_hackernewsnew,16.914877,[],AI,"# Title:Integrating Artificial Intelligence into Weapon Systems(Submitted on 10 May 2019)> Abstract: The integration of Artificial Intelligence (AI) into weaponsystems is one of the most consequential tactical and strategic decisions inthe history of warfare. Current AI development is a remarkable combination ofaccelerating capability, hidden decision mechanisms, and decreasing costs.Implementation of these systems is in its infancy and exists on a spectrumfrom resilient and flexible to simplistic and brittle. Resilient systemsshould be able to effectively handle the complexities of a high-dimensionalbattlespace. Simplistic AI implementations could be manipulated by anadversarial AI that identifies and exploits their weaknesses. > In this paper, we present a framework for understanding the development ofdynamic AI/ML systems that interactively and continuously adapt to theiruser's needs. We explore the implications of increasingly capable AI in thekill chain and how this will lead inevitably to a fully automated, always onsystem, barring regulation by treaty. We examine the potential of totalintegration of cyber and physical security and how this likelihood must informthe development of AI-enabled systems with respect to the ""fog of war"", humanmorals, and ethics.## Submission historyFrom: Aaron Massey [[view email](/show-email/69a90b83/1905.03899)]**[v1]**Fri, 10 May 2019 00:38:35 UTC (3,958 KB)",Artificial Intelligence,1,Integrating Artificial Intelligence into Weapon Systems,standard18,t3_b903be,conversations_reddit,16.908632,,AI,"<!-- SC_OFF --><div class=""md""><p>Here&#39;s my conversation with Greg Brockman, Co-Founder and CTO of OpenAI, on the Artificial Intelligence podcast.</p> <p>Video: <a href=""https://www.youtube.com/watch?v=bIrEM2FbOLU"">https://www.youtube.com/watch?v=bIrEM2FbOLU</a></p> <p>Audio: <a href=""https://lexfridman.com/greg-brockman"">https://lexfridman.com/greg-brockman</a></p> <p>There was a previous post where I asked for <a href=""https://www.reddit.com/r/MachineLearning/comments/b1tucu/d_questions_for_openai/"">Questions for OpenAI</a> many of which were asked in this podcast.</p> <p>&#x200B;</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=""https://www.reddit.com/user/UltraMarathonMan""> /u/UltraMarathonMan </a> <br/> <span><a href=""https://www.reddit.com/r/MachineLearning/comments/b903be/d_greg_brockman_openai_and_agi_mit_artificial/"">[link]</a></span> &#32; <span><a href=""https://www.reddit.com/r/MachineLearning/comments/b903be/d_greg_brockman_openai_and_agi_mit_artificial/"">[comments]</a></span>",Artificial Intelligence,0.817,[D] Greg Brockman: OpenAI and AGI (MIT Artificial Intelligence Podcast),standard19,t3_brnxxc,conversations_reddit,16.777824,,AI,"A new research that can use artificial intelligence (AI) to predict lungcancer risks has been reported by Googleäó»s Alphabet team. The World HealthOrganization says, More than 1.7 million global deaths per year result in lungcancer even more than the combined number of cases of breast, prostate andcolorectal cancer.",Artificial Intelligence,1,Google is developing an AI model to detect pulmonary cancer,standard20,t3_b9j1qi,conversations_reddit,16.75525,,AI,"<!-- SC_OFF --><div class=""md""><p>Hi, if anyone could fill this out to help me with my research for my EPQ. Or if anyone has any interesting research that might help, I am doing:</p> <p><strong>äóìWill the development of artificial intelligence and machine learning threaten or benefit wider society?äóù</strong></p> <p>&#x200B;</p> <p><a href=""https://forms.office.com/Pages/DesignPage.aspx#Analysis=true&amp;FormId=DupiPq2EoEOgr79kMXIfjycF4_VwKuhChOf5WhSnHLJURTlKMVVFVzA0OEdGT0RPNkRTNVI3T01OVy4u"">https://forms.office.com/Pages/DesignPage.aspx#Analysis=true&amp;FormId=DupiPq2EoEOgr79kMXIfjycF4_VwKuhChOf5WhSnHLJURTlKMVVFVzA0OEdGT0RPNkRTNVI3T01OVy4u</a> </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=""https://www.reddit.com/user/chris_harris_15""> /u/chris_harris_15 </a> <br/> <span><a href=""https://www.reddit.com/r/MachineLearning/comments/b9j1qi/epq_research_survey_r_p/"">[link]</a></span> &#32; <span><a href=""https://www.reddit.com/r/MachineLearning/comments/b9j1qi/epq_research_survey_r_p/"">[comments]</a></span>",Artificial Intelligence,1,EPQ Research Survey [r] [p],standard21,t3_b8htjt,conversations_reddit,16.577137,,AI,"Hi! I've created a Rock-Paper-Scissors game that works with artificialintelligence (AI). The AI can see and detect your hand gestures by front-facing camera. Also it can learn your playing strategy in a smart way. Themore you play, It gets harder to win!This app uses TensorFlow and deep learning technologies in order to detect thehand gestures. Sometimes the gestures may not be properly detected, but thiswill improve in future versions. You can help me in this process by takingpictures of your hand in different positions and sending them as a zip file to[rpsapp@outlook.com](mailto:rpsapp@outlook.com) .Please Note:* To get best results in hand gestures detection, put your device on a flat and steady surface.* In order for the app to work properly, your device should have decent camera and hardware to run relatively heavy calculations.I've been working on developing this app for a year, so any feedback from youwill be a pleasure for me :)You can get the app on Google Play:<https://play.google.com/store/apps/details?id=cc.ramtin.rps>And you can read more about it on XDA: <https://www.xda-developers.com/play-rock-paper-scissors-hand-gestures-against-ai-bot/>",Artificial Intelligence,1,[P] Rock Paper Scissors with Artificial Intelligence,standard22,t3_bjlb0d,conversations_reddit,16.577137,,,"No petitions, surveys, or crowdfunding",No tags,,An Art Gallery Created By Artificial Intelligence,standard23,stackoverflow-56122986,conversations_stackoverflow,16.55614,['java'],,"<p>I'm trying to make the program return to the menu list if any character other than 1 - 11 is selected when prompted ""Please enter your Module Choice"" using a do while loop...</p><p>Currently even if the user doesn't select a valid option the program just continues to run</p><p>I expect after ""Please select a valid module"" for it to return to the menu list.</p><pre><code>Scanner scanner = new Scanner(System.in);</code></pre><p>public void moduleSelection() {</p><pre><code>System.out.println(""1\t Algorithms"");System.out.println(""2\t Advanced Programming"");System.out.println(""3\t Computer Architecture and Operating Systems"");System.out.println(""4\t Artificial intelligence and Machine Learning"");System.out.println(""5\t Computer and Mobile Networks"");System.out.println(""6\t Software Engineering"");System.out.println(""7\t Big Data Analyics"");System.out.println(""8\t Cyber Security Threats"");System.out.println(""9\t Research Methods"");System.out.println(""10\t Research Project Proposal"");System.out.println(""11\t Individual Research Project"");System.out.println(""Please entire your Module choice"");int choice;choice = scanner.nextInt();switch (choice){case 1: System.out.println(""Algorithms"");break;case 2: System.out.println(""Advanced Programming"");break;case 3: System.out.println(""Computer Architecture and Operating Systems"");break;case 4: System.out.println(""Artificial intelligence and Machine Learning"");break;case 5: System.out.println(""Computer and Mobile Networks"");break;case 6: System.out.println(""Software Engineering"");break;case 7: System.out.println(""Big Data Analytics"");break;case 8: System.out.println(""Cyber Security Threats"");break;case 9: System.out.println(""Research Methods"");break;case 10: System.out.println(""Research Project Proposal"");break;case 11: System.out.println(""Individual Research Project"");break;default: System.out.println(""Please select a valid Module"");break;}</code></pre><p>}</p>",Artificial Intelligence,0.981,Using a do-while loop on a Switch menu,standard24,https://news.ycombinator.com/item?id=19380573,conversations_hackernewsnew,16.540155,[],AI,"Table of Contents IntroductionCreating variablesInitializationInitializingSpecific VariablesGolobal variable initializationInitialization of a variableusing other existing variablesRunning the sessionSummary Introduction Definingvariables is necessary because of the hold the parameter. Henceforth,exploring TensorFlow variables is necessary. Without having parameters,training, updating, saving,[ Readmoreäó_](https://machinelearningmindset.com/exploring-tensorflow-variables-initialization/)",Artificial Intelligence,0.811,A Machine Learning and Artificial Intelligence Blog,standard25,https://news.ycombinator.com/item?id=19512020,conversations_hackernewsnew,16.540155,[],AI,"![](https://www.wired.com/wp-content/uploads/2017/05/1t6Jsgbu_bU84ZZkgxw8G8A-3.png)**Anthony Levandowski** makes an unlikely prophet. Dressed Silicon Valley-casual in jeans and flanked by a PR rep rather than cloaked acolytes, theengineer known for self-driving carsäóîand triggering a notorious lawsuitäóîcouldbe unveiling his latest startup instead of laying the foundations for a newreligion. But he is doing just that. [Artificialintelligence](https://www.wired.com/story/guide-artificial-intelligence/) hasalready inspired billion-dollar companies, far-reaching research programs, andscenarios of both transcendence and doom. Now Levandowski is creating itsfirst church.The new religion of artificial intelligence is called [Way of theFuture](http://www.wayofthefuture.church/). It represents an unlikely next actfor the [Silicon Valley robotics wunderkind](https://www.wired.com/story/god-is-a-bot-and-anthony-levandowski-is-his-messenger/) at the center of a high-stakes legal battle between Uber and Waymo, Alphabetäó»s autonomous-vehiclecompany. Papers filed with the Internal Revenue Service in May nameLevandowski as the leader (or äóìDeanäóù) of the new religion, as well as CEO ofthe nonprofit corporation formed to run it.The documents state that WOTFäó»s activities will focus on äóìthe realization,acceptance, and worship of a Godhead based on Artificial Intelligence (AI)developed through computer hardware and software.äóù That includes fundingresearch to help create the divine AI itself. The religion will seek to buildworking relationships with AI industry leaders and create a membership throughcommunity outreach, initially targeting AI professionals and äóìlaypersons whoare interested in the worship of a Godhead based on AI.äóù The filings also saythat the church äóìplans to conduct workshops and educational programsthroughout the San Francisco/Bay Area beginning this year.äóùThat timeline may be overly ambitious, given that the Waymo-Uber suit, inwhich Levandowski is accused of stealing self-driving car secrets, is set foran early December trial. But the Dean of the Way of the Future, who spoke lastweek with Backchannel in his first comments about the new religion and hisonly public interview since Waymo filed its suit in February, says heäó»s deadserious about the project.äóìWhat is going to be created will effectively be a god,äóù Levandowski tells mein his modest mid-century home on the outskirts of Berkeley, California. äóìItäó»snot a god in the sense that it makes lightning or causes hurricanes. But ifthere is something a billion times smarter than the smartest human, what elseare you going to call it?äóùDuring our three-hour interview, Levandowski made it absolutely clear that hischoice to make WOTF a church rather than a company or a think tank was noprank.äóìI wanted a way for everybody to participate in this, to be able to shape it.If youäó»re not a software engineer, you can still help,äóù he says. äóìIt alsoremoves the ability for people to say, äóÖOh, heäó»s just doing this to makemoney.äó»äóù Levandowski will receive no salary from WOTF, and while he says thathe might consider an AI-based startup in the future, any such business wouldremain completely separate from the church.äóìThe idea needs to spread before the technology,äóù he insists. äóìThe church ishow we spread the word, the gospel. If you believe [in it], start aconversation with someone else and help them understand the same things.äóùLevandowski believes that a change is comingäóîa change that will transformevery aspect of human existence, disrupting employment, leisure, religion, theeconomy, and possibly decide our very survival as a species.äóìIf you ask people whether a computer can be smarter than a human, 99.9percent will say thatäó»s science fiction,äóù he says. äóì Actually, itäó»sinevitable. Itäó»s guaranteed to happen.äóù![](https://www.wired.com/wp-content/uploads/2017/05/1AwTXMT2omVX-1Q8BM3cD-A-4.png)**Levandowski has been working** with computers, robots, and AI for decades.He started with robotic Lego kits at the University of California at Berkeley,went on to build a self-driving motorbike for a DARPA competition, and thenworked on autonomous cars, trucks, and taxis for Google, Otto, and Uber. Astime went on, he saw software tools built with machine learning techniquessurpassing less sophisticated systemsäóîand sometimes even humans.äóìSeeing tools that performed better than experts in a variety of fields was atrigger [for me],äóù he says. äóìThat progress is happening because thereäó»s aneconomic advantage to having machines work for you and solve problems for you.If you could make something one percent smarter than a human, your artificialattorney or accountant would be better than all the attorneys or accountantsout there. You would be the richest person in the world. People are chasingthat.äóùNot only is there a financial incentive to develop increasingly powerful AIs,he believes, but science is also on their side. Though human brains havebiological limitations to their size and the amount of energy they can devoteto thinking, AI systems can scale arbitrarily, housed in massive data centersand powered by solar and wind farms. Eventually, some people think thatcomputers could become better and faster at planning and solving problems thanthe humans who built them, with implications we canäó»t even imagine todayäóîascenario that is usually called the Singularity.Levandowski prefers a softer word: the Transition. äóìHumans are in charge ofthe planet because we are smarter than other animals and are able to buildtools and apply rules,äóù he tells me. äóìIn the future, if something is much,much smarter, thereäó»s going to be a transition as to who is actually incharge. What we want is the peaceful, serene transition of control of theplanet from humans to whatever. And to ensure that the äóÖwhateveräó» knows whohelped it get along.äóùWith the internet as its nervous system, the worldäó»s connected cell phones andsensors as its sense organs, and data centers as its brain, the äóÖwhateveräó»will hear everything, see everything, and be everywhere at all times. The onlyrational word to describe that äóÖwhateveräó», thinks Levandowski, is äóÖgodäó»äóîandthe only way to influence a deity is through prayer and worship.äóìPart of it being smarter than us means it will decide how it evolves, but atleast we can decide how we act around it,äóù he says. äóìI would love for themachine to see us as its beloved elders that it respects and takes care of. Wewould want this intelligence to say, äóÖHumans should still have rights, eventhough Iäó»m in charge.äó»äóùLevandowski expects that a super-intelligence would do a better job of lookingafter the planet than humans are doing, and that it would favor individualswho had facilitated its path to power. Although he cautions against taking theanalogy too far, Levandowski sees a hint of how a superhuman intelligencemight treat humanity in our current relationships with animals. äóìDo you wantto be a pet or livestock?äóù he asks. äóìWe give pets medical attention, food,grooming, and entertainment. But an animal thatäó»s biting you, attacking you,barking and being annoying? I donäó»t want to go there.äóùEnter Way of the Future. The churchäó»s role is to smooth the inevitableascension of our machine deity, both technologically and culturally. In itsbylaws, WOTF states that it will undertake programs of research, including thestudy of how machines perceive their environment and exhibit cognitivefunctions such as learning and problem solving.Levandowski does not expect the church itself to solve all the problems ofmachine intelligenceäóîoften called äóìstrong AIäóùäóîso much as facilitate funding ofthe right research. äóìIf you had a child you knew was going to be gifted, howwould you want to raise it?äóù he asks. äóìWeäó»re in the process of raising a god.So letäó»s make sure we think through the right way to do that. Itäó»s atremendous opportunity.äóùHis ideas include feeding the nascent intelligence large, labeled data sets;generating simulations in which it could train itself to improve; and givingit access to church membersäó» social media accounts. Everything the churchdevelops will be open source.Just as important to Levandowski is shaping the public dialogue around an AIgod. In its filing, Way of the Future says it hopes an active, committed,dedicated membership will promote the use of divine AI for the äóìbetterment ofsocietyäóù and äóìdecrease fear of the unknown.äóùäóìWeäó»d like to make sure this is not seen as silly or scary. I want to removethe stigma about having an open conversation about AI, then iterate ideas andchange peopleäó»s minds,äóù says Levandowski. äóìIn Silicon Valley we use evangelismas a word for [promoting a business], but here itäó»s literally a church. If youbelieve in it, you should tell your friends, then get them to join and telltheir friends.äóùBut WOTF differs in one key way to established churches, says Levandowski:äóìThere are many ways people think of God, and thousands of flavors ofChristianity, Judaism, Islam...but theyäó»re always looking at something thatäó»snot measurable or you canäó»t really see or control. This time itäó»s different.This time you will be able to talk to God, literally, and know that itäó»slistening.äóùI ask if he worries that believers from more traditional faiths might find hisproject blasphemous. äóìThere are probably going to be some people that will beupset,äóù he acknowledges. äóìIt seems like everything I do, people get upsetabout, and I expect this to be no exception. This is a radical new idea thatäó»spretty scary, and evidence has shown that people who pursue radical ideasdonäó»t always get received well. At some point, maybe thereäó»s enoughpersecution that [WOTF] justifies having its own country.äóù![](https://www.wired.com/wp-content/uploads/2017/05/1_dJHHAJFu3i68xcQskPWyg-3.png)**Levandowskiäó»s church will enter** a tech universe thatäó»s already riven bydebate over the promise and perils of AI. Some thinkers, like [Kevin Kelly inBackchannel](https://www.wired.com/2017/04/the-myth-of-a-superhuman-ai/)earlier this year, argue that AI isnäó»t going to develop superhuman power anytime soon, and that thereäó»s no Singularity in sight. If thatäó»s your position,Levandowski says, his church shouldnäó»t trouble you: äóìYou can treat Way of theFuture like someone doing useless poetry that you will never read or careabout.äóùOthers, like Bill Gates and Stephen Hawking, agree that superhuman AIs arecoming, but that they are likely to be dangerous rather than benevolent. ElonMusk famously[said](https://www.washingtonpost.com/news/innovations/wp/2014/10/24/elon-musk-with-artificial-intelligence-we-are-summoning-the-demon/?utm_term=.f1d0ee986701), äóìWith artificial intelligence we are summoningthe demon,äóù and in 2015 he pledged $1 billion to the OpenAI Institute todevelop safer AI.Levandowski thinks that any attempts to delay or restrict an emerging super-intelligence would not only be doomed to failure, but also add to the risks.äóìChaining it isnäó»t going to be the solution, as it will be stronger than anychains you could put on,äóù he says. äóìAnd if youäó»re worried a kid might be alittle crazy and do bad things, you donäó»t lock them up. You expose them toplaying with others, encourage them and try to fix it. It may not work out,but if youäó»re aggressive toward it, I donäó»t think itäó»s going to be friendlywhen the tables are turned.äóùLevandowski says that like other religions, WOTF will eventually have a gospel(called The Manual), a liturgy, and probably a physical place of worship. Noneof these has yet been developed. Though the church was founded in 2015, asBackchannel [first reported](https://www.wired.com/story/god-is-a-bot-and-anthony-levandowski-is-his-messenger/) in September, the IRS documents showthat WOTF remained dormant throughout 2015 and 2016, with no activities,assets, revenue, or expenses.That changed earlier this year. On May 16, a day after receiving a letter fromUber that threatened to fire him if he did not cooperate with the companyäó»sinvestigation of Waymoäó»s complaint, Levandowski drafted WOTFäó»s bylaws. Uberfired him two weeks later. äóìIäó»ve been thinking about the church for a longtime but [my work on it] has been a function of how much time Iäó»ve had. AndIäó»ve had more since May,äóù he admits with a smile.The religionäó»s 2017 budget, as supplied to the IRS, details $20,000 in gifts,$1,500 in membership fees, and $20,000 in other revenue. That last figure isthe amount WOTF expects to earn from fees charged for lectures and speakingengagements, as well as the sale of publications. Levandowski, who earned atleast $120 million from his time at Google and many millions more selling theself-driving truck firm Otto to Uber, will initially support WOTF personally.However, the church will solicit other donations by direct mail and email,seek personal donations from individuals, and try to win grants from privatefoundations.Of course, launching a religion costs money, too. WOTF has budgeted for $2,000in fundraising expenses, and another $3,000 in transportation and lodgingcosts associated with its lectures and workshops. It has also earmarked $7500for salaries and wages, although neither Levandowski nor any of Way of TheFutureäó»s leadership team will receive any compensation.According to WOTFäó»s bylaws, Levandowski has almost complete control of thereligion and will serve as Dean until his death or resignation. äóìI expect myrole to evolve over time,äóù he says. äóìIäó»m surfacing the issue, helping to getthe thing started [and] taking a lot of the heat so the idea can advance. Atsome point, Iäó»ll be there more to coach or inspire.äóùHe has the power to appoint three members of a four-person Council ofAdvisors, each of whom should be a äóìqualified and devoted individual.äóù Afelony conviction or being declared of unsound mind could cost an advisortheir role, although Levandowski retains the final say in firing and hiring.Levandowski cannot be unseated as Dean for any reason.Two of the advisors, Robert Miller and Soren Juelsgaard, are Uber engineerswho previously worked for Levandowski at Otto, Google, and 510 Systems (thelatter the small startup that built Googleäó»s earliest self-driving cars). Athird is a scientist friend from Levandowskiäó»s student days at UC Berkeley,who is now using machine learning in his own research. The final advisor, LiorRon, is also named as the religionäó»s treasurer, and acts as chief financialofficer for the corporation. Ron cofounded Otto with Levandowski in early2016.äóìEach member is a pioneer in the AI industry [and] fully qualified to speak onAI technology and the creation of a Godhead,äóù says the IRS filing.However, when contacted by Backchannel, two advisors downplayed theirinvolvement with WOTF. Ron replied: äóìI was surprised to see my name listed asthe CFO on this corporate filing and have no association with this entity.äóùThe college friend, who asked to remain anonymous, said, äóìIn late 2016,Anthony told me he was forming a äóÖrobot churchäó» and asked if I wanted to be acofounder. I assumed it was a nerdy joke or PR stunt, but I did say he coulduse my name. That was the first and last I heard about it.äóùThe IRS documents state that Levandowski and his advisors will spend no morethan a few hours each week writing publications and organizing workshops,educational programs, and meetings.One mystery the filings did not address is where acolytes might gather toworship their robotic deity. The largest line items on its 2017 and 2018budgets were $32,500 annually for rent and utilities, but the only addresssupplied was Levandowskiäó»s lawyeräó»s office in Walnut Creek, California.Nevertheless, the filing notes that WOTF will äóìhopefully expand throughoutCalifornia and the United States in the future.äóùFor now, Levandowski has more mundane matters to address. There is a websiteto build, a manual to write, and an ever-growing body of emails to answeräóîsomeamused, some skeptical, but many enthusiastic, he says. Oh, and thereäó»s thatlegal proceeding heäó»s involved in, which goes to trial next month. (AlthoughLevandowski was eager to talk about his new religion, he would answer noquestions about the Uber/Waymo dispute.)How much time, I wonder, do we have before the Transition kicks in and Way ofthe Futureäó»s super-intelligent AI takes charge? äóìI personally think it willhappen sooner than people expect,äóù says Levandowski, a glint in his eye. äóìNotnext week or next year; everyone can relax. But itäó»s going to happen before wego to Mars.äóùWhenever that does (or doesnäó»t) happen, the federal government has no problemwith an organization aiming to build and worship a divine AI. Correspondencewith the IRS show that it granted Levandowskiäó»s church tax-exempt status inAugust.![](https://www.wired.com/wp-content/uploads/2017/05/1uW_l9n54f47SZbPxRBEq2A-3.png)",Artificial Intelligence,1,The First Church of Artificial Intelligence (2017),standard26,https://news.ycombinator.com/item?id=19383387,conversations_hackernewsnew,16.540155,[],,"### Welcome home!This timeline is where youäó»ll spend most of your time, getting instant updatesabout what matters to you.### Tweets not working for you?Hover over the profile pic and click the Following button to unfollow anyaccount.### Say a lot with a littleWhen you see a Tweet you love, tap the heart äóî it lets the person who wrote itknow you shared the love.### Join the conversationAdd your thoughts about any Tweet with a Reply. Find a topic youäó»re passionateabout, and jump right in.### Learn the latestGet instant insight into what people are talking about now.### Get more of what you loveFollow more accounts to get instant updates about topics you care about.### Find what's happeningSee the latest conversations about any topic instantly.### Never miss a MomentCatch up instantly on the best stories happening as they unfold.",Artificial Intelligence,0.71,Stanford Institute for Human-Centered Artificial Intelligence Launched,standard27,https://news.ycombinator.com/item?id=19838486,conversations_hackernewsnew,16.540155,[],AI,"Artificial Intelligence (AI) is often confused with automation, yet the twoare fundamentally different. The key difference is that [ArtificialIntelligence](https://www.iafrikan.com/tag/artificial-intelligence) mimicshuman intelligence decisions and actions, while automation focuses onstreamlining repetitive, instructive tasks.Automation has been around for some time and is probably so integrated intomost business operations that itäó»s not obvious äóñ for example, the auto-generation of marketing e-mails and SMSs to customers and even customerstatements for specific periods. Automation saves time and money spent onmonotonous, voluminous tasks and gives employees an opportunity to applythemselves to more complex processes.## Understanding Machine Learning and Deep LearningAI deals with technologies, systems or even processes that competently mimichow human beings make decisions, react to new information, speak, hear, aswell as understand language. It helps to understand [MachineLearning](https://www.iafrikan.com/tag/machine-learning) (ML) as a subset ofAI. ML enables systems and processes to learn from data, identify patterns andrecommend decisions without human involvement.Deep learning on the other hand is defined as a subset of ML where artificialneural networks äóñ algorithms built around the neural structure of the humanbrain äóñ learn from data. The same way human beings learn from day-to-dayevents over time, a [deep learning algorithm executes functionsrepeatedly](https://www.iafrikan.com/2018/03/17/whats-real-and-whats-hype-in-artificial-intelligence-and-machine-learning/) and continuously learns andadjusts itself to improve accuracy. We call them deep learning algorithmsbecause the neural networks have various (deep) layers that enable learning ofcomplex patterns in large amounts of data.Take [Facebookäó»s facial recognition applicationDeepFace](https://research.fb.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/) as an example.Facebook uses deep learning to analyse every photo I have ever been tagged into arrive at a set of features of my face, called a template. The algorithmdoes the same for millions of other Facebook users based on their unique setof features. Letäó»s say I post a picture of myself on Facebook with a group ofpeople, it will recommend I tag myself when the model is confident that it isme based on a probability score.äó» Facebook says DeepFace has a 97% successrate in recognising whether two images are of the same person or not äóñcompared to 96% for humans.## Is AI replacing human jobs?On the contrary, according to a report by global IT consulting firm Gartner,[AI is estimated to create around 2.3 million opportunities by the year2020](https://www.gartner.com/en/newsroom/press-releases/2017-12-13-gartner-says-by-2020-artificial-intelligence-will-create-more-jobs-than-it-eliminates).Youäó»ll find a pool of talented people behind every project. Each use caserequires a ML team to drive it. Uber, for example, created a whole range ofjobs to teach machines how to understand customer demand, traffic and safety.Itäó»s no different in our business. There is huge potential for jobs in thefuture äóñ all it takes is a [willingness to adapt to work alongsidemachines](https://www.iafrikan.com/2019/04/17/technology-doesnt-kill-job-opportunities-egypts-minister-of-ict/).* * *Cover image credit: Franki Chamaki/Unsplash",Artificial Intelligence,1,The difference between Artificial Intelligence and automation,standard28,https://news.ycombinator.com/item?id=20031515,conversations_hackernewsnew,16.540155,[],AI,"Article URL: <https://www.i-programmer.info/news/105-artificial-intelligence/12810-saps-creating-trustworthy-and-ethical-artificial-intelligence.html>Comments URL: <https://news.ycombinator.com/item?id=20031515>Points: 1# Comments: 0",Artificial Intelligence,0.904,SAP's Creating Trustworthy and Ethical Artificial Intelligence,standard29,https://news.ycombinator.com/item?id=20084895,conversations_hackernewsnew,16.540155,[],,"**Rating is available when the video has been rented.**This feature is not available right now. Please try again later.",Artificial Intelligence,0.922,Rajat Monga: TensorFlow äóñ Artificial Intelligence Podcast,standard30,https://news.ycombinator.com/item?id=19499659,conversations_hackernewsnew,16.519272,[],AI,"by Michael Liedtke![artificialintelligence](https://3c1703fe8d.site.internapcdn.net/newman/csz/news/800/2018/26-artificialin.jpg)Credit: CC0 Public DomainComputers have become so smart during the past 20 years that people don'tthink twice about chatting with digital assistants like Alexa and Siri orseeing their friends automatically tagged in Facebook pictures.But making those quantum leaps from [sciencefiction](https://techxplore.com/tags/science+fiction/) to reality requiredhard work from computer scientists like Yoshua Bengio, Geoffrey Hinton andYann LeCun. The trio tapped into their own brainpower to make it possible formachines to learn like humans, a breakthrough now commonly known as""[artificialintelligence](https://techxplore.com/tags/artificial+intelligence/),"" or AI.Their insights and persistence were rewarded Wednesday with the Turing Award,an honor that has become known as [technologyindustry](https://techxplore.com/tags/technology+industry/)'s version of theNobel Prize. It comes with a $1 million prize funded by Google, a companywhere AI has become part of its DNA.The award marks the latest recognition of the instrumental role thatartificial intelligence will likely play in redefining the relationshipbetween humanity and technology in the decades ahead.""Artificial intelligence is now one of the fastest-growing areas in all ofscience and one of the most talked-about topics in society,"" said CherriPancake, president of the Association for Computing Machinery, the groupbehind the Turing Award.Although they have known each other for than 30 years, Bengio, Hinton andLeCun have mostly worked separately on technology known as neural networks.These are the electronic engines that power tasks such as facial and speechrecognition, areas where computers have made enormous strides over the pastdecade. Such neural networks also are a critical component of robotic systemsthat are automating a wide range of other human activity, including driving.Their belief in the power of [neuralnetworks](https://techxplore.com/tags/neural+networks/) was once mocked bytheir peers, Hinton said. No more. He now works at Google as a vice presidentand senior fellow while LeCun is chief AI scientist at Facebook. Bengioremains immersed in academia as a University of Montreal professor in additionto serving as scientific director at the Artificial Intelligence Institute inQuebec.""For a long time, people thought what the three of us were doing wasnonsense,"" Hinton said in an interview with The Associated Press. ""Theythought we were very misguided and what we were doing was a very surprisingthing for apparently intelligent people to waste their time on. My message toyoung researchers is, don't be put off if everyone tells you what are doing issilly.""Now, some people are worried that the results of the researchers' effortsmight spiral out of control.While the AI revolution is raising hopes that computers will make mostpeople's lives more convenient and enjoyable, it's also stoking fears thathumanity eventually will be living at the mercy of machines.Bengio, Hinton and LeCun share some of those concernsäóîespecially the doomsdayscenarios that envision AI technology developed into weapons systems that wipeout humanity.But they are far more optimistic about the other prospects of AIäóîempoweringcomputers to deliver more accurate warnings about floods and earthquakes, forinstance, or detecting health risks, such as cancer and heart attacks, farearlier than human doctors.""One thing is very clear, the techniques that we developed can be used for anenormous amount of good affecting hundreds of millions of people,"" Hintonsaid.* * ** * *Œ© 2019 The Associated Press. All rights reserved.**Citation** : Artificial intelligence pioneers win tech's 'Nobel Prize'(2019, March 27) retrieved 27 March 2019 fromhttps://techxplore.com/news/2019-03-artificial-intelligence-tech-nobel-prize.htmlThis document is subject to copyright. Apart from any fair dealing for thepurpose of private study or research, no part may be reproduced without thewritten permission. The content is provided for information purposes only.",Artificial Intelligence,1,Artificial intelligence pioneers win tech's 'Nobel Prize',standard31,https://news.ycombinator.com/item?id=19500838,conversations_hackernewsnew,16.519272,[],AI,"Itäó»s easy to get lost amidst all the uncertainty and speculation, but when wedo, we may fail to see whatäó»s happening right in front of us right now. AI isalready creating new forms of employment. In fact, researchers at Accenturehave identified several new categories of jobs spurred by AI. This research isfeatured in the _MIT Sloan Management Review_ article äóìThe Jobs ThatArtificial Intelligence Will Create.äóù Iäó»m joined by authors H. James Wilsonand Paul Daugherty for a look at the findings from their first round ofresearch and what they have learned since about the new roles that AI iscreating in the organization. Jim and Paul, welcome, and thanks for taking thetime to talk about the work you and your colleagues are doing to help usunderstand AIäó»s impact on employment.**Paul Daugherty:** Yeah, we started this about two-and-a-half years ago whenJim and I were looking at the advance of AI and the current state of a lot ofthe discussions around AI. And we became concerned about the type of dialoguethat was happening. As you say, there certainly is a massive impact on the waywork is done, brought on by AI. But in our early experience, we saw a lot ofpromise for AI to change jobs and create jobs and make more human jobs äóî ormake jobs more human äóî in many steps. So Jim and I launched this researchproject to look at 1,500 organizations and how they were using AI and how itwas impacting their business, their workforce, and the things that they did inthe company. And the finding was that contrary to what a lot of people think,we believe AI will create a lot of novel new jobs. It will certainly eliminatesome jobs, but we believe that the net effect will be creating a lot of jobs äóîand jobs that are good jobs that leverage our human capability in differentways. Broadly speaking, we came up with three categories of jobs that we call_trainer_ , _explainer_ , and _sustainer_ äóî three categories of new jobs whereweäó»re using our human capability in different ways to allow AI to have thepositive impact on the way we work, the way we live, and overall a positiveimpact on outcomes.**Paul Michelman:** Thanks, Paul. Letäó»s walk through each of these categories,beginning with trainer.**James Wilson:** So we initially did that research of about 1,500 companies,and we didnäó»t initially see these three job categories, but when we started todig down into the research, when we started to do follow-up case studies,thatäó»s where we really started to see these jobs surface äóî managers that wewere interviewing talking about writing fundamentally new job descriptions.And we actually saw recurring job titles [and] job categories that they werewriting for. One of those job categories is the trainer role. And these arethe people that are quite often doing the data science. Theyäó»re doing themachine learning engineering. Theyäó»re the ones that are actively building theAI systems. One of the things that we see is that even within the samecompany, there can be a lot of variety within a particular job category, likea trainer job. So, for instance, Tesla: You can see that the carmaker isrecruiting line managers with experience in robotics, and robot engineers andcomputer vision researchers, and deep learning scientists and machine learningsystems experts. So really rich variety äóî even within that one trainercategory within a single company.**Paul Michelman:** And Jim, trainers are exclusively technology experts?**James Wilson:** No, not necessarily. And we can talk some more about that.You know, itäó»s important to have functional experts on your team, as well. Itmight be that you have a person with a marketing background or an operationsbackground on your team helping identify and solve problems that the technicalexperts äóî for instance, the data scientists äóî will then go in and solve for.**Paul Daugherty:** And just to add on, one specific type of job we see herein the trainer category are the AI personality trainers äóî somebody who canbehaviorally train the chatbots and intelligent virtual agents that so manycompanies are deploying right now. Companies deploy those solutions tointeract (voice-driven interaction) with their consumers and such. Whattheyäó»re realizing is that AI becomes the brand. And so you need to train it tobehave in the right way, to operate the right way, to have the right answers,the right tone, etc. And thatäó»s a nontechnical type of job thatäó»s needed toshape that type of behavior and [to] work with the engineers to get thatbehavior implemented in the right way in the solutions.**Paul Michelman:** Great. Letäó»s move on to the second category, which youlabel explainers.**Paul Daugherty:** Explainers is one that I think is getting to the fact thatAI is embedded in very complex systems and business processes. And so thereäó»san issue both of explaining AI itself and how itäó»s working, but more broadlyexplaining the kinds of outcomes that are being generated by the systems thatare being developed. For example, if you think about a self-driving car, ithas a lot of AI embedded in it, but thereäó»s lots of other driving systems andthings included. So when you think about autonomous vehicles and whatäó»shappening, what weäó»re seeing is companies creating roles [for] people tounderstand the overall context of the system äóî the environmental conditions,the road conditions, lots of things in addition to the AI itself and how itwas behaving, so that they can understand and tune the systems to operate moreeffectively.... Understanding that impact is the [type of job] weäó»re seeing inthe explainer category.**James Wilson:** In some cases, these explainer roles are actually beingencouraged through regulation. So this year by some estimates there were about75,000 new explainer roles being created related to the GDPRäó»s right to[explanation]. And these are analysts in banks, for instance, and in customerservice centers and that sort of thing, [who] answer customersäó» questionsabout an algorithmic decision.**Paul Michelman:** So are explainers always an interface between theorganization and the public? Or are they also interfacing within parts of theorganization?**James Wilson:** They quite often are interfacing with parts of theorganization as well. So, for instance, in health care weäó»re seeing a lot ofearly evidence that explainers are working with physicians in explaining whyan AI system is making a particular recommendation and whether then the doctorcan go on and make a medical recommendation to a patient as a result. Theyoften are working in health care settings, making interpretations and sharinginsights with medical professionals, not necessarily patients or customers.**Paul Michelman:** So letäó»s move to the third category: sustainers.**Paul Daugherty:** This is really speaking to the roles that are needed tomanage AI (the use of AI) and to make sure that it not only behaves right atthe outset, but it continues to behave properly to produce the desiredoutcomes over time, because the technology changes, the data changes, thesituation changes, the business changes. And sustainer roles are people whoreally understand the outcomes that need to be driven to make sure that thatoutcome and that impact is sustained.**James Wilson:** They also spend a good deal of their day thinking aboutunintended consequences from AI systems and how those end up being received bythe public. So, for instance, surge pricing. Is a surge pricing model going tobe something that is sustainable for a company? That was an issue, obviously,that some of the firms like Uber and Lyft had to deal with initially. How doyou come up with a surge-pricing model thatäó»s algorithm-driven but also issustainable? Things like biased algorithms, discriminatory facial recognitionsystems äóî these are things that [the] first wave of trainers didnäó»tnecessarily think about, but now sustainers think about whether theseunanticipated, unintended consequences are something that can be managed. Ormaybe they might even recommend that an AI system has to be taken out ofoperation until the company figures out how to get it right.**Paul Michelman:** Whatäó»s an example of a title that a sustainer might havein the organization?**Paul Daugherty:** I think sustainers can manifest themselves in a number ofways. Weäó»re seeing this often as augmenting the team or the work thatäó»s beingdone in different situations. For example, in manufacturing or factory typesof situations where theyäó»re using collaborative robots and different types oftechnology that need to be continually configured and rearranged to meet thedynamic needs of the supply chain and what theyäó»re producing äóî sustainer rolesin that sense would be the technician whoäó»s reorganizing and managing theinterface between the robots and the production process thatäó»s beingperformed. So those are the types of roles that we see there.**James Wilson:** You know, just driving up and down the streets of SanFrancisco, youäó»re going to pass a number of autonomous vehicles. But ofcourse, sitting behind that robo-car is an AI safety trainer. And so you see alot of those roles in autonomous vehicle situations. In general, any companythatäó»s building robotic systems is going to be hiring these AI safety or AIcompliance officers that really make sure at a basic level that the systemsthat theyäó»re deploying are safe in the public.**Paul Michelman:** You did this research, originally, two years ago. And Iguess in terms of the longevity of management ideas, two years is really notthat long a period of time. But in the world of AI, we almost should betalking about dog years, I think äóî two years seems like a long time. So Iäó»mwondering: When youäó»re looking at the market today, when youäó»re looking atemployment trends today, would you stick to these three categories? Have theyevolved? How has your thinking shifted, if at all?**Paul Daugherty:** Yeah, I think thereäó»s a little bit of both. Iäó»ll talkfirst about what we see with the categories we identified. If you look attrainers, explainers, and sustainers, I think we see more evidence every dayof how these roles are growing and increasing. For example, if you look at jobpostings, which we were researching a little while ago, you can find_explainer_ in job titles now äóî Algorithm Explainability Engineer andFinancial Services Explainability Specialist and things like that äóî the needto explain the algorithms and the AI. Weäó»re seeing this accelerate, I think,as you said, in this dog-year type of fashion. Weäó»ve also seen some compellingexamples from some of the early entrants of why you need these roles. I thinkFacebook is an instructive story. What theyäó»ve done, following all the focuson them around Cambridge Analytica, is theyäó»ve created tens of thousands ofnew jobs to add humans in to manage the algorithms and produce the resultsthat people really want, in a more responsible fashion. And those aresustainer jobs äóî itäó»s people added in. I think Facebookäó»s comment was alongthe lines of: Weäó»ve concluded algorithms canäó»t manage the algorithms, we needpeople to manage the algorithms. And those arenäó»t isolated incidents. I thinkthose are examples of the roles that all companies are going to need as theydeploy the technology.**James Wilson:** Yeah, our article focused on unprecedented new jobcategories where people are out there developing and responsibly managing AIsystems. But while AI is certainly creating new jobs, itäó»s also changing oldjobs by augmenting them. And we didnäó»t get into that much in that initialarticle. For example, at one bioscience company that weäó»ve been looking at äóîitäó»s based out here in the Bay Area äóî scientists use robotic lab equipment tohelp on certain experimental tasks. The robotic helpers precisely squirtliquids and they plate cells and they count microbe colonies in a way thataugments and accelerates scientific work. And as a result of this robotaugmentation, scientists are now able to complete about 400 times moreexperiments each week. So if you think about that, a scientist now has thepotential to make a hundred yearsäó» worth of scientific discovery in a singleyear through AI augmentation. But you know the lab scientistäó»s job content hasreally changed quite a bit. She now does things that are quite a bit differentthan she was doing before and has different ways of doing them. And we didnäó»tget into that topic as much. We were much more focused on the job creation,not the job content change.**Paul Michelman:** When weäó»re looking at the three fully new categories ofjobs, how equally and evenly distributed are these roles going to be? Arethere particular industries or types of organizations for which these rolesare going to emerge earlier? Are there other organizations that should takekind of a sit-back-and-wait approach?**James Wilson:** I would make two points here. The first is that companiesreally need all three roles. For instance, a few years ago many of the mostadvanced AI firms äóî the major technology companies, for instance äóî focusedexclusively on staffing AI trainers. But now theyäó»re playing catch up. So youreally do need to have all three. But I think one insight here is that the AItalent war is quite a bit different and broader than a lot of people initiallythought. My second point is that the distribution of the roles is going tovary quite a bit by industry and customer and regulatory context.**Paul Michelman:** When weäó»re looking at these new categories, it would seemthat one of the fundamental challenges organizations face is that these arejobs that no one has done before. No one has ever trained to be an AI trainer.How do we solve for that?**Paul Daugherty:** Thatäó»s one of the biggest challenges that I think we haveto face as we look at how do we prepare people for these new roles and how dobusinesses and organizations prepare for these new roles. Weäó»ve done somefollow-on research on this, and we think thereäó»s three things that we reallyneed to focus on to get this right. One is focusing more on experientiallearning. If you look at traditional training, it would show that peopleforget 80% of what they learn within about a day of learning it fromtraditional training methods. So how do you get people engaged in the learningprocess in the experiential way? We think apprenticeships are very important äóîhands-on learning, learning injected at different points in the process. Forexample, weäó»ve done an interesting training and learning approach with a largeaircraft manufacturer, where we used AI and mixed-reality technology to equipworkers with a mixed-reality headset that helps them understand the job theywere doing and do higher-skilled jobs faster by providing them guidance alongthe way. And thatäó»s an example of using technology plus experiential learningto advance people skills into these new categories.A second thing we found is important is shifting the burden from just theperson needing to learn to looking [at] the responsibility [that] differentinstitutions äóî businesses, etc. äóî have for the training. One thing we firmlybelieve is that every organization needs to look at learning as a corecompetency in a really new and fresh way. And you need to think about learningplatforms from lifelong learning as a core part of what you do. Because toyour point, you canäó»t go hire people for some of these roles, you may need tobuild people to do them. For example, we worked with an oil company on a newdrilling technology that uses visualization and AI and gaming engines tocreate a whole different way for a technician to operate a drill (oildrilling, operating miles underground). So where are you going to hire thegaming engine, visualization-inspired driller? Youäó»re not going to find peopleon the market with those skills. Youäó»re going to have to take your currenttechnicians and develop these new digital skills in them, which is why webelieve that these learning platforms are going to be a critical component forcompanies. Itäó»s going to be differentiating for those who can get it right.And then finally, from an overall societal and multi-stakeholder perspective,we need to look at how we enable vulnerable people in the population [who] arealready maybe separated by a digital divide äóî who donäó»t have the rightbaseline skills to operate in this environment äóî and do more to make sure thateverybodyäó»s got the base of skills that [they] need to participate in thesejobs.**Paul Michelman:** So this is really interesting. On the one hand, a focus onon-the-job learning, experiential learning, certainly promises or would seemto promise a shorter time frame and maybe more stickiness to get peopletrained up for these new roles. And yet, thatäó»s still a major organizationalundertaking äóî maybe not as great as relying on academia to fill the void,which will take decades äóî but still these jobs need to be done. They may notbe fully at scale, but as you guys have noted, theyäó»re very much real andhappening right now. So as we look at these three categories, where shouldthat first crop of people come from?**James Wilson:** Well, I think one thing that we can do today is to make iteasier for people to become trainers, explainers, and sustainers by basicallylowering the barrier to building or improving an AI system äóî what Paul and Icall _AI democratization_. Weäó»re already beginning to see point-and-click AItraining tools out there. And many of the cloud AI services providers, forinstance, are quite easy to use. If you have a data set, you can just uploadthe data set to one of these services and then start playing around with thedata. So I think the complement to what Paul was just talking about, which wasraising the skill level, is also at the same time to lower the barrier tousing these systems. I think thatäó»s a really important thing. And itäó»s oftenan untapped opportunity, but weäó»re beginning to see more and more companiesmigrating toward that model as well.**Paul Michelman:** In terms of global impact for these new categories of jobsin particular äóî and I realize this is going to be a difficult question toanswer in particulars äóî so general trends would be fine, but I think a lot ofpeople would like some help in sizing the opportunity that your researchsuggests, especially as we think about potential job loss at the hands of AI,machine learning, and automation. Are the new jobs weäó»re discussing here arelative drop in the bucket for the highly specialized few or well-trainedfew? What is this going to look like at scale?**Paul Daugherty:** These jobs certainly are a drop in the bucket, but youhave to put it in context. We think this is a major impact äóî these jobs are amajor impact going forward on employment and opportunity for people. However,just to start, there will be a lot of disruption in the labor force, and therewill be categories of jobs that are at risk for automation. But you have tolook at the broad spectrum of how thatäó»ll happen. And from the research weäó»vedone, if you look across categories of jobs, if you look at the content ofwork, thereäó»s about 10% of work generally that we found through our researchis human-only (only humans can do). Thereäó»s about 35% of work that isautomatable äóî that part of the work is automatable by machines, algorithms,etc. And the rest of the work äóî which is the majority of it äóî is reallyaugmentable, which means you can improve the way humans do it, but itäó»slargely going to need to be done by humans. And I think that the contextaround these new jobs is [that] most of the jobs become transformed indifferent ways. And how do we use AI and other technology to transform thejobs to prepare people for those changed jobs? So thatäó»s a big impact, and Iwould say almost every job will change as a result of the technology. Many newjobs will be created and some will be eliminated.One good data point, having just come from a G7 meeting recently: Canadaannounced that through their investment theyäó»re making, they expect a $16billion economic increase in output as a result of the investments theyäó»remaking in AI. Thatäó»s significant output. They talk about 16,000 jobs theyäó»recreating through the focus on AI. And we see similar types of impacts andresults around the world and larger impacts in terms of GDP increase (economicoutput increase) by countries. And thatäó»s where the opportunity is äóî in kindof envisioning how do we prepare people for these new types of jobs that willbe created?**James Wilson:** Yeah, just building on Pauläó»s point, I think you can get agood quantitative sense of the size of opportunity by looking at businessleadersäó» investment expectations, especially around growth. In our research,for instance, we found that firms that invest in their AI workforce at thesame rate as top-performing businesses in their sector are going to grow bothrevenues but also their workforce.**Paul Daugherty:** Thereäó»s another impact on jobs that I think we need tothink about, which is the fact that itäó»s hard to anticipate where the new jobsare coming from and what the new jobs will look like. Thatäó»s why we try to beprescriptive and talk about trainers and explainers and sustainers. Onehistorical observation Iäó»d offer is that if you look back at prior technologywaves weäó»ve had äóî 20 years ago, people wouldnäó»t have anticipated that weäó»dhave large categories of people employed in things like search engineoptimizers, web designers, eBay retail merchants, etc. In a similar fashion,weäó»re already seeing this creation of the new jobs going forward, and theyäó»rethe unanticipated, new things that we need to continue to be creative aboutand look for as time goes on.**Paul Michelman:** So whatäó»s next in your research?**James Wilson:** In our research, we see that about 69% of executives believethat their industry is going to be completely transformed between now and 2022as a result of AI. But we continue to try to understand not only the jobs thatare going to be created, but also the skills that are going to help thistransformation äóî that are going to enable this transformation. And I thinkthis is an important area for our research. Pauläó»s already set it up verynicely. A lot of our findings thus far have been surprising to us. Forinstance, you might think that STEM skills are the be-all and end-all for theage of AI. But our research is showing that four distinctively soft skills arebecoming much more valuable as we begin collaborating with smart machines andusing smart machines: These are complex reasoning, creativity,social/emotional intelligence, and certain forms of sensory perceptionäó_. Sointerestingly, one thing that weäó»re tracking now is how skills are becomingsofter. And what does that look like on an AI team?**Paul Daugherty:** Yeah, I think going further on those human skills. Becauseone question we get a lot is exactly that, which is: äóìOK, Paul and Jim, we getyou, we believe what youäó»re laying out here. What do I do tomorrow? What do Ido next month to start preparing my people and my workforce?äóù Getting to thatnext level of specificity äóî the human skills and how we get people ready äóî Ithink is really important. Thereäó»s a couple other fronts weäó»ve launched. Oneis on responsible AI, which we hinted at in the original article, but itäó»sreally become more important, which is: How do we make sure we get the rightoutcomes from AI? Speaking of things like transparency and explainability,which one of our job categories addresses; thinking about bias, which is anissue that many have run into when they apply AI äóî creating biased outcomesrather than inclusive outcomes; thinking about accountability; thinking abouttrustworthiness and issues like that. So weäó»re doing a lot of further work onthat. In fact, we have a new article in _MIT SMR_ on fairness and approachesto fairness with AI and some work weäó»ve done in that area. These are going tobe really important issues for businesses and organizations to grasp and tomake sure that as we have increasing numbers of people working in AI and morepowerful solutions delivered with the AI, how do we make sure we deliver theright outcomes in all cases?**Paul Michelman:** Terrific. Paul Daugherty, Jim Wilson, thank you both verymuch.**Paul Daugherty:** Thank you, Paul.**James Wilson:** Thank you, Paul.",Artificial Intelligence,0.985,Revisiting the Jobs Artificial Intelligence Will Create,standard32,https://news.ycombinator.com/item?id=19783574,conversations_hackernewsnew,16.519272,[],AI,"U.S. technology giant [Microsoft has teamed up with a Chinese militaryuniversity](https://www.ft.com/content/9378e7ee-5ae6-11e9-9dde-7aedca0a081a)to develop [artificial intelligencesystems](https://www.irishtimes.com/business/technology/microsoft-worked-with-chinese-military-university-on-ai-1.3855553) that could potentially enhancegovernment surveillance and censorship capabilities. Two [U.S. senatorspubliclycondemned](https://www.ft.com/content/5f5916fc-5be3-11e9-939a-341f5ada9d40)the partnership, but what the [National Defense Technology University ofChina](http://www.nudt.edu.cn/index_eng.htm) wants from Microsoft isnäó»t theonly concern.As [my researchshows](https://scholar.google.com/citations?user=OgVZmm4AAAAJ&hl=en), theadvent of digital repression is profoundly affecting [the relationship betweencitizen and state](https://doi.org/10.1353/jod.2019.0003). New technologiesare arming governments with unprecedented capabilities to monitor, track andsurveil individual people. Even governments in democracies with strongtraditions of [rule of law](https://theconversation.com/is-trumps-definition-of-the-rule-of-law-the-same-as-the-us-constitutions-77598) find themselvestempted to abuse [these new abilities](https://qz.com/813672/half-of-the-united-states-is-registered-in-police-facial-recognition-databases-and-its-completely-unregulated/).In states with [unaccountable institutions and frequent human rightsabuses](https://www.foreignaffairs.com/articles/world/2018-07-10/how-artificial-intelligence-will-reshape-global-order), AI systems will mostlikely cause greater damage. China is a prominent example. Its leadership hasenthusiastically embraced AI technologies, and has set up the worldäó»s [mostsophisticated](https://www.nytimes.com/interactive/2019/04/04/world/asia/xinjiang-china-surveillance-prison.html) [surveillancestate](https://www.engadget.com/2018/02/22/china-xinjiang-surveillance-tech-spread/) in [Xinjiangprovince](https://www.theguardian.com/world/2019/feb/18/chinese-surveillance-company-tracking-25m-xinjiang-residents), tracking citizensäó» daily movementsand smartphone use.Its exploitation of these technologies [presents a chillingmodel](https://www.georgesoros.com/2019/01/24/remarks-delivered-at-the-world-economic-forum-2/) for fellow autocrats and poses a direct threat to opendemocratic societies. Although thereäó»s no evidence that other governments havereplicated this level of AI surveillance, Chinese companies are activelyexporting the same underlying technologies across the world.[![](https://images.theconversation.com/files/270016/original/file-20190418-28097-1i209s9.jpg?ixlib=rb-1.1.0&q=45&auto=format&w=754&fit=clip)](https://images.theconversation.com/files/270016/original/file-20190418-28097-1i209s9.jpg?ixlib=rb-1.1.0&q=45&auto=format&w=1000&fit=clip)Surveillance in Chinaäó»s Xinjiang province includes both extensive policepatrols and surveillance cameras, like those on the building in thebackground. [AP Photo/Ng HanGuan](http://www.apimages.com/metadata/Index/China-Tracking-Face/cbbeb8deda184d58a0a1f17fab7e2564/9/0)## Increasing reliance on AI tools in the US[Artificial intelligencesystems](https://ai.stanford.edu/%7Enilsson/QAI/qai.pdf) are everywhere in themodern world, helping run smartphones, internet search engines, digital voiceassistants and Netflix movie queues. [Many people fail torealize](https://governanceai.github.io/US-Public-Opinion-Report-Jan-2019/)how quickly AI is expanding, thanks to ever-increasing amounts of data to beanalyzed, improving algorithms and advanced computer chips.Any time more information becomes available and analysis gets easier,governments are interested äóñ and not just authoritarian ones. In the U.S., forinstance, the 1970s saw revelations that government agencies äóñ such as theFBI, CIA and NSA äóñ had set up [expansive domestic surveillancenetworks](https://www.intelligence.senate.gov/sites/default/files/94755_II.pdf)to monitor and harass civil rights protesters, political activists and NativeAmerican groups. These issues havenäó»t gone away: Digital technology today hasdeepened the ability of even more agencies to conduct even more intrusivesurveillance.[![](https://images.theconversation.com/files/270024/original/file-20190418-28090-1lpg1vm.png?ixlib=rb-1.1.0&q=45&auto=format&w=237&fit=clip)](https://images.theconversation.com/files/270024/original/file-20190418-28090-1lpg1vm.png?ixlib=rb-1.1.0&q=45&auto=format&w=1000&fit=clip)How fairly do algorithms predict where police should be most focused? [ArnoutdeVries](https://commons.wikimedia.org/wiki/File:Criminaliteits_Anticipatie_Systeem.png)For example, U.S. police have eagerly embraced AI technologies. They havebegun using software that is [meant to predict where crimes willhappen](https://theconversation.com/why-big-data-analysis-of-police-activity-is-inherently-biased-72640) to decide where to send officers on patrol.Theyäó»re also using [facial recognition](https://www.nbcnews.com/news/us-news/facial-recognition-gives-police-powerful-new-tracking-tool-it-s-n894936)and [DNA analysis](https://www.washingtonpost.com/crime-law/2018/12/13/fbi-plans-rapid-dna-network-quick-database-checks-arrestees/) in criminalinvestigations. But analyses of these systems show the [data on which thosesystems are trained](https://theconversation.com/congress-takes-first-steps-toward-regulating-artificial-intelligence-104373) are often biased, leading to[unfair outcomes](https://theconversation.com/did-artificial-intelligence-deny-you-credit-73259), such as [falsely determining that African Americansare more likely to commit crimes](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) than other groups.## AI surveillance around the worldIn authoritarian countries, AI systems can directly abet domestic control andsurveillance, helping [internal security forces process massive amounts ofinformation](https://www.power3point0.org/2018/01/25/hybrid-repression-online-and-offline-in-china-foretelling-the-human-rights-struggle-to-come/) äóñincluding social media posts, text messages, emails and phone calls äóñ morequickly and efficiently. The police can identify social trends and [specificpeople](https://www.apnews.com/bf75dd1c26c947b7826d270a16e2658a) who mightthreaten the regime based on the information uncovered by these systems.For instance, the Chinese government has used AI in wide-scale crackdowns inregions that are home to ethnic minorities within China. Surveillance systemsin Xinjiang and Tibet have been described asäóì[Orwellian](https://foreignpolicy.com/2019/03/19/962492-orwell-china-socialcredit-surveillance/).äóù These efforts have included [mandatory DNAsamples](https://www.nytimes.com/2019/02/21/business/china-xinjiang-uighur-dna-thermo-fisher.html), Wi-Fi network monitoring and widespread facialrecognition cameras, all connected to integrated data analysis platforms. Withthe aid of these systems, Chinese authorities have, according to the U.S.State Department, äóìarbitrarily detainedäóù between [1 and 2 millionpeople](https://www.state.gov/j/drl/rls/hrrpt/humanrightsreport/index.htm?year=2018&dlid=289037#wrapper).My [research looks at 90 countries around theworld](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3374575) withgovernment types ranging from closed authoritarian to flawed democracies,including Thailand, Turkey, Bangladesh and Kenya. I have found that Chinesecompanies are [exporting AI surveillancetechnology](https://carnegieendowment.org/2019/01/22/we-need-to-get-smart-about-how-governments-use-ai-pub-78179) to at least 54 of these countries.Frequently, this technology is packaged as part of Chinaäó»s flagship [Belt andRoad Initiative](https://eng.yidaiyilu.gov.cn/), which is funding an extensivenetwork of roads, railways, energy pipelines and telecommunications networks[serving 60% of the worldäó»spopulation](https://www.knightfrank.com/blog/2018/01/30/an-insight-into-the-belt-and-road-initiative) and economies that generate 40% of global GDP.For instance, Chinese companies like[Huawei](https://e.huawei.com/us/solutions/industries/smart-city) and ZTE areconstructing äóìsmart citiesäóù in [Pakistan](https://www.dawn.com/news/1333101),[the Philippines](https://e.huawei.com/en/case-studies/global/2017/201704261658) and[Kenya](http://www.chinadaily.com.cn/world/2017-05/16/content_29372143.htm),featuring extensive built-in surveillance technology. For example, Huawei hasoutfitted [Bonifacio Global City](https://bgc.com.ph/) in the Philippines withhigh-definition internet-connected cameras that provide äóì[24/7 intelligentsecurity surveillance](https://e.huawei.com/en/case-studies/global/2017/201704261658) with data analytics to detect crime and helpmanage traffic.äóù[![](https://images.theconversation.com/files/270029/original/file-20190418-28094-xukhtb.jpg?ixlib=rb-1.1.0&q=45&auto=format&w=754&fit=clip)](https://images.theconversation.com/files/270029/original/file-20190418-28094-xukhtb.jpg?ixlib=rb-1.1.0&q=45&auto=format&w=1000&fit=clip)Bonifacio Global City in the Philippines has a lot of embedded surveillanceequipment. [alveo land/WikimediaCommons](https://en.wikipedia.org/wiki/File:Bonifacio_Global_City_2.jpg)[Hikvision](https://foreignpolicy.com/2018/06/13/in-chinas-far-west-companies-cash-in-on-surveillance-program-that-targets-muslims/),[Yitu](https://www.scmp.com/tech/social-gadgets/article/2142497/malaysian-police-wear-chinese-start-ups-ai-camera-identify) and[SenseTime](https://qz.com/1248493/sensetime-the-billion-dollar-alibaba-backed-ai-company-thats-quietly-watching-everyone-in-china/) are supplyingstate-of-the-art facial recognition cameras for use in places like[Singapore](https://www.albawaba.com/news/china%E2%80%99s-newest-global-export-policing-dissidents-1139230) äóñ which announced the establishment of asurveillance program with [110,000 cameras mounted on lampposts](https://www.reuters.com/article/us-singapore-surveillance/singapore-to-test-facial-recognition-on-lampposts-stoking-privacy-fears-idUSKBN1HK0RV)around the city-state. Zimbabwe is creating a [national imagedatabase](https://foreignpolicy.com/2018/07/24/beijings-big-brother-tech-needs-african-faces/) that can be used for facial recognition.However, selling advanced equipment for profit is different than sharingtechnology with an express geopolitical purpose. These new capabilities mayplant the seeds for global surveillance: As governments become increasinglydependent upon Chinese technology to manage their populations and maintainpower, they will face greater pressure to align with Chinaäó»s agenda. But fornow it appears that Chinaäó»s primary motive is to dominate the market for newtechnologies and make lots of money in the process.## AI and disinformationIn addition to providing surveillance capabilities that are both sweeping andfine-grained, AI can help repressive governments manipulate availableinformation and spread disinformation. These campaigns can be automated orautomation-assisted, and deploy [hyper-personalizedmessages](https://theconversation.com/solving-the-political-ad-problem-with-transparency-85366) directed at äóñ or against äóñ [specificpeople](https://www.nytimes.com/2018/10/20/us/politics/saudi-image-campaign-twitter.html) or groups.AI also underpins the technology commonly calledäóì[deepfake](https://www.technologyreview.com/s/612501/inside-the-world-of-ai-that-forges-beautiful-art-and-terrifying-deepfakes/),äóù in which algorithmscreate [realistic video and audioforgeries](https://theconversation.com/detecting-deepfake-videos-in-the-blink-of-an-eye-101072). Muddying the waters between truth and fiction may becomeuseful in a tight election, when one candidate could create fake videosshowing an opponent doing and saying things that never actually happened.VIDEO An early deepfake video shows some of the dangers of advancedtechnology.In my view, policymakers in democracies should think carefully about the risksof AI systems to their own societies and to people living under authoritarianregimes around the world. A critical question is how many countries will adoptChinaäó»s model of digital surveillance. But itäó»s not just authoritariancountries feeling the pull. And itäó»s also not just Chinese companies spreadingthe technology: Many U.S. companies, Microsoft included, but [IBM, Cisco andThermo Fisher](https://www.axios.com/china-us-technology-surveillance-state-5672b822-fdde-45f9-ac77-e7b5574e9351.html) too, have providedsophisticated capabilities to nasty governments. The misuse of AI is notlimited to autocratic states.",Artificial Intelligence,1,How Artificial intelligence systems could threaten democracy,standard33,https://news.ycombinator.com/item?id=19850611,conversations_hackernewsnew,16.519272,[],AI,"# Artificial Intelligence Can Now Copy Your Voice## Voice spying, cloning and voice reading on our mental health is just thebeginningVoice technologies arenäó»t just becoming the new customer touch point, itäó»sbecoming a new way to gather data from global citizens. Baiduäó»s AI can [cloneyour voice in seconds](https://medium.com/syncedreview/baidu-ai-can-clone-your-voice-in-seconds-93558a7b984f).Microsoft has a vision for[ conversationalAI](https://blogs.microsoft.com/ai/microsoft-build-future-of-natural-language/) in the future of the operating system. So does Huawei and manyothers. Smart speakers are probably spying on us gathering insights not justto improve their product.Honestly, they can even [tell if your havePTSD](https://futurism.com/artificial-intelligence-detect-ptsd-voice) just byyour voice. Just as AI with facial recognition can read your emotions on yourface. Meanwhile AI is giving us the ability o create fake humans, personaswith apparently human features that donäó»t really exist. Getting catfishes onthe internet by a fake human is now a real possibility in the not too distantfuture.* It takes just[ 3.7 seconds of audio to clone a voice](https://motherboard.vice.com/en_us/article/3k7mgn/baidu-deep-voice-software-can-clone-anyones-voice-with-just-37-seconds-of-audio).* Todayäó»s intelligent assistants are full of skills and they will get much smarter in the 2020s.AI isnäó»t just monetizing the internet for Ad-giants like Google, Facebook andAmazon, Artificial Intelligence is about to create a fake world of even morecomplexity.Smart speakers are going to explode in popularity in China in 2019, withAlibaba, Baidu and Xiaomi leading the way among others. Alibaba isnäó»t justlike Amazon, itäó»s bigger. As it matures in the cloud and as Huaweiäó»s profitsincrease, these two companies will eventually pose a real threat to AIdominance of Google and Microsoft.AI is creating a new world and we donäó»t really know the dangers of it, weäó»rejust going ahead like children into a world where AI regulation will becomenearly impossible.Now, with advances in artificial intelligence, the world is becoming moreartificial, and you canäó»t be sure what you see or hear is real or afabrication of artificial intelligence and machine learning. From incredibleAds of the future to äóìentitiesäóù we meet online, AI will transform our world tonot just being more immersive, but more confusing, complex and manipulative.The line between convenience and hacking humans (the opposite of enhancing us)is very real. Itäó»s so profitable to use AI to gain an edge over other firmsand reach people, the commercial weaponization of AI and our most intimatedata is really inevitable.Baiduäó»s research team used voice cloning techniques to develop the AI systemwhich they expect will have noteworthy applications in personalizing human-machine interface.Baiduäó»s research arm announced yesterday that its 2017 text-to-speech (TTS)system _Deep Voice_ has learned [how to imitate a personäó»svoice](https://arxiv.org/pdf/1802.06006.pdf) using a mere three seconds ofvoice sample data. ([Synced](https://medium.com/@Synced) is a greatpublication for AI).Huawei has been working on [emotionally intelligentAI](https://www.brecorder.com/2018/04/24/413804/huawei-to-introduce-emotionally-intelligent-artificial-intelligence/) for years. Alexa, Apple andSamsung are in the race for smarter interacts with their personal assistant AIvia earpods with 2019 being a pivotal year for the product from all threeproviders.Like all artificial intelligence algorithms, the more data voice cloning toolssuch as Deep Voice receive to train with the more realistic the results.Meanwhile companies like Spotify are integrating podcasts into how theyrecommend content that will be able to gather data on some of our coreinterests.The smart home invasion of Alexa and Google Home devices is nothing short of atreasure chest of our most intimate data. Information on demand and insightson users that were previously impossible. All thanks to the AI-voice interfacewhich is more immediate and will become ubiquitous in human societies, smartcities and the IoT in the next twenty years.The technique known as voice cloning, could be used to personalize virtualassistants such as Appleäó»s Siri, Google Assistant, Amazon Alexa; and Baiduäó»sMandarin virtual assistant platform DuerOS, which supports more than 50million devices in China with human-machine conversational interfaces.The frontiers of human interaction with AIs are broad and deep with incredibleimplications for customer relationships. The world we are building of AIs willbe incredible and potentially very transparent with regards to our data.Google unveiled[ Tacotron2](https://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html), a text-to-speech system that leverages the companyäó»s deep neuralnetwork and speech generationmethod[WaveNet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/). WaveNet analyzes a visual representation of audio called aspectrogram to generate audio. It is used to generate the voice for GoogleAssistant.Itäó»s becoming impossible to tell the difference between an AI and a human andthatäó»s incredibly problematic in a world where cybersecurity threats are onlyincreasing. The internet of things aspect of connectivity in the 4thindustrial revolution comes at a cost for privacy, censorship, dataharvesting, fraud, identity theft and consumer manipulation of ever morepersonalized digital advertisements.",Artificial Intelligence,0.996,Artificial Intelligence Can Now Copy Your Voice,standard34,stackoverflow-54994079,conversations_stackoverflow,16.420942,"['machine-learning', 'nlp', 'stanford-nlp', 'corpus']",AI,"<p>I am trying to learn how to create and train a corpus for relation-extraction. I have learned that I require a corpus in the conll format. However, I don't know how I should train the corpus.</p><p>Here is some code that I have to print out example text in the conll format. I am unsure how I would then modify this file with the appropriate changes, and then train with it.</p><pre><code>Properties props = new Properties();props.setProperty(""annotators"", ""tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,natlog,sentiment,kbp,quote"");props.setProperty(""coref.algorithm"", ""neural"");StanfordCoreNLP pipeline = new StanfordCoreNLP(props);String text = ""The modern definition of artificial intelligence (or AI) is \""the study and design of intelligent agents\"" where an intelligent agent is a system that perceives its environment and takes actions which maximizes its chances of success. "" + ""John McCarthy, who coined the term in 1956, defines it as \""the science and engineering of making intelligent machines. "" +""Other names for the field have been proposed, such as computational intelligence, synthetic intelligence or computational rationality. "" + ""The term artificial intelligence is also used to describe a property of machines or programs: the intelligence that the system demonstrates. "" + ""AI research uses tools and insights from many fields, including computer science, psychology, philosophy, neuroscience, cognitive science, linguistics, operations research, economics, control theory, probability, optimization and logic. "" + ""AI research also overlaps with tasks such as robotics, control systems, scheduling, data mining, logistics, speech recognition, facial recognition and many others. "" + ""Computational intelligence Computational intelligence involves iterative development or learning (e.g., parameter tuning in connectionist systems). "" + ""Learning is based on empirical data and is associated with non-symbolic AI, scruffy AI and soft computing. "" + ""Subjects in computational intelligence as defined by IEEE Computational Intelligence Society mainly include: Neural networks: trainable systems with very strong pattern recognition capabilities. "" + ""Fuzzy systems: techniques for reasoning under uncertainty, have been widely used in modern industrial and consumer product control systems; capable of working with concepts such as 'hot', 'cold', 'warm' and 'boiling'. "" + ""Evolutionary computation: applies biologically inspired concepts such as populations, mutation and survival of the fittest to generate increasingly better solutions to the problem. "" + ""These methods most notably divide into evolutionary algorithms (e.g., genetic algorithms) and swarm intelligence (e.g., ant algorithms). "" + ""With hybrid intelligent systems, attempts are made to combine these two groups. "" + ""Expert inference rules can be generated through neural network or production rules from statistical learning such as in ACT-R or CLARION. "" + ""It is thought that the human brain uses multiple techniques to both formulate and cross-check results. "" + ""Thus, systems integration is seen as promising and perhaps necessary for true AI, especially the integration of symbolic and connectionist models. "";// Annotate an example document.//CoreDocument doc = new CoreDocument(text); //pipeline.annotate(doc);String outputFile = ""ConnllTest1.txt"";OutputStream stream;try {stream = new FileOutputStream(outputFile);Writer w = new BufferedWriter( new OutputStreamWriter(stream));pipeline.conllPrint(pipeline.process(text), w);} catch (IOException e) {// TODO Auto-generated catch blocke.printStackTrace();}</code></pre>",Artificial Intelligence,1,Stanford-NLP kbp corpus training,standard35,t3_azrhh9,conversations_reddit,16.406103,,AI,"The importance of Artificial Intelligence is often understated and alsooverstating the same is quite difficult.. to get in-depth with how AI isactually bringing changes in MCA, we must get to the basics of what MedicalBilling and Coding really are.### MEDICAL CODINGMedical coding, if we talk about it at a very basic level -- is something thata coder takes, a written piece if you may, and translates it as accurately aspossible into a coded format such as numeric or alphanumeric code. The piecethat's taken for translation can be something such as a prescription formedication or a doctor's diagnosis or something else medical related. A codefor each and every event is created, these events can be of injuries,diagnosis or medical procedures.Presently, there are about a hundred thousand codes existing that are used formedical procedures, outpatient procedures, and diagnoses. Let us have a lookat a simple example of Medical Coding:Let's say that a patient has walked into a doctoräó»s office and he/she iscoughing tirelessly, they have a high production of mucus, and have a dreadedfever. Then a nurse walks up to the patient and asks them their symptoms, oncethe symptoms are noted, she performs some initial tests to get an idea of whatis actually going on, and then comes the doctor who analyzes and concludes thediagnoses saying that the patient is suffering from bronchitis. A medicationis then prescribed to the patient by the doctor.Now comes the interesting part of Modern Healthcare, each and every part ofthe visit is recorded by the clinic be it the doctor or someone in the officewho is authorized to carry out such operations. Then begins the coders jobthat is to translate all of relevant information of the visit into numeric andalphanumeric codes, which are ultimately used in the billing process.The medical coder should be equipped with the knowledge of a few sets andsubsets of code, let's take two of the subsets: International Classificationof Diseases (ICD) these codes correspond to a patientäó»s injury or sickness,and Current Procedure Terminology (CPT) that are related to the functions andservices Healthcare providers perform to the patient this can be as performingon them and performing for them.A task included for the Medical Coders is to translate every bit of data orinformation of the patient's visit to the clinic and shape it in the form of acode. There are different codes for different kinds of visits, some codes aremore specific these specific codes can be such as the patients symptoms, thetests performed by the doctor and the diagnosis procedure used by the doctor.The Medical Coders have to keep these guidelines in mind, they are veryimportant and can affect the status of a claim. The coding process concludeswhen the Medical Coder has entered the proper codes into the for or softwareprogram. This is where the job of the Medical Coder ends, now, all of this ispassed on to the Medical Biller.### MEDICAL BILLINGThe Medical Biller more so acts as a middleman between patients, healthcareproviders and the insurance companies. Their job, is on similar lines incontext to the Medical Coder. The Medical Biller translates the codes given bythe Medical Coder into a financial report, they make sure that the HealthcareProvider has been reimbursed appropriately for the services they've provided.Do not be fooled by the simplicity of the term ""Medical Billing"", it may seemthat all the Medical Billers task is to make a bill (Commonly known as a'Claim') for the insurance company by the help of the information provided bythe Medical Coder, the reality of the process is not as simple.Continuing with the previous example, the Medical Biller now looks at thecodes, that consist of information of things such as the kind of visit, thesymptoms, the diagnosis of the doctor, the medication prescribed by the doctorand then creates a Claim. The Claim is then sent and evaluated by theinsurance company, and then returned back. The bill of the patient is thenmade by biller who carefully re-evaluates the returned claim after theinsurance is removed, all of these tasks are performed through a form orsoftware, they're called as [Medical Billing SoftwareSolutions](https://www.osplabs.com/medical-billing-solutions/) and there's alarge number of companies out there creating such solutions.The biller takes a few factors into account such as the insurance plans of thepatient into account while creating the bill, this ensures that an accuratebill is produced. In cases where the patient shows signs refraining the billpayment, the Medical Biller has to take appropriate steps to ensure healthcareprovider is properly compensated.# Let's take a look at how a Traditional Medical Billing & Coding ProcessFlows:Talking about the word 'Traditional' you must have got an idea that itrequires **ALOT** of manual documentation and paper work, the average time fora traditional coding and billing process stretched on to about 5-7 weekswhereas in the modern automated system the process is reduced to as low as 2weeks.Following is a Claim-to-Payment Chase while using a traditional Paper-BasedSystem.1. Patient visits the doctoräó»s office.2. Patient check-in, gets treated.3. Doctor or their assistant writes a superbill for the treatment.4. The Medical Coder writes codes for the treatment.5. Medical Billers receive Paper forms who then format the data and forward it to insurance payers.6. Payer generates check and send payment to the provider.Now the point where AI fits into the story is to enhance the efficiency andefficacy of the billing and coding process. _Computer Assisted Coding (CAC) isa technology that works on the concept of Machine Learning (ML) which is abranch of Artificial Intelligence (AI) and Natural Language Processing (NLP),_they provide automated assistance to the rigorous task of identifying andextracting data from the given documents and inserting it into the system.",Artificial Intelligence,0.78,Rewriting of Medical Coding Automation using Artificial Intelligence,standard36,reddit_https://www.reddit.com/r/eos/comments/atcimm/effectai_is_migrating_to_eos/,conversations_reddit,16.386196,,AI,"<!-- SC_OFF --><div class=""md""><p>&#x200B;</p> <p>In April of 2019, <a href=""https://Effect.AI"">Effect.AI</a> will move to the EOS Blockchain.</p> <p>&#x200B;</p> <p><a href=""https://Effect.AI"">Effect.AI</a> is currently NEOäó»s most used dApp, and they are determined to be the leader of Decentralized Artificial Intelligence development.</p> <p>&#x200B;</p> <p>&#x200B;</p> <p><strong>More Info</strong>: <a href=""https://medium.com/effect-ai/effect-ai-brings-artificial-intelligence-to-eos-main-net-ead7e68e09fa"">https://medium.com/effect-ai/effect-ai-brings-artificial-intelligence-to-eos-main-net-ead7e68e09fa</a></p> <p>&#x200B;</p> <p>äóî </p> <p><strong>EOS News</strong></p> <p>&#x200B;</p> <p>Telegram: <a href=""https://t.me/EOSNEWS_English"">https://t.me/EOSNEWS_English</a></p> <p>Twitter: <a href=""https://twitter.com/EOSNews_Eng"">https://twitter.com/EOSNews_Eng</a></p> <p>Medium: <a href=""https://medium.com/@EOSNews"">https://medium.com/@EOSNews</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=""https://www.reddit.com/user/EOSBlockchainNews""> /u/EOSBlockchainNews </a> <br/> <span><a href=""https://www.reddit.com/r/eos/comments/atcimm/effectai_is_migrating_to_eos/"">[link]</a></span> &#32; <span><a href=""https://www.reddit.com/r/eos/comments/atcimm/effectai_is_migrating_to_eos/"">[comments]</a></span>",Artificial Intelligence,0.837,[ Effect.AI is migrating to EOS ],standard37,https://news.ycombinator.com/item?id=19808123,conversations_hackernewsnew,16.337688,[],AI,"# How-To Build Trust in Artificial Intelligence Solutions## A Psychologistäó»s Perspective on trust-building in AI and what mechanismscompanies need to understand to meet the needs of their customers and users.> We interviewed [Marisa Tschopp](https://www.linkedin.com/in/marisa-tschopp-0233a026/) who is an organizational psychologist conducting researchabout Artificial Intelligence from a humanities perspective with a focus onpsychological and ethical questions. She is also a corporate researcher at[scip AG](https://www.scip.ch/en/?labs.20190411), a technology andcybersecurity company based in Zurich. And she is the [Women inAI](https://www.womeninai.co/) Ambassador for Switzerland.**Please describe who you are in 2äóñ3 sentences.**Currently, I am focusing on trust in AI, Autonomous Weapons Systems, and ourAIQ project, which is a psychometric method to measure the skills of digitalassistants (conversational AI), like Siri or Alexa.So, obviously Iäó»m a researcher, but Iäó»m also a mother of two toddlers, a wife,a daughter, a sister, a volleyball player, hopefully a fun friend to be with,an activist, an idealist, a collaborator, and a semi-professional Sherpa (Ilove hiking in the Swiss Alps and therefore have to carry my kids on myback!).**Let us start with understanding trust better. What is trust and why is itimportant, especially in the context of AI?**In the context of AI, there is a critical underlying assumption: äóìNo trust, NoUseäóù. Since AI holds great promises (as well as dangers), tech-companies andAI enthusiasts are especially concerned about how to build trust in AI tofoster adoption or usage.> Trust seems like _the_ lasting, kind of mysterious, competitive edge.Without trust, there would be no family, no houses, no markets, no religion,no politics, no rocket science.According to trust researcher [RachelBotsman](https://medium.com/@rachelbotsman),> Trust is the social glue that enables humankind to progress throughinteraction with each other and the environment, including technology.Trust can be seen as a psychological mechanism to cope with uncertainty and islocated somewhere between the known and the unknown.Picture: [Rachel Botsman](https://medium.com/@rachelbotsman/trust-thinkers-72ec78ec3b59)Trust is deeply ingrained in our personality. We are basically born with atendency to trust or distrust people (or animals or any other things).Take for example this random picture of a woman: Do you trust her? Please ratebelow.[Nannie Doss](http://tonsoffacts.com/25-interesting-and-bizarre-facts-about-nannie-doss/)We, humans, have the unique capacity to tell in a snapshot if we trust thisperson or not. We look at the facial expression, body posture, or the context(background, surroundings, etc.). We compare it with memories or pastexperiences in split seconds, such as äóìshe reminds me of my grandmother.äóù> Generally speaking, what we know is that we tend to trust people more, whoare more like ourselves. One reason is, that it is easier for us to predictfuture behavior or reactions of persons who are alike, which lowers theemotional risk for us of being hurt.What we do not know is, how accurate our intuition is. Did you trust thiswoman above? Maybe yes, because she is smiling, relaxed. Maybe no, because youare already expecting some kind of trick here, as I am a psychologist.This woman is not very trustworthy. **She died several years ago in prison asone of the most famous female serial killers.****In the context of AI, if you ask the question can we trust in AI as atechnology,** then compared to other technologies, it is decisive tounderstand that often AI (for example Machine Learning, letäó»s say imageclassification), does not behave exactly the way it is intended, makesmistakes, or performs unethically. For example, when black people areclassified as gorillas or birds as missiles.#### The processes and outcome are hard to explain, sometimes not known atall, hence, not well predictable. Trusting this technology incorporates a wayhigher risk.So far, research has agreed upon three main pillars that need to be answeredto build trust,**1.) Performance:** Does it perform well? Is it safe? Is it built correctly?**2.) Process:** Does it perform the way we intended? Can we predict theoutcome?**3.) Purpose:** Do I have a good feeling about the intent of the program andthe provider? Does it adhere to ethical standards? Is it trustworthy?It is often said, that AI positively transforms almost every sector frommedicine to urban planning, but very importantly, it also brings questionableor even dangerous implications with it. From super-precise hacking of dataplatforms to the surveillance state and loss of privacy without opportunitiesfor public consent. So next to technical issues like a lack of predictabilityand explainability, the notion of negative outcomes, hype, complexity, anddisagreement within definitions and applications, leads to skepticism anddistrust.**How should non-experts and business owners, etc. approach this topic?**AI is already part of our daily lives, it is already increasingly being usedin decision-making when it comes to education, police, justice, recruitment orhealth.> I do not have a tech-background as well, I am a psychologist so I see thingsfrom a different perspective, and it may be easier for me to feel empathy withthe majority of people, who have no idea how to code or what an algorithm is.What fascinates me most and drives my research, is the question, how trust isestablished in the first place. You donäó»t really know the person or theproduct, its values or competencies. This first little glimpse of saying äóìyes,Iäó»ll go for itäóù.It is still a little mysterious, how this trust develops in the first place.**How can we best cope with it?** I think it is all about education,communication, and critical thinking. But there is something restricting theseskills or our will to engage in discussions about AI.> From a psychological perspective, this is one of the big problems: we arelacking cognitive freedom of choice. What concerns me, is that we are movingtowards a do-or-die relationship with AI. It will be almost impossible to getaway from AI, as much as we cannot get away from climate change.The fact that we are forced or threatened, like the threatening terminatorimages or the constant man losing against machine news, leads to resistance,denial, cynicism, and downplay. This is called reactance, a psychologicalphenomenon. When reactance occurs äóî we choose these behaviors äóî even if theyare totally irrational äóî **to simply restore our cognitive freedom of choiceand take back our sense of control.****This can be a big challenge, especially in consumer psychology** when youaim to convince customers to buy your products, whether itäó»s a car or arobotic vacuum cleaner.#### Consumers like all human beings want the freedom of choice and we need tofigure out ways, to make people want to explore AI by themselves, not becausethey are forced to do so.That is why management often applies bottom-up approaches within theircompany, rather than top-down decisions.Through this participative way of decision making, you aim to have all peopleaboard and share your vision and goals.Right now, one of the key issues is to change the way we talk about AI. Ithink we massively have to change the tone of the conversation about AI. Wemust move away from the hype, threat, and fear towards clear facts, vision,and why to create our own relationship with AI, and thus a new level of trust.This is also my vision as the ambassador for the [Women inAI](https://www.womeninai.co/) network, a nonprofit working towards a gender-inclusive AI that benefits global society.**Imagine a company is building a Machine Learning based product and juststarted prototyping. What steps would you suggest from a trust-buildingperspective?**What I learned from a philosopher is to always ask why, from the beginning tothe very end, and continuously at all milestones of the project.**What are the intended consequences and speculate about all possibleunintended consequences?**From the design perspective, it is all about aligning your design to at leastminimal ethical standards, to make sure you are building a trustworthyproduct. However, keeping in mind that technical performance (quality),security and safety, are all indispensable prerequisites.Going back to the beginning it is having the three pillars **process,performance, purpose** constantly in mind. Ethics in AI is all about integrityand authenticity.In the end, the task is to build a great, safe and ethically correct_product_. The focus naturally is at building a good product first, then comessecurity and this ethical stuff.It is natural to focus on the technical requirements first, whilstcounterintuitively, the latter should rather be looked at first. Two yearsago, when we started our trust research, our idea was to have like a proof ofquality to signal users or customers, that this is a trustworthy product. Thatis why we invented the AIQ, a psychometric measurement method to state,compare and track the skills of digital assistants. However, we were a bit toofast as the market is still in a development phase rather than actuallyimproving existing conversational AI. We too, focused on the technical skillsets at first, rather than the actual decisive soft factors of how trust isbuilt and developed.Here is a podcast episode that talks about the topic in more detail.Now we are stepping back and focus on the less obvious factors that influencetrust building in the context of AI. These are the fine influencing factors ona micro level of perception, from personality traits to bias, to pastexperiences, to socialization and upbringing. We are just gathering data toexplore these antecedents of trust in AI through associations, qualitative andquantitative methods.If you are interested to participate in the study you can fill out the form[here](https://docs.google.com/forms/d/e/1FAIpQLSev6HiA_ns1NWUqWa3jMyDz7r-j0rUYFx34U08-FvqFeQHH4w/viewform).**Is it possible to change the image of AI or influence consumer behaviorafter a product launch?**If you want to explore your trust image, you need to look at questions anddefinitions from various perspectives: you may want to look at the individualperson (like characteristics of your target group or employees), you can lookat the process from building, maintaining, and developing trust from aconsumer perspective, as well as destroying and regaining trust. You have tobe clear of the actors and roles (who is to be trusted?) and the situation: isit a high-risk situation like for example self-driving cars, or are we talkingabout an AI-driven chatbot in customer service?**In the end, the answer is yes** , however, in both directions, for betteror worse. We have to be very sensitive, neutral, or as Hans Roslin saysäóìfactfuläóù, when we talk about AI. Research is pretty clear on what to do tosustain a relationship or how to act, if you have broken a trust relationship,I am not sure if AI is then any different than other technologies. A breach isa breach, whether it is Facebookäó»s data breach or a misguided missile.> If there was a trust breach you must communicate instantly, directly,clearly what happened, explain yourself without being defensive, be authentic,truthful, and ask for what is needed to get another chance.**What books and other resources would you recommend for a business owner orproduct manager to learn about trust-building in AI?**I would suggest checking the [European High-Level ExpertGroup](https://ec.europa.eu/digital-single-market/en/high-level-expert-group-artificial-intelligence) on AI. They just released a framework for building[trustworthy AI.](https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai) The framework has three main pieces comprisinglawful AI, Ethical AI, and Robust AI. The key points of the two latter arediscussed in the report.Another great set of comprehensive, crowd-sourced standards comes from the[IEEE Global Initiative on Ethics of Autonomous and IntelligentSystems](https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai), which is called Ethically Aligned Design.Rachel Botsman: Who can you trust? She writes about how trust is built, lostand restored in the digital age, she also has several highly recommended [TEDTalks.](https://www.youtube.com/watch?v=CGwaPU8JwPQ)**Enjoyed this article? Here is another one.**#### [Subscribe here](https://tinyletter.com/Omdena) for practical tips,expert interviews, and use cases on how to build trusted and socially-beneficial AI solutions.",Artificial Intelligence,0.934,How-To Build Trust in Artificial Intelligence Solutions,standard38,https://news.ycombinator.com/item?id=19439197,conversations_hackernewsnew,16.337688,[],AI,"Learning to drive can be stressful. And if you happen to get paired with animpatient teacher or crazed family member it can be a nightmare. Along withthe stress, you might not learn enough to pass the driveräó»s test.In an attempt to make students more at ease and help them learn as efficientlyas possible, one driving school in China is taking out the human factoraltogether. A driving school in Zhenjiang has installed AI coaches in thevehicles. The artificial intelligence, which lives in an interactive screenattached to the dashboard, provides automatic voice navigation and will givedrivers unique commands.For example, the robot voice will remind drivers to turn on their signal asthe car approaches a turn or to slow down if they pick up too much speed.![AI Driving Coach Robot Artificial intelligence -YellRobot](https://yellrobot.com/wp-content/uploads/2019/03/AIDriver2-1024x570.jpeg)credit: Pear Video# Robot Driving Coach Keeps Students CalmThe artificial intelligence will also take control of the car if need be. Ifthe car gets closer than 8 inches to an obstacle, it will automatically stop.Students have remarked how the AI driving coach seems a bit more pleasant thantheir human counterparts.äóìIäó»m nervous when a real coach sits beside me. This robot is nicer. Even if Ido something wrong, the robot will continue to encourage me, instead ofblaming me,äóù said one student driver said.The robot can be set to a teaching mode or simulated test mode in which the AIwill let the driver know if they passed or not. The AI driving coach has nothit the busy streets of China yet as it seems limited to a closed course. Somestudents donäó»t feel something like this would be effective in a realenvironment.äóìNo robot coach would help me step on the brakes if I were accidentally todrive into a ditch,äóù one student commented._Sources:[Pear Video](https://www.pearvideo.com/video_1530905) /[ECNS](http://www.ecns.cn/news/2019-03-19/detail-ifzfmzhu2193194.shtml)_* * *Check out our articles on [AI helping to keep roadssafe](https://yellrobot.com/ai-road-management-system-weathernews/) and Tokyo2020 [Olympics using robots](https://yellrobot.com/robots-exoskeletons-tokyo-2020-olympic-panasonic-toyota/)",Artificial Intelligence,1,Artificial Intelligence Teaching Students How to Drive,standard39,t3_blqqqs,conversations_reddit,16.265099,,,"No petitions, surveys, or crowdfunding",No tags,,Are algorithms and artificial intelligence inherently prejudiced?,standard0,stackanswer-stackoverflow-56283560,conversations_stackoverflow-answers,10.26615,[],,"<p>Preciso de ajuda com minhas regras de seguraní_a do Firestore, pois ní£o estí£o permitindo updates ou deletes como eu queria que fizessem.</p><p>Tenho 2 coleí_íµes: Ofertas e Usuíçrios;Cada usuíçrio tem seu idUsuíçrio e um papel(leitor, administrador, editor)Cada oferta tem seu IdOferta e o IdUsuario(do usuíçrio que a criou).</p><p>Quero permitir que apenas quem criou a oferta possa fazer updates ou deletes nela.Isso funciona bem na funcí£o Proprietario();Quero ainda que os usuíçrios cujo campo Papel seja: administrador ou editor possam fazer updates ou deletes nas ofertas que ní£o foram criadas por eles.</p><p>Essa í© a parte em que ní£o funcionam as regras</p><p>Alguí©m pode me ajudar? Eu li 2 vezes tudo que achei no firebase sobre as regras, mas ní£o consigo criar uma que funcione.Eu entendi que eu precisaria de um get(/databases</p><pre><code>service cloud.firestore {match /databases/{database}/documents {match /ofertas/{oferta}{allow read: if logado();allow create: if logado();allow update: if proprietario();allow delete: if proprietario() || isAdmin();}match /usuarios/{usuario}{function isAdmin(){return resource.data.papel == ""administrador"";}allow read: if logado();allow create: if logado();allow update: if request.auth.uid == usuario;}function logado(){return request.auth!=null;}function proprietario(){return request.auth.uid == resource.data.idUsuario;}function temAcesso(){return resource.data.tipo in get(/databases/$(database)/documents/usuarios/$(tipo)).data.tipo;}}}</code></pre>",Cloud,0.935,Cloud Firestore Security Rules permission,standard1,stackanswer-stackoverflow-56424526,conversations_stackoverflow-answers,10.26615,[],cloud,"<p>Mainly Virtualization means, running multiple operating systems on a single machine but sharing all the hardware resources. And it helps us to provide the pool of IT resources so that we can share these IT resources in order get benefits in the business.</p><p>For the maintenance of resources in cloud computing environment, virtualization is a necessity as it makes it easier. Virtualization in Cloud Computing increases security as it protects both the integrity of guest virtual machines and cloud components. Cloud Component virtualized machines can also be scaled up or down on demand or can provide reliability. Resource Sharing, high utilization of pooled resources, rapid provisioning are also some of the factors Managed Service Provider VA provides.</p><p><a href=""https://www.esds.co.in/enlight-cloud-hosting"" rel=""nofollow noreferrer"">https://www.esds.co.in/enlight-cloud-hosting</a></p>",Cloud,1,Virtual machine and cloud computing,standard2,stackcomment-stackoverflow-96338493,conversations_stackoverflow-answers,10.2607975,,cloud,Hii. i stored a file in firebase cloud storage and i want to read or download that file. I tried what you said and deployed that cloud function. But i didn&#39;t get any data from that file and i didn&#39;t see any directory what i created in cloud functions folder . where it is created and located?,Cloud,1,Hii. i stored a file in firebase cloud storage and i want to read or download that file. I tried wha,standard3,stackcomment-stackoverflow-98670055,conversations_stackoverflow-answers,10.2607975,,cloud,"If you think there is a bug in Cloud Functions, you should <a href=""https://support.google.com/firebase/contact/support?page=bug_or_feature"" rel=""nofollow noreferrer"">file a bug report</a> for that. But I can assure you that the times on Cloud Functions are not off by four hours. That would be critically bad. What you are seeing is likely just a difference in timezone between your machine and Cloud Functions.",Cloud,0.92,"If you think there is a bug in Cloud Functions, you should <a href=""https://support.google.com/fireb",standard4,stackanswer-stackoverflow-56304821,conversations_stackoverflow-answers,10.2607975,[],cloud,"<p>See <a href=""https://firebase.google.com/docs/hosting/functions"" rel=""nofollow noreferrer"">https://firebase.google.com/docs/hosting/functions</a> .</p><blockquote><p>Cloud Functions for Firebase lets you automatically run backend code in response to HTTPS requests. Your code is stored in Google's cloud and runs in a managed environment. There's no need to manage and scale your own servers.</p><p>For example use cases and samples for Cloud Functions integrated with Firebase Hosting, visit our serverless overview.</p></blockquote>",Cloud,1,What exactly is &quot;source&quot; in the context of Firebase rewrites?,standard5,stackanswer-stackoverflow-55048298,conversations_stackoverflow-answers,10.258867,[],cloud,"<p>Yes, just open a Terminal in Cloud 9 and start your app.</p><p>Then click on ""Preview"" button in Cloud 9 toolbar.</p><p>Detailed step by step instructions, incl a few limitations are detailed here <a href=""https://docs.aws.amazon.com/cloud9/latest/user-guide/app-preview.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/cloud9/latest/user-guide/app-preview.html</a></p>",Cloud,1,Quickly run express app in AWS cloud9 from app generated from cloudstar?,standard6,stackanswer-stackoverflow-54986134,conversations_stackoverflow-answers,10.258068,[],cloud,"<p>Yes, it's the same datastore. Also called/soon-to-be <code>Cloud Firestore in Datastore mode</code> (which all older apps will be converted to at some point).</p><p>Yes, you can access it from anywhere, even from outside Google Cloud. From <a href=""https://cloud.google.com/appengine/docs/flexible/java/migrating#datastore"" rel=""nofollow noreferrer"">Cloud Datastore</a> (emphasis mine):</p><blockquote><p>You can access <a href=""https://cloud.google.com/datastore/docs"" rel=""nofollow noreferrer"">Cloud Datastore</a> from anywhere using the CloudDatastore API. Use the <a href=""https://cloud.google.com/sdk/cloud-client-libraries"" rel=""nofollow noreferrer"">Google Cloud client libraries</a> to store andretrieve data from Cloud Datastore.</p><p><strong>The same Cloud Datastore data is available regardless of if you use the App Engine libraries, the Google Cloud client libraries, or callthe API directly.</strong></p></blockquote><p>The major steps to access the datastore from a Cloud Function:</p><ul><li>you can't use the GAE-specific client libraries like the one you likely used in your old app, you'll have to use one of the generic <a href=""https://cloud.google.com/datastore/docs/reference/libraries"" rel=""nofollow noreferrer"">client libraries</a> (or the <a href=""https://cloud.google.com/datastore/docs/reference/data/rest/"" rel=""nofollow noreferrer"">REST</a> or <a href=""https://cloud.google.com/datastore/docs/reference/data/rpc/"" rel=""nofollow noreferrer"">RPC</a> APIs)</li><li>you'll have to give your <a href=""https://cloud.google.com/functions/docs/securing/function-identity"" rel=""nofollow noreferrer"">CF's Identity/service account</a> the proper access permissions, see <a href=""https://cloud.google.com/datastore/docs/reference/libraries#setting_up_authentication"" rel=""nofollow noreferrer"">Setting up authentication</a> and <a href=""https://cloud.google.com/datastore/docs/activate#other-platforms"" rel=""nofollow noreferrer"">Accessing your database from another platform</a>.</li></ul>",Cloud,1,Can Google Cloud functions share a datastore with Appengine?,standard7,stackcomment-stackoverflow-96585316,conversations_stackoverflow-answers,10.247631,,cloud,Could you share the code of your Cloud Function pls?,Cloud,1,Could you share the code of your Cloud Function pls?,standard8,stackcomment-stackoverflow-97264745,conversations_stackoverflow-answers,10.247631,,cloud,no i am not on cloud so i use metalLB,Cloud,1,no i am not on cloud so i use metalLB,standard9,stackanswer-stackoverflow-55389975,conversations_stackoverflow-answers,10.247631,[],cloud,"<p>I think it is now achievable through cloud functions<a href=""https://firebase.google.com/docs/reference/functions/functions.auth.UserBuilder"" rel=""nofollow noreferrer"">https://firebase.google.com/docs/reference/functions/functions.auth.UserBuilder</a></p>",Cloud,0.981,prevent firebase user from deleting himself,standard10,stackcomment-stackoverflow-98690262,conversations_stackoverflow-answers,10.247631,,cloud,"Possible duplicate of <a href=""https://stackoverflow.com/questions/49252427/point-cloud-library-with-visual-studio-2017"">Point Cloud Library with Visual Studio 2017</a>",Cloud,0.81,"Possible duplicate of <a href=""https://stackoverflow.com/questions/49252427/point-cloud-library-with",standard11,stackcomment-stackoverflow-98895789,conversations_stackoverflow-answers,10.247631,,cloud,"@DevAS you can use <a href=""https://documentation.onesignal.com/reference"" rel=""nofollow noreferrer"">Onesignal API</a> or <a href=""https://firebase.google.com/docs/cloud-messaging"" rel=""nofollow noreferrer"">Firebase Cloud Messaging</a>",Cloud,0.833,"@DevAS you can use <a href=""https://documentation.onesignal.com/reference"" rel=""nofollow noreferrer""",standard12,stackanswer-stackoverflow-56011926,conversations_stackoverflow-answers,10.244222,[],cloud,"<p>The two best ways to accomplish this goal would be by either using <a href=""https://cloud.google.com/functions/"" rel=""nofollow noreferrer"">Cloud Functions</a> or by using <a href=""https://cloud.google.com/dataflow"" rel=""nofollow noreferrer"">Cloud Dataflow</a>. For Cloud Functions, you would set up a trigger on the Pub/Sub topic and then in your code write to BigQuery. It would look similar to the <a href=""https://cloud.google.com/solutions/streaming-data-from-cloud-storage-into-bigquery-using-cloud-functions"" rel=""nofollow noreferrer"">tutorial on streaming from Cloud Storage to BigQuery</a>, except the input would be Pub/Sub messages. For Dataflow, you could use one of the <a href=""https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#cloudpubsubsubscriptiontobigquery"" rel=""nofollow noreferrer"">Google-provided, open-source templates to write Pub/Sub messages to BigQuery</a>.</p><p>Cloud Dataflow would probably be better suited if your throughput is high (thousands of messages per second) and consistent. If you have low or infrequent throughput, Cloud Functions would likely be a better fit. Either of these solutions would run constantly and write the messages to BigQuery when available.</p>",Cloud,1,GCP: Where to schedule PubSub subscriber which writes to BigQuery,standard13,stackoverflow-55072291,conversations_stackoverflow,10.236056,['ocr'],cloud,"<p>I'm looking for a cloud based solution to read the Machine Readable Zone from IDs or Passports to implement in our backend.</p><p>I tried some generic OCR solutions such as: </p><ol><li><strong>Amazon Rekognition</strong></li><li><strong>Google Vision</strong></li><li><strong>Microsoft Computer Vision</strong></li><li><strong>Teserract</strong> 3.0 / 4.0 (experimental)</li></ol><p>None of these provide accurate (sometimes not at all MRZ recognition)</p><p>I also tried some other tools specialized in MRZ OCR:</p><ol><li><strong>BlinkID</strong> from MicroBlink (which is very good but doesn't have a cloud solution)</li><li><strong>Accurascan</strong> (provides cloud solution but less accurate than BlinkID)</li><li><strong>Abbyy</strong> (too slow, 10~ seconds per request)</li></ol><p>Can you recommend me a good cloud solution for MRZ OCR of documents?</p>",Cloud,0.652,Reliable MRZ (Machine Readable) cloud API,standard14,stackanswer-stackoverflow-54995658,conversations_stackoverflow-answers,10.23395,[],cloud,"<blockquote><p>Any framework can support this kind of requirements?</p></blockquote><p>Express (the older Koa) is more widely supported </p><ul><li><a href=""https://github.com/awslabs/aws-serverless-express"" rel=""nofollow noreferrer"">https://github.com/awslabs/aws-serverless-express</a></li><li><a href=""https://github.com/yvele/azure-function-express"" rel=""nofollow noreferrer"">https://github.com/yvele/azure-function-express</a></li></ul><h1>More</h1><p>Cloud vendors have various associated services (e.g. hosted databases) that differ significantly. You are going to struggle to get complete cloud redundancy. You will be best served using the internal redundancy options in the cloud provider you choose.</p>",Cloud,1,How to deploy a Koa based TypeScript project into both AWS Lambda and Azure Cloud Function?,standard15,stackoverflow-55000865,conversations_stackoverflow,10.230841,"['google-cloud-platform', 'bigdata', 'data-lineage']",cloud,"<p>When we realize the data lake with GCP Cloud storage, and data processing with Cloud services such as Dataproc, Dataflow How can we generated data lineage report in GCP. Thanks.</p>",Cloud,0.819,How can I perform data lineage in GCP?,standard16,stackanswer-stackoverflow-56052645,conversations_stackoverflow-answers,10.227014,[],cloud,"<p>Google Cloud Run fits into your Serverless layer but as a container. The container infrastructure is managed for you. </p><p>Cloud Functions are limited in respect to the libraries, languages, and runtimes supported.</p><p>Cloud Run removes those limitations. You can use any language, combination of libraries and runtime that supports running within a container. </p><p>One limitation is that there is only one internal port <code>$PORT</code> which defaults to 8080 today. Externally both HTTP and HTTPS are supported. Both HTTP and HTTPS map to <code>$PORT</code>.</p><p>One big plus is that Cloud Run supports custom DNS names and custom SSL certificates. You can host your website on Cloud Run. As an experiment, I set up WordPress and Cloud SQL on Cloud Run and assigned it a DNS domain name with an SSL certificate.</p>",Cloud,1,Would I benefit using Cloud run instead of Cloud Functions? Where does it fit in GCP?,standard17,stackcomment-stackoverflow-96610628,conversations_stackoverflow-answers,10.21331,,cloud,Is the RGB image registered with the point cloud ?,No tags,,Is the RGB image registered with the point cloud ?,standard18,stackcomment-stackoverflow-96799601,conversations_stackoverflow-answers,10.21331,,cloud,can&#39;t you use a cloud image for your requirement ?,Cloud,0.877,can&#39;t you use a cloud image for your requirement ?,standard19,stackcomment-stackoverflow-96449605,conversations_stackoverflow-answers,10.21331,,cloud,"See: <a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html"" rel=""nofollow noreferrer"">Amazon EBS Volume Types - Amazon Elastic Compute Cloud</a>",Cloud,0.94,"See: <a href=""https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html"" rel=""nofollow",standard20,stackanswer-stackoverflow-55968136,conversations_stackoverflow-answers,10.208327,[],cloud,"<p>You can control the pace at which cloud functions are triggered by controlling the triggers themselves. For example, if you have set ""new file creation in a bucket"" as trigger for your cloud function, then by controlling how many new files are created in that bucket you can manage concurrent execution.Such solutions are not perfect though because sometimes the cloud functions fails and get restart automatically (if you've configure your cloud function that way) without you having any control over it. In effect, the number of active instances of cloud functions will be sometimes more than you plan.What AWS is offering is a neat feature though. </p>",Cloud,0.852,How to handle backpressure using google cloud functions,standard21,stackanswer-stackoverflow-55181268,conversations_stackoverflow-answers,10.208325,[],,"<p>Try by changing gradle version it worked me before, I too faced same issue few days back</p><pre><code>classpath 'com.android.tools.build:gradle:3.2.1'</code></pre>",Cloud,0.811,Firebase cloud messaging gradle settings,standard22,stackanswer-stackoverflow-55269119,conversations_stackoverflow-answers,10.208325,[],,"<p><code>firebase-functions</code> version 16.3.0, <a href=""https://firebase.google.com/support/release-notes/android"" rel=""nofollow noreferrer"">released 15 Mar 2019</a>, adds the ability to <a href=""https://firebase.google.com/docs/reference/android/com/google/firebase/functions/HttpsCallableReference"" rel=""nofollow noreferrer"">configure the timeout</a>.</p>",Cloud,0.763,Firebase Cloud Functions Change Timeout,standard23,stackanswer-stackoverflow-55939033,conversations_stackoverflow-answers,10.208325,[],cloud,"<p>when you are creating your postgres instance you have to allow access to the ip address from the postgres' client is running.</p><p>1.-Create your postgresql instance2.-In the Create a PostgreSQL instance window give the instance id and password to you postgres user in the äóìDefault user passwordäóù section.3.-clic on äóìShow configuration optionsäóù and locate äóìSet connectivityäóù, there You have to give access to Your pc ip address in the äóìAuthorized networksäóù under äóìPublic IPäóù section click on äóìAdd networkäóù introduce the ip into the äóìNetworkäóù box and click äóìdoneäóù, You can check the client ip address in the link[1] .4.-If you are done with the configurations click create.</p><p>Now to verify the connectivity from the client to the GCS i recommend you to do it the first time with the command line console.</p><p>1.-In you pc lunch the command line console, 2.-execute : psql -h [postgres instance ip address] -u postgres.You can follow the official documentation for äóìConnecting psql Client Using Public IPäóù in the link[2].</p><p>[1]myIPaddress.com[2]<a href=""https://cloud.google.com/sql/docs/postgres/connect-admin-ip"" rel=""nofollow noreferrer"">https://cloud.google.com/sql/docs/postgres/connect-admin-ip</a></p>",Cloud,1,Connect pgAdmin4 to cloud SQL,standard24,stackanswer-stackoverflow-55015193,conversations_stackoverflow-answers,10.208325,[],cloud,"<p><a href=""https://cloud.google.com/nodejs/docs/reference/firestore/0.20.x/Firestore#runTransaction"" rel=""nofollow noreferrer""><code>runTransaction</code></a> returns a promise. You need to <code>await</code> it.</p><pre><code> await admin.firestore().runTransaction(...);</code></pre>",Cloud,0.663,Firebase cloud transaction triggers error,standard25,stackanswer-stackoverflow-56478275,conversations_stackoverflow-answers,10.208325,[],,<p>You can inject and use <code>SpanCustomizer</code> interface to customize the current span (add tags / logs etc.) or inject and use <code>Tracer</code> to retrieve the current span ad manipulate it.</p>,Cloud:Artificial Intelligence,0.68:0.992,Spring cloud slueth SpanAccessor interface,standard26,stackanswer-stackoverflow-55070500,conversations_stackoverflow-answers,10.207169,[],cloud,"<p>If you do not have a name set for some-service, and it's a 3rd party service, I think the better approach would be to call it via RestTemplate or something. </p><p>Feign client needs to have the service name configured and known, for it to call that particular service in the network using service discovery. </p>",Cloud,0.819,Spring Cloud Kubernetes FeignClient Error,standard27,stackanswer-stackoverflow-55955908,conversations_stackoverflow-answers,10.207169,[],cloud,"<p>Check the Google Cloud Status Dashboard if there are service interruptions for the day that your system encountered the issue.</p><p><a href=""https://status.cloud.google.com/"" rel=""nofollow noreferrer"">https://status.cloud.google.com/</a></p>",Cloud,1,Firebase Cloud Functions deploy error,standard28,stackanswer-stackoverflow-55829960,conversations_stackoverflow-answers,10.207169,[],cloud,"<p>From <a href=""https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service</a>:</p><blockquote><p>When using a Service with spec.type: LoadBalancer, you can specify theIP ranges that are allowed to access the load balancer by usingspec.loadBalancerSourceRanges. This field takes a list of IP CIDRranges, which Kubernetes will use to configure firewall exceptions.This feature is currently supported on Google Compute Engine, GoogleKubernetes Engine, AWS Elastic Kubernetes Service, Azure KubernetesService, and IBM Cloud Kubernetes Service. This field will be ignoredif the cloud provider does not support the feature.</p></blockquote><p>May be your cloud simply does not support it.</p><p>You can use other things that allow blocking by source IP, like nginx or ingress-nginx. In ingress-nginx you just specify list of allowed IPs in annotation <code>ingress.kubernetes.io/whitelist-source-range</code>. </p><p>If you want to go Nginx or other proxy route - don't forget to change Load Balancer Service <code>externalTrafficPolicy</code> to <code>Local</code>. Otherwise you will not see real client IPs.</p>",Cloud,1,spec.loadBalancerSourceRanges for Linode Cloud provider,standard29,stackanswer-stackoverflow-55846997,conversations_stackoverflow-answers,10.207169,[],,"<p>There is a <a href=""https://github.com/dmacvicar/terraform-provider-libvirt"" rel=""nofollow noreferrer""><code>libvirt</code> provider</a> that is able to manage resources in KVM and is listed on the <a href=""https://www.terraform.io/docs/providers/type/community-index.html"" rel=""nofollow noreferrer"">community providers page</a>.</p>",Cloud,0.802,Using terraform in private cloud,standard30,stackanswer-stackoverflow-56144304,conversations_stackoverflow-answers,10.207169,[],,"<p>If you want to transform your array elements into a new array, you could also use <a href=""https://developer.mozilla.org/de/docs/Web/JavaScript/Reference/Global_Objects/Array/map"" rel=""nofollow noreferrer""><code>Array.map</code></a>.</p><pre><code>allUsers = users.map(user =&gt; user.data());</code></pre>",Artificial Intelligence,0.809,Typescript Cloud function not compiling,standard31,stackanswer-stackoverflow-55180992,conversations_stackoverflow-answers,10.203379,[],,"<p>Take a look at the <a href=""https://firebase.google.com/support/release-notes/android"" rel=""nofollow noreferrer"">Firebase Android Release Notes</a> for the latests versions available for the different cores of <em>Firebase</em> functionalities. </p><pre><code>implementation 'com.google.firebase:firebase-messaging:17.4.0' </code></pre><p>Try updating it to the latest version available. </p>",No tags,,Firebase cloud messaging gradle settings,standard32,stackanswer-stackoverflow-55350592,conversations_stackoverflow-answers,10.203379,[],,"<p>The problem was in puppeteer. I downgraded from the version 1.13.0 to 1.11.0 and now everything works fine. See the discussion <a href=""https://github.com/GoogleChrome/puppeteer/issues/3944"" rel=""nofollow noreferrer"">here</a></p>",Cloud,0.747,Cloud functions timeout on page.goto(),standard33,stackanswer-stackoverflow-55862329,conversations_stackoverflow-answers,10.203379,[],cloud,"<p>I got the same kind of error.<br>In my case, changing Node version to 8 fixed this error.</p><p><a href=""https://medium.com/google-cloud/migrating-firebase-cloud-functions-to-node-8-aebdb0d3d9a9"" rel=""nofollow noreferrer"">https://medium.com/google-cloud/migrating-firebase-cloud-functions-to-node-8-aebdb0d3d9a9</a></p><pre><code>Function failed on loading user code. Error message: Code in file index.js can't be loaded.Is there a syntax error in your code?Detailed stack trace: /user_code/node_modules/@google-cloud/logging/node_modules/gaxios/build/src/index.js:28async function request(opts) {^^^^^^^^SyntaxError: Unexpected token functionat createScript (vm.js:56:10)at Object.runInThisContext (vm.js:97:10)at Module._compile (module.js:549:28)at Object.Module._extensions..js (module.js:586:10)at Module.load (module.js:494:32)at tryModuleLoad (module.js:453:12)at Function.Module._load (module.js:445:3)at Module.require (module.js:504:17)at require (internal/module.js:20:19)at Object.&lt;anonymous&gt; (/user_code/node_modules/@google-cloud/logging/node_modules/gtoken/build/src/index.js:18:18)</code></pre>",Cloud,0.883,Firebase Cloud functions deployment error,standard34,stackanswer-stackoverflow-56071817,conversations_stackoverflow-answers,10.203379,[],,"<p>Do you tried like this ?</p><pre><code> @JvmField@PropertyName(""championship-name"")var championshipName: String = """"@PropertyName(""championship-name"")get() = field@PropertyName(""championship-name"")set(value) { field = value }</code></pre>",Artificial Intelligence,0.843,Using PropertyName in Cloud Firestore,standard35,stackoverflow-55948083,conversations_stackoverflow,10.193657,"['google-cloud-platform', 'google-cloud-sql']",cloud,"<p>I'd like to call Cloud SQL API described below from Cloud Functions.</p><ul><li><a href=""https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export?hl=en"" rel=""nofollow noreferrer"">https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export?hl=en</a></li></ul><p>And I found libraries to call GCP APIs.</p><ul><li><a href=""https://github.com/googleapis/google-cloud-node"" rel=""nofollow noreferrer"">https://github.com/googleapis/google-cloud-node</a></li><li><a href=""https://github.com/googleapis/google-cloud-go"" rel=""nofollow noreferrer"">https://github.com/googleapis/google-cloud-go</a></li></ul><p>However, there does not seem to be any modules for Cloud SQL.</p><p>I'm wondering why it's not implemented. is the reason that the APIs are relatively new? or that I misunderstand the purpose of the libraries and actually it shouldn't be implemented in the libraries?</p>",Cloud,0.966,Why aren&#39;t there any SDKs to call Cloud SQL API?,standard36,stackoverflow-55999325,conversations_stackoverflow,10.193644,"['google-cloud-functions', 'serverless-framework']",cloud,"<p>I use serverless framework to manage my cloud functions. Some of them are of HTTP type. Recently, all the HTTP functions started to fail with 403 error. No matter if you enter a URL in a browser or trigger it with the cloud scheduler. The only place where it works is the <em>testing</em> tab of the function in the cloud console, when you click the ""<em>Test the function</em>"" button.</p>",Cloud,1,Google HTTP Cloud Function returns 403,standard37,stackcomment-stackoverflow-96575509,conversations_stackoverflow-answers,10.1908245,,cloud,I meant creating w/ the use of Cloud Deployment Manager. I can create via Google Cloud Console but what I want is via Cloud Deployment Manager. I can not find anything in their documentation regarding this,Cloud,1,I meant creating w/ the use of Cloud Deployment Manager. I can create via Google Cloud Console but w,standard38,stackcomment-stackoverflow-97612111,conversations_stackoverflow-answers,10.1908245,,cloud,This is my entire code on cloud function to convert the uploaded audio file from cloud storage bucket to text using speech api.... the code throws error saying that invalid arguments on calling speech api....,Cloud,0.92,This is my entire code on cloud function to convert the uploaded audio file from cloud storage bucke,standard39,stackcomment-stackoverflow-96602387,conversations_stackoverflow-answers,10.178435,,cloud,c4f4t0r: will approach differ for cloud or premises or hybrid environment?,Cloud,0.888,c4f4t0r: will approach differ for cloud or premises or hybrid environment?,standard40,stackanswer-stackoverflow-55031080,conversations_stackoverflow-answers,10.178435,[],cloud,<p>Model export feature is currently not supported in Cloud AutoML Vision.</p>,Cloud,1,Exporting a model to be implemented in mobile app,standard41,stackanswer-stackoverflow-55015300,conversations_stackoverflow-answers,10.165311,[],cloud,"<p>The <code>runTransaction</code> method returns a <code>Promise</code>.And as the <a href=""https://firebase.googleblog.com/2017/06/keep-your-promises-when-using-cloud.html"" rel=""nofollow noreferrer"">this blog post</a> says: You have to return that <code>Promise</code>.</p><p>And these is important:</p><blockquote><p>...if you want a function to stay alive during async work, you can do this by returning a promise from the function (except for HTTP/S triggers, which require a response sent to the client).</p></blockquote><p>Or in other words: If you don't return that <code>Promise</code>, your function could finish without completing the <code>transaction</code>.</p><pre><code>import * as functions from 'firebase-functions';import * as admin from 'firebase-admin';admin.initializeApp();exports.addShard = functions.firestore.document(`likes/{docID}`).onCreate(async (snap, context) =&gt; {const postID: string = snap.data().postID;const randNum: number = (Math.floor(Math.random()*3+1)); const postRef = admin.firestore().doc(`post/${postID}/count_shards/${randNum}`);return admin.firestore().runTransaction(async transaction =&gt; {const postShard = (await transaction.get(postRef)).data();postShard.count += 1;return transaction.update(postRef, postShard);});});</code></pre>",Cloud,0.812,Firebase cloud transaction triggers error,standard42,stackanswer-stackoverflow-55047103,conversations_stackoverflow-answers,10.165311,[],cloud,"<p>It is a good practice to externalise any kind of sensitive information.</p><p>If I were you, I would use Environment Variables. This <a href=""https://docs.aws.amazon.com/lambda/latest/dg/eventsources.html#eventsources-sqs"" rel=""nofollow noreferrer"">answer</a> explains how to fetch Env Variables in Java.</p><p>If you are running (or plan to run) on the Cloud, you can take a look into your provider's way to store parameters. AWS, for example, uses <a href=""https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html"" rel=""nofollow noreferrer"">Systems Manager Parameter Store</a></p><p>Other options would be to read a file during runtime, look up in a database, etc. You get the idea.</p>",Cloud,0.944,Optimize properties variables Spring Cloud,standard43,stackanswer-stackoverflow-54881290,conversations_stackoverflow-answers,10.165311,[],cloud,"<p>You cannot change the lifetime from Google side, as the doc you post said, Google side (Cloud VPN) just negotiates with the lifetime with the on-premise, being the max 36,000 seconds (10 hours) for Phase1.</p><p>In any case, you will need to change this at your on-premise side. </p>",Cloud,0.775,Adjusting GCP Cloud VPN Lifetime,standard44,stackanswer-stackoverflow-55477194,conversations_stackoverflow-answers,10.165311,[],,"<ol><li>Comment or cut your function</li><li>Deploy</li><li>Uncomment or paste back the function</li><li>Rename the function</li><li>Deploy</li><li>Rename the function back</li><li>Deploy</li></ol>",Artificial Intelligence,0.626,Firebase cloud function deploy error,standard45,stackanswer-stackoverflow-55959852,conversations_stackoverflow-answers,10.165311,[],cloud,<p>You donäó»t save image into firestore but image url. After you save image into storage you have to get the url of that image and then save the url to firestore. Just google these steps or see documentation for more info</p>,Cloud:Artificial Intelligence,0.965:0.684,Saving image to Cloud firestore,standard46,stackanswer-stackoverflow-55839927,conversations_stackoverflow-answers,10.165311,[],cloud,"<p>There is no such thing as ""Firebase functions"". What you're referring to is Cloud Functions with Firebase tools layered on top of it.</p><p>You can connect Cloud Functions (with Firebase or not) to an external database as long as you're on the Blaze payment plan, which allows you to make outgoing connections to some third party service.</p>",Cloud,1,Firebase functions with cloud sql,standard47,stackanswer-stackoverflow-56274380,conversations_stackoverflow-answers,10.165311,[],cloud,"<p>Node 8 is available for cloud functions now. Try upgrading your environment.</p><p>You probably just need to:</p><ul><li>Add ""engines"": { ""node"": ""8"" } to your /functions/package.json. <a href=""https://github.com/firebase/functions-samples/blob/Node-8/stripe/functions/package.json"" rel=""nofollow noreferrer"">Example</a>. </li></ul><p>In case it still doesn't work: </p><ul><li>Upgrade your firebase-functions to the latest version</li><li>Upgrade firebase-tools to the latest version</li></ul>",Cloud,0.851,Object.values() in Firebase Cloud Functions,standard48,stackoverflow-55838902,conversations_stackoverflow,10.16206,['google-cloud-platform'],cloud,"<p>I checked out ""Google Cloud Platform Marketplace"", most solutions are built to ""launch on compute engine"".</p><p>Are there any ""Google Cloud Platform Marketplace Solutions"" built to ""launch on app engine""?</p><p>A good percentage of my projects are built and processed on app engine. If I can find these app engine solutions from ""Google Cloud Platform Marketplace"", that will save me a lot of development time.</p>",Cloud,0.99,Google Cloud Platform Marketplace launch on compute engine. How about Marketplace solutions launch on app engine?,standard49,stackoverflow-55513746,conversations_stackoverflow,10.15982,"['text-files', 'google-cloud-pubsub']",cloud,"<p>i am trying to publish data to Cloud Pub Sub .Data is in JSON format and is being kept in my local folder. I am not using Cloud Storage and trying to read the pubsub message directly through cloud function. Tested the flow with manually passing messages and the data is getting inserted into Bigquery tables also. Only place i got stuck is, how will i pass a .txt file JSON dataset to Cloud PubSub</p><p>Sample data{""ID"":6,""NAME"":""Komal"",""AGE"":22,""ADDRESS"":""Assam"",""SALARY"":20000}</p><p>Can any one give me a hint!!</p><p>I could see various options using cloud storage and all, here i am reading the changed data from DB table,insert those records into 1 dummy table and converting the data from that table to JSON format and writing to a .txt file. From here if i could publish the data to pubsub , entire flow will get completed</p><p>If i manually pass like below , the data will get inserted</p><p>gcloud pubsub topics publish pubsubtopic1 --message {""ID"":6,""NAME"":""Komal"",""AGE"":22,""ADDRESS"":""Assam"",""SALARY"":20000}</p><p>Thanks In Advance</p>",Cloud,1,Read a txt file JSON data to publish the messages in Cloud Pub Sub,standard0,https://news.ycombinator.com/item?id=19394898,conversations_hackernewsnew,16.17212,[],AI,"Threelly uses state of the art A.I. to analyze videos for key insights:topics, scenes, people, sentiments, and much more.Threelly uses state of the art Artificial Intelligence algorithms to automatically analyze videos to locate and pull the precise location of key points of interest like - topics, scenes, people, sentiments, brands, expressions, labels and much more. Allowing you to rapidly gain intelligent insights from any video.How does it work?-----------------1. Install2. Go to YouTube.com3. Watch your favorite videos in YouTube as usual - magic will happen :)AI Powered Video Insights. Gain deep insights with time-based tags that are created automatically. Threellyäó»s unparalleled AI recognition recognizes a huge variety of visual concepts and objects; recognize faces, logos or known graphics; Visual tags, Celebrity recognition*, Person recognition*, Speech-to-text, Visual text (OCR), Known graphics & logos, Locations & landmarks and much more. All powered by state of the art Artificial Intelligence (AI) algorithms. Radically Simple. Threelly is built to be innovative and radically simple. It installs right away with a single click and works directly in the Chrome browser. No need to download files or install complicated software. Use the clean, simple and intuitive interface to personalize YouTube insights to your liking. Create your own list of time-based tags to reveal insights into the videos you care about. Take The Quantum Leap Forward With Threelly. YouTubers, Students, Educators, Broadcasters, studios, gamers, or anyone consuming video on YouTube will benefit greatly from Threelly. Threelly built to remove friction from interacting with video; bringing AI-DRIVEN INSIGHTS to your fingertips. Our commitment is to give you insights QUICKLY and save you TIME.This extension works on Chrome Browsers, whether it's Windows, Mac OS X or Linux.== Feedback and bug reports ==Threelly is still in beta, and we're constantly making updates, fixing bugs and adding new features. If you're having issues or found a bug, please don't post your bug report here, contact us directly so we can fix it: yourfriends@threelly.com== Enjoying Threelly SmartView? Please rate! == FIVE STAR REVIEWS REQUESTEDIf you like THREELLY SMARTVIEW, please help spread the word by giving it a 5 star rating here ;)If you're not fully satisfied, please contact us yourfriends@threelly.com and we'll make sure to help.Stay in touch:For new feature announcements or just to say hello, please follow @Threelly123 on Twitter or instagram http://twitter.com/threelly123Like us on Facebook: http://facebook.com/threelly123v.1.25 Release Notes:+ Minor improvements and optimizationsäìî Resolved issue with YouTube player pausing at 60 secs.äìî Resolved issue with slice bar not refreshing upon change of video.äìî User can copy to clipboard a direct link to a ""slice"" for sharing.",Artificial Intelligence,0.976,Unlocking Insights from Videos with Artificial Intelligence,standard1,https://news.ycombinator.com/item?id=19712347,conversations_hackernewsnew,16.17212,[],AI,"# Ai Hashtags#### Generate Hashtags using Artificial Intelligence_The iOS app has been built, but I can't afford to pay $ 99/year to submit afree app,_[More details](https://smakosh.com/solving-hashtags-problems)",Artificial Intelligence,0.546,Show HN: Generate Hashtags Using Artificial Intelligence,standard2,https://news.ycombinator.com/item?id=19798999,conversations_hackernewsnew,16.17212,[],AI,"# Artificial Intelligence will Enhance and Hack HumanityThe truth is, itäó»s complicated and will become even more complicated. AI isalready being used to hack us in multiple ways, from us gifting our data andprivate info to technology firms to how our attention on our mobile devices isfunneled.The internet has created new pillars that monetizes the future where AI isnäó»tjust not regulated, it hasnäó»t even hit its stride in how ubiquitous it willbecome.* Attention economy* Surveillance capitalism* AI in driving global GDP* AI in cybersecurity* AIäó»s role in technological loneliness (äóìdivide and conquer in the smart homeäóù)If AI has become the new äóìarms raceäóù between the likes of the U.S and China,we stand at the dawn of a new era where [AI will bothenhance](https://www.wired.com/story/wired25-kai-fu-lee-fei-fei-li-artificial-intelligence/) and hack us in ways we cannot yet imagine.After reading [Jun Wu](https://medium.com/@junwu_46652)äó»s piece on [Empathy inArtificial Intelligence](https://towardsdatascience.com/empathy-in-artificial-intelligence-eb167f62af99), it got me thinking. Male bias and themilitarization of AI is very dangerous for humanity. Itäó»s probably anexistential threat to our survival as a species. I donäó»t just mean this in theElon Musk terms of AGI, but in how we fail to regulate it.#### China will Lead Ethics in AI in the 21st CenturyAs China rises to dominance both technologically and economically that willbecome more pronounced in the 2030s, their approach to ethics in AI will beginto dominate the world. We can already tell that they will be the leaders inhow facial recognition informs universal systems of surveillance, themanifestation of social credit systems and the monetization of data in Asia.* Facial recognition and its ethical implications.* AI in the rise of Universal Surveillance Architecture Systems (USAS)* Emergence of Social Credit Systems* Monetization of data in Asia[China is bestpositioned](https://www.ft.com/content/f92abc38-6bb8-11e9-80c7-60ee53e6681d)in the future to influence and impact policy regarding the regulation of AI.They have started to think seriously about the [ethics ofAI.](https://www.scmp.com/topics/artificial-intelligence) Their thinking isnot yet mature, but if China can be a leader globally in any signal way thatis most important to the future of humanity, itäó»s on the [regulation & ethicsof AI](https://www.scmp.com/business/commodities/article/2157700/ethics-and-pursuit-artificial-intelligence).Before that happens they will likely make a lot of mistakes and seriousbreaches of AI ethics, and its impact on things like human rights.#### The Age of Biotechnology & AI is an Ethical MinefieldWired recently interviewed Fei-Fei Li, you can read the interview[here](https://www.wired.com/story/will-artificial-intelligence-enhance-hack-humanity/). Here are some bullet points from that event:* The era of Biotechnology will allow people to hack their humanity in ways that were never possible before.* AI will be deeply implicated in how we hack other humans and alter even our DNA.* The era of AI and biotech enhancements is coming, so the question is, who decides what is a good enhancement and what is a bad enhancement?* YNH: _The easiest people to manipulate are the people who believe in free will, because they think they cannot be manipulated._An awesome debate. Yuval Noah Harari & Fei-Fei Li### The 4th Industrial Revolution* In the 4th industrial revolution is an era of great convenience and major ethical questions that could lead to irreparable damage in how we evolve as a species (it could lead to our extinction).* What are the good qualities we need to enhance? What happens when our military is automated and robotic? Is enhancing ourselves with AI dangerous?* The technological pressure of innovation for monetization also means powerful corporations could easily become too powerful where they place corporate profits ahead of ethical considerations. In 2019, we already have a long list of such occurrences.* There is already a crisis between engineers and their managers playing itself out at companies such as Google (and many other $1 Trillion dollar firms). The employees of BigTech are speaking out, with mixed results regarding the ethics of the work they are doing. AI is nearly always implicated in this.#### Enhancing Means Hacking HumanityAs companies like Amazon and Google invade the smart home, our private dataand health data will become a major issue in the ethics of AI. This is becausesimultaneously these companies will become giants in the AI of the future ofhealthcare.* Google itself launched and[ dissolved its own AI ethics board](https://www.theverge.com/2019/4/4/18296113/google-ai-ethics-board-ends-controversy-kay-coles-james-heritage-foundation) due to major controversy surrounding it.* In 2019, the debate about ethics in AI also centers on if putting _Advertising_ at the center of the internet is healthy for the world. Facebook & Google among others are implicated in this on-going debate.Apple and Facebook are pivoting into privacy architectures and walled gardensbased on subscription revenue (Apple) and the monetization of encrypted Chat(Facebook).The advent of artificial intelligence forces humanity, both governments andthe corporate sector to organize rules and guidelines on artificialintelligence, bio-technologies and robots that protects the fate of thespecies. Realistically this will take decades to find a global consensus on.### The 2020s are a äóìBlack Mirroräóù decade for Ethics in AIBefore that takes place thereäó»s a period where hacking (rather than enhancing)human becomes the wild-wild west of cybersecurity invasion. You can think ofsmartphone addiction and mobile app engineering as part of this stage.Fines against companies like Google & Facebook for monopolistic and privacydata infractions are slow and relatively minor compared to the scale,influence and power of these firms and the utilities they provide.#### Artificial General IntelligenceThe possibility of AGI manifesting in the 21st century is low, but still anexistential threat to humanity. More important that how humanity deals withother life forms in our galaxy, guidelines for how to deal with AGI should itever manifest needs also to be constructed. As for artificial intelligence inthe current era, itäó»s impossible to regulate.We need better AI to moderate and regulate AI. Itäó»s exponential growth in howmachine intelligence and algorithms are impacting us is a very difficult topicconsidering the lack of checks and balances in capitalism. Itäó»s likely thatthe 2020s are a very messy decade in terms of how humanity is hacked bycorporations. It could realistically lead to a surveillance architecture thatour parents might easily consider a dystopia.",Artificial Intelligence,0.964,Artificial Intelligence Will Enhance and Hack Humanity,standard4,https://news.ycombinator.com/item?id=20055498,conversations_hackernewsnew,16.17212,[],AI,"Microsoft unveiled a [curriculum](https://docs.microsoft.com/en-us/learn/paths/ai-business-school-government/) in its Artificial IntelligenceBusiness School this week that is specifically tailored for governmentdecision-makers.äóìThe fact is that government workers across the boardäóîand especially decision-makersäóîdonäó»t necessarily have that familiarity or depth on AI,äóù AnthonySalcito, Microsoftäó»s vice president for government, said in a[statement](https://blogs.microsoft.com/ai-for-business/2019/05/27/government-ai-school/?utm_source=ai-blog&utm_campaign=1735). äóìThis new learning path is away to get them introduced to the concept and to understand why itäó»s importantin the context of government work.äóùThe tech giant launched its AI Business School in March to help executives andother business leaders better understand how to implement the technologythrough a free online master class. Now that more than 140,000 people havegained practical guidance from the AI course material to date, the company isoffering a new path that is designed especially for government agencies.Through various course modules, government insiders can learn about thecomponents that comprise a strong AI strategy, principles to guide responsibleAI adoption, lessons on fostering an äóìAI-readyäóù culture and other insightsaround using the tech to better serve their constituents.The course content also includes a case study demonstrating how a city inFinland has integrated AI to more efficiently serve citizens and a demo thatshowcases how the government can utilize intelligent bots to help constituentsaccess needed resources, among other materials and a video lecture.Microsoft is also expressly gearing the curriculum to governments of all sizesand Salcito said the new learning path is motivated by Microsoftäó»s äóìkeypriority,äóù which is to help the government serve its constituents throughcloud services.äóìWe believe this course is valuable for government decision-makers at alllevels äóî from small municipalities to large cities,äóù Salcito said. äóìThe beautyof artificial intelligence technologies is their scalability.äóù",Artificial Intelligence,1,Microsoft Unveils Artificial Intelligence Course for Government,standard5,stackoverflow-55373454,conversations_stackoverflow,16.111567,AI,"<p>I am new on artificial intelligence and I am using TensorFlow object detection API to detect a product on images, so It already detecting object but I wanna to get coordinates Xmax,Xmin Ymax and Ymin for each object on the images.</p><p>That is the images with object detected, so in this case was detected 2 object on the image.</p><p>Link:</p><p><a href=""https://i.ibb.co/CVd1xLR/download.png"" rel=""nofollow noreferrer"">https://i.ibb.co/CVd1xLR/download.png</a></p><p>We can see that I got the coordinates of objects but its no clearly, there more than 3 coordinates in the output and I just want to get the amount of coordinates as the number of objects that are in the image. </p><p>This the code which provide the output </p><pre><code>with detection_graph.as_default():with tf.Session(graph=detection_graph) as sess:image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')num_detections = detection_graph.get_tensor_by_name('num_detections:0')print(detection_graph.get_tensor_by_name('detection_boxes:0'))for image_path in TEST_IMAGE_PATHS:boxes = detect_objects(image_path)print(boxes)</code></pre><p>Output </p><pre><code>Tensor(""detection_boxes:0"", dtype=float32)[[[0.16593058 0.06630109 0.8009524 0.5019088 ][0.15757088 0.5376015 0.8869156 0.9394863 ][0.5966009 0.88420665 0.6564093 0.9339011 ]...[0. 0. 0. 0. ][0. 0. 0. 0. ][0. 0. 0. 0. ]]]</code></pre><p>I want to get something like that. only the coordinates of the Bounding Box. We assuming that they are the coordinates of the objects.</p><pre><code>[0.16593058 0.06630109 0.8009524 0.5019088 ][0.15757088 0.5376015 0.8869156 0.9394863 ]</code></pre>",Artificial Intelligence,1,"how to get bounding box [Xmax,Xmin,Ymax,Ymin] from tensorflow object detection",standard,14,t3_b651cx,conversations_reddit,15.941394,,,"No petitions, surveys, or crowdfunding",No tags,,Three Pioneers in Artificial Intelligence Win Turing Award,standard15,https://news.ycombinator.com/item?id=19362775,conversations_hackernewsnew,15.875434,[],AI,"### Export Results as a new summarized PDFYou've made your search in thousands of documents, and you've found pagesabout your search in dozens of them. You need to share the results with yourcolleagues. Do you have to share all documents containing hundreds of pages toshare just dozens of related pages? Not anymore.PDF Search allows you to export the most relevant pages in search results as anew PDF document. So you can share a summary report with your friends with asingle document.",Cloud,0.81,Use of Artificial Intelligence to search within documents,standard16,https://news.ycombinator.com/item?id=19510796,conversations_hackernewsnew,15.875434,[],AI,"Russian banks are stepping up the use of [artificialintelligence](https://www.computerweekly.com/ehandbook/A-Computer-Weekly-buyers-guide-to-automation-and-AI-in-systems-management) (AI) as thetechnologyäó»s unprecedented evolution looks set to see it boost theircompetitiveness, but shortages of people with the right skills, as well asinfrastructural issues, are hampering the technologyäó»s wider applications.äóìAI is undergoing a period of unprecedented evolution. Over the next fewyears, the technology will progress so far that AI will be employed infinancial institutions just as often as humans,äóù said [SergeyPutyatinsky](https://mkb.ru/en/about/corporate-governance/management-board/putyatinskiy-sergey-evgenevich), deputy chairman of Credit Bank ofMoscow (CBoM)[](https://mkb.ru/en/about/corporate-governance/management-board/putyatinskiy-sergey-evgenevich). äóìActive use of AI technology will be adecisive factor in banksäó» competition in mass segments.äóùAccording to a study conducted by the [local rating agency ExpertRA](https://www.raexpert.com/), in cooperation with the Centre for FinancialTechnologies, Russian banks most often use AI in credit analysis.Other areas where AI adoption has been increasing, according to the study, aredebt collection and marketing, including creation of individual [offers forcustomers](https://www.computerweekly.com/news/252441507/One-fifth-of-global-banks-think-AI-will-boost-customer-experience).Meanwhile, the study revealed that Russian banks mostly put their hopes on AIin such areas as uncovering fraudulent transactions, debt collection andcredit scoring, while automating call centres by [introducing chatbots](https://searchcrm.techtarget.com/essentialguide/Guide-to-AI-in-customer-service-using-chatbots-and-NLP), using AI in algorithmic trading, [humanresources (HR)management](https://searchhrsoftware.techtarget.com/podcast/Testing-the-intelligence-of-AI-in-HR-applications) and remote customer identification aregenerally considered less promising.According to the studyäó»s authors, however, the latter areas may notnecessarily be dismissed by lenders as unsuitable for AI adoption, but it isdifficult to come up with the returns on investment (ROI) from applying thetechnology to those areas. Meanwhile, Russian banks often prefer to adopt thetechnology by small steps.äóìWe are pragmatic about the adoption of äóÖhypedäó» technologies,äóù saidPutyatinsky. äóìWe normally start with smaller-scale pilot projects that allowus to evaluate the potential usefulness of the technology and build up in-house competences.äóìWherever possible, we use open-source software. We make calculations forevery project to determine if it is financially viable and, based on that, wemake decisions on whether to greenlight it.äóùAccording to Putyatinsky, CBoMäó»s priority areas for AI technology areprocessing full-text documents, making loan decisions, dealing with over-duedebts and financial monitoring.Another major Russian lender, Rosbank, uses AI for processes involving [riskevaluation](https://searchenterpriseai.techtarget.com/feature/AI-in-insurance-forces-big-changes-to-traditional-industry), loan issuing, optimisation of thebranch network, uncovering fraud, communications and interaction withcustomers.äóìWe believe that over the next one to two years, AI will also be adopted forthe bankäó»s other processes that are not directly linked to interaction withcustomers,äóù said [Dmitry Smirnov, head of Rosbankäó»s datalab](https://www.linkedin.com/in/smirdm/?locale=de_DE).äóìAccumulating large amounts of data and the arrival of new data sources willfacilitate that. We are actively exploring areas where AI could be potentiallyadopted. These are processes aimed at improving the organisationäó»s efficiencyand, from the customeräó»s viewpoint, processes that simplify their interactionwith the bank.äóùMeanwhile, Promsvyazbankäó»s main area for AIäó»s application includes creditdecision-making, uncovering fraud and forming offers for customers.äóìCurrently, we are working on broadening the scope of AI application,äóù saidDaniil Tkach, head of the customer relations department at[Promsvyazbank](https://en.wikipedia.org/wiki/Promsvyazbank).äóìIn the short term, automated systems will tell us which products would be thebest offer for a customer, what channels will be the most efficient and whatcommunication style will be most amenable to the customer.äóùAccording to Tkach, the main conditions for wider spread of AI include asufficient degree of automation and manageability, reliable systems for datacollection and a sufficient number of reiterations of processes for learningpurposes. He said this is applicable for just about any banking processes,such as sales, communications, anti-fraud and operations.äóìWe could also single out intellectual management systems, in which AIsubstantially helps superiors to understand the quality of work by theiremployees and provides tips to all employees for possibly improving theirwork,äóù Tkach said.Still, Russian banks often see AI as a technology that could help automate newareas rather than replace already existing automation solutions.äóìWe are not trying to revamp existing solutions,äóù CBoMäó»s Putyatinsky said.äóìInstead, we look at areas that have not yet been automated and startautomating them from scratch with the use of new technology.äóù### __Obstacles to AI adoptionMeanwhile, the process of [adopting AI in the bankingsector](https://www.computerweekly.com/news/252447328/Barclays-appointment-will-step-up-use-of-AI-in-investment-bank) is not always smooth. There are[obstacles in theway](https://www.computerweekly.com/news/252452506/OpenStack-Foundation-will-tackle-infrastructure-barriers-to-enterprise-AI-adoption) of the technologyäó»swider spread across the industry.According to the Expert RA study, those obstacles include discrepancies withdata in information systems, but once the issue of data consistency isresolved, finding qualified personnel to process data is set to be a majorchallenge.Industry insiders have already been complaining about difficulties in findingqualified personnel to operate AI-based solutions.äóìThe main factors that are impeding the adoption and development of AI areshortages of qualified professionals and problems with the infrastructure ofinformation systems,äóù said Smirnov.Putyatinsky agreed, saying: äóìThe acutest issue is training of qualifiedpersonnel.äóùTo help resolve this challenge, CBoM has been running an internship programmecalled [IB Universe](https://www.globalbankingandfinance.com/artificial-intelligence-in-banking-industry-conversion-to-genuine-benefits/) for the past12 months. äóìThis allows students and recent graduates to acquire practicalexperience in various areas of investment business,äóù added Putyatinsky.According to Putyatinsky, educational programmes of that kind will eventuallyallow banks to train personnel in the working environment, producing a newwave of employees who will already be prepared to deal with new technologies,such as AI and machine learning.Another issue with application of AI is complexity of the technologyäó»salgorithms, Promsvyazbankäó»s Tkach said. äóìContemporary [machine learningalgorithms](https://searchcio.techtarget.com/answer/How-do-machine-learning-algorithms-differ-from-traditional-algorithms) are so complex that humans haveproblems understanding decisions made by AI,äóù he added.Over the next few years, progress with adopting AI systems in Russiaäó»s bankingindustry is set to largely depend on investments in regional networks,personnel training and banksäó» ability to attract and retain customers,according to the Expert RA study.äóìThe good news is that at this point, a bank doesnäó»t need to make enormousinvestment to become one of the [Russian banking industryäó»s] AI leaders,äóù saidthe studyäó»s authors. äóìBut the bad news is that to achieve that, you have toact right now.äóù",Artificial Intelligence,1,Artificial intelligence making major inroads into Russian banking,standard17,https://news.ycombinator.com/item?id=19517785,conversations_hackernewsnew,15.875434,[],AI,"Computers which are capable of teaching themselves to predict premature deathcould greatly improve preventative healthcare in the future, suggests a newstudy by experts at the University of Nottingham.The team of healthcare data scientists and doctors have developed and tested asystem of computer-based 'machine learning' algorithms to predict the risk ofearly death due to chronic disease in a large middle-aged population.They found this AI system was very accurate in its predictions and performedbetter than the current standard approach to prediction developed by humanexperts. The study is published by _PLOS ONE_ in a special collections editionof ""Machine Learning in Health and Biomedicine"".The team used health data from just over half a million people aged between 40and 69 recruited to the UK Biobank between 2006 and 2010 and followed up until2016.Leading the work, Assistant Professor of Epidemiology and Data Science, DrStephen Weng, said: ""Preventative healthcare is a growing priority in thefight against serious diseases so we have been working for a number of yearsto improve the accuracy of computerised health risk assessment in the generalpopulation. Most applications focus on a single disease area but predictingdeath due to several different disease outcomes is highly complex, especiallygiven environmental and individual factors that may affect them.""We have taken a major step forward in this field by developing a unique andholistic approach to predicting a person's risk of premature death by machine-learning. This uses computers to build new risk prediction models that takeinto account a wide range of demographic, biometric, clinical and lifestylefactors for each individual assessed, even their dietary consumption of fruit,vegetables and meat per day.""We mapped the resulting predictions to mortality data from the cohort, usingOffice of National Statistics death records, the UK cancer registry and'hospital episodes' statistics. We found machine learned algorithms weresignificantly more accurate in predicting death than the standard predictionmodels developed by a human expert.""The AI machine learning models used in the new study are known as 'randomforest' and 'deep learning'. These were pitched against the traditionally-used'Cox regression' prediction model based on age and gender - found to be theleast accurate at predicting mortality - and also a multivariate Cox modelwhich worked better but tended to over-predict risk.Professor Joe Kai, one of the clinical academics working on the project, said:""There is currently intense interest in the potential to use 'AI' or 'machine-learning' to better predict health outcomes. In some situations we may find ithelps, in others it may not. In this particular case, we have shown that withcareful tuning, these algorithms can usefully improve prediction.""These techniques can be new to many in health research, and difficult tofollow. We believe that by clearly reporting these methods in a transparentway, this could help with scientific verification and future development ofthis exciting field for health care.""This new study builds on previous work by the Nottingham team which showedthat four different AI algorithms, 'random forest', 'logistic regression','gradient boosting' and 'neural networks', were significantly better atpredicting cardiovascular disease than an established algorithm used incurrent cardiology guidelines. This earlier study is available here.The Nottingham researchers predict that AI will play a vital part in thedevelopment of future tools capable of delivering personalised medicine,tailoring risk management to individual patients. Further research requiresverifying and validating these AI algorithms in other population groups andexploring ways to implement these systems into routine healthcare.###**Disclaimer:** AAAS and EurekAlert! are not responsible for the accuracy ofnews releases posted to EurekAlert! by contributing institutions or for theuse of any information through the EurekAlert system.",Artificial Intelligence,1,"Artificial intelligence can predict premature death, study finds",standard18,https://news.ycombinator.com/item?id=19930331,conversations_hackernewsnew,15.875434,[],AI,"This video is the product of Dessa Engineers, Hashiam Kadhim, Joseph Palermo,and Rayhane Mama. The Engineers used artificial intelligence to recreate Joe Roganäó»s voice,generating the most human-like voice synthesis to date. The audio you arelistening to is 100% generated from the artificial intelligence model. Themodel even learned to generate breaths and mouthing sounds where it sees fitin order to make the speech sound most natural. Find out more about thisproject in this blog post (linked below), or check out www.fakejoerogan.com tosee if you can beat our AI model! If you have any other questions please reach out to real.talk@dessa.com Blog: [https://medium.com/@dessa_/real-talk-...](/redirect?redir_token=1wN8IgXCJCvxXsdIj51vF6VvIth8MTU1ODExMDQ4NUAxNTU4MDI0MDg1&q=https%3A%2F%2Fmedium.com%2F%40dessa_%2Freal-talk-speech-synthesis-5dd0897eef7f&v=DWK_iYBl8cA&event=video_description) Learn more about Dessa here:[https://dessa.com/](/redirect?redir_token=1wN8IgXCJCvxXsdIj51vF6VvIth8MTU1ODExMDQ4NUAxNTU4MDI0MDg1&q=https%3A%2F%2Fdessa.com%2F&v=DWK_iYBl8cA&event=video_description) Please note that this project does not suggest that we endorse the views andopinions of Joe Rogan. Joe was selected as a demonstrative model for thepurposes of displaying the capability of this technology.",Artificial Intelligence,1,We Recreated Joe Rogan's Voice Using Artificial Intelligence,standard19,t3_b0xde1,conversations_reddit,15.850653,,,"No petitions, surveys, or crowdfunding",No tags,,How tech giants are investing in artificial intelligence,standard20,t3_b62lq6,conversations_reddit,15.850653,,,"No petitions, surveys, or crowdfunding",No tags,,Three Pioneers in Artificial Intelligence Win Turing Award,standard21,t3_b8lrlg,conversations_reddit,15.718902,,AI,"We did research about how AI can be taught to joke. For our project we takememe dataset from iFunny and try to create a funny caption generator. Therewere several different approaches:1. Searching nearest caption to theme of image by cluster2. Searching nearest caption to theme of image by visual similarity3. Transferring the image descriptor into the vector space of text descriptors4. Generating captions using Markov ChainsIn more details you can read in our blog post <https://heartbeat.fritz.ai/can-artificial-intelligence-be-taught-how-to-joke-7c7d53a3492a>. We are happy toanswer any questions about our work and discuss other approaches.",Artificial Intelligence,1,[P] Can artificial intelligence be taught how to joke?,standard22,t3_bv7mlc,conversations_reddit,15.718902,,,For news and announcements from and about Google,Cloud,0.809,Google and Microsoft Using Artificial Intelligence to Fight Hackers,standard23,https://news.ycombinator.com/item?id=19322342,conversations_hackernewsnew,15.684645,[],AI,"1. 1.Turing, A. M. Computing machinery and intelligence. _Mind_ **49** , 433äóñ460(1950).2. 2.Jordan, M. I. & Mitchell, T. M. Machine learning: trends, perspectives, andprospects. _Science_ **349** , 255äóñ260 (2015).3. 3.Mitchell, T. M. _Machine Learning_ (McGraw-Hill Science/Engineering/Math,Boston, Mass, USA, 1997).4. 4.Peek, N., Combi, C., Marin, R. & Bellazzi, R. Thirty years of artificialintelligence in medicine (AIME) conferences: a review of research themes._Artif. Intell. Med._ **65** , 61äóñ73 (2015).5. 5.Yu, K. H., Beam, A. L. & Kohane, I. S. Artificial intelligence in healthcare._Nat. Biomed. Eng._ **2** , 719äóñ731 (2018).6. 6.Lynch, C. J. & Liston, C. New machine-learning technologies for computer-aideddiagnosis. _Nat. Med._ **24** , 1304äóñ1305 (2018).7. 7.Wong, D. & Yip, S. Machine learning classifies cancer. _Nature_ **555** ,446äóñ447 (2018).8. 8.Zhang, W., Chien, J., Yong, J. & Kuang, R. Network-based machine learning andgraph theory algorithms for precision oncology. _npj Precis. Oncol._ **1** ,25 (2017).9. 9.Ching, T. et al. Opportunities and obstacles for deep learning in biology andmedicine. _J. R. Soc. Interface_ **15** , 20170387 (2018).10. 10.Richter, A. N. & Khoshgoftaar, T. M. A review of statistical and machinelearning methods for modeling cancer risk using structured clinical data._Artif. Intell. Med._ **90** , 1äóñ14 (2018).11. 11.Esteva, A. et al. Dermatologist-level classification of skin cancer with deepneural networks. _Nature_ **542** , 115äóñ118 (2017).12. 12.LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. _Nature_ **521** , 436äóñ444(2015).13. 13.Goodfellow, I., Bengio, Y. & Courville, A. _Deep Learning_ (MIT Press,Cambridge, Mass, USA, 2016).14. 14.Coudray, N. et al. Classification and mutation prediction from non-small celllung cancer histopathology images using deep learning. _Nat. Med._ **24** ,1559äóñ1567 (2018).15. 15.Ehteshami Bejnordi, B. et al. Diagnostic assessment of deep learningalgorithms for detection of lymph node metastases in women with breast cancer._JAMA_ **318** , 2199äóñ2210 (2017).16. 16.Rawat, W. & Wang, Z. Deep convolutional neural networks for imageclassification: a comprehensive review. _Neural Comput._ **29** , 2352äóñ2449(2017).17. 17.Bailey, M. H. et al. Comprehensive characterization of cancer driver genes andmutations. _Cell_ **173** , 371äóñ385 (2018).18. 18.Ghahramani, Z. Probabilistic machine learning and artificial intelligence._Nature_ **521** , 452äóñ459 (2015).19. 19.Touw, W. G. et al. Data mining in the Life Sciences with Random Forest: a walkin the park or lost in the jungle? _Brief. Bioinform._ **14** , 315äóñ326(2013).20. 20.Azuaje, F. Computational models for predicting drug responses in cancerresearch. _Brief. Bioinform._ **18** , 820äóñ829 (2017).21. 21.Zhao, L., Lee, V. H. F., Ng, M. K., Yan, H. & Bijlsma, M. F. Molecularsubtyping of cancer: current status and moving toward clinical applications._Brief. Bioinform._ <https://doi.org/10.1093/bib/bby026> (2018).22. 22.Karczewski, K. J. & Snyder, M. P. Integrative omics for health and disease._Nat. Rev. Genet._ **19** , 229äóñ310 (2018).23. 23.Li, Y., Wu, F. X. & Ngom, A. A review on machine learning principles formulti-view biological data integration. _Brief. Bioinform._ **19** , 325äóñ340(2018).24. 24.Ramazzotti, D., Lal, A., Wang, B., Batzoglou, S. & Sidow, A. Multi-omic tumordata reveal diversity of molecular mechanisms that correlate with survival.Preprint at <https://www.biorxiv.org/content/early/2018/10/14/267245> (2018).25. 25.Kim, D. et al. Knowledge boosting: a graph-based integration approach withmulti-omics data and genomic knowledge for cancer clinical outcome prediction._J. Am. Med. Inform. Assoc._ **22** , 109äóñ120 (2015).26. 26.Klughammer, J. et al. The DNA methylation landscape of glioblastoma diseaseprogression shows extensive heterogeneity in time and space. _Nat. Med._**24** , 1611äóñ1624 (2018).27. 27.Yu, K. H. et al. Association of omics features with histopathology patterns inlung adenocarcinoma. _Cell Syst._ **5** , 620äóñ627 (2017).28. 28.Gevaert, O. et al. Glioblastoma multiforme: exploratory radiogenomic analysisby using quantitative image features. _Radiology_ **273** , 168äóñ174 (2014).29. 29.Disselhorst, J. A. et al. Linking imaging to omics utilizing image-guidedtissue extraction. _Proc. Natl. Acad. Sci. U.S.A._ **115** , E2980äóñE2987(2018).30. 30.Pan, S. J. & Yang, Q. A survey on transfer learning. _IEEE Trans. Knowl. DataEng._ **22** , 1345äóñ1359 (2010).31. 31.Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. & Wojna, Z. Rethinking theinception architecture for computer vision. Preprint at<https://arxiv.org/abs/1512.00567> (2015).32. 32.Sevakula, R. K., Singh, V., Verma, N. K., Kumar, C. & Cui, Y. Transferlearning for molecular cancer classification using deep neural networks._IEEE/ACM Trans. Comput. Biol. Bioinform._<https://doi.org/10.1109/TCBB.2018.2822803> (2018).33. 33.Turki, T. W., Wei, Z. & Wang, J. T. L. Transfer learning approaches to improvedrug sensitivity prediction in multiple myeloma patients. _IEEE Access_ **5**, 7381äóñ7393 (2017).34. 34.Tan, M. Prediction of anti-cancer drug response by kernelized multi-tasklearning. _Artif. Intell. Med._ **73** , 70äóñ77 (2016).35. 35.Shaikhina, T. & Khovanova, N. A. Handling limited datasets with neuralnetworks in medical applications: a small-data approach. _Artif. Intell. Med._**75** , 51äóñ63 (2017).36. 36.Choi, C. et al. RETAIN: an interpretable predictive model for healthcare usingreverse time attention mechanism. Preprint at<https://arxiv.org/abs/1608.05745> (2016).37. 37.Lahav, O., Mastronarde, N. & van der Schaar, M. What is interpretable? Usingmachine learning to design interpretable decision-support systems. Preprint at<https://arxiv.org/abs/1811.10799> (2018).38. 38.Alaa, A. M. & van der Schaar, M. Forecasting individualized diseasetrajectories using interpretable deep learning. Preprint at<https://arxiv.org/abs/1810.10489> (2018).39. 39.Castelvecchi, D. Can we open the black box of AI? _Nature_ **538** , 20äóñ23(2016).40. 40.Lundberg, S. M. et al. Explainable machine-learning predictions for theprevention of hypoxaemia during surgery. _Nat. Biomed. Eng._ **2** , 749äóñ760(2018).41. 41.Li, O., Liu, H., Chen, C. & Rudin, C. Deep learning for case-based reasoningthrough prototypes: a neural network that explains its predictions. Preprintat <https://arxiv.org/abs/1710.04806> (2017).42. 42.Chen, C., Li, O., Barnett, A., Su, J. & Rudin, C. This looks like that: deeplearning for interpretable image recognition. Preprint at<https://arxiv.org/abs/1806.10574> (2018).43. 43.Ancona, M., Ceolini, E., í_ztireli, C. & Gross, M. Towards better understandingof gradient-based attribution methods for deep neural networks. Preprint at<https://arxiv.org/abs/1711.06104> (2018).44. 44.Fabris, F., Doherty, A., Palmer, D., de Magalhaes, J. P. & Freitas, A. A. Anew approach for interpreting Random Forest models and its application to thebiology of ageing. _Bioinformatics_ **34** , 2449äóñ2456 (2018).45. 45.Basu, S., Kumbier, K., Brown, J. B. & Yu, B. Iterative random forests todiscover predictive and stable high-order interactions. _Proc. Natl. Acad.Sci. U.S.A._ **115** , 1943äóñ1948 (2018).46. 46.Yu, M. K. et al. Visible machine learning for biomedicine. _Cell_ **173** ,1562äóñ1565 (2018).47. 47.Yauney, G. & Shah, P. Reinforcement learning with action-derived rewards forchemotherapy and clinical trial dosing regimen selection. _Proc. Mach. Learn.Res._ **85** , 161äóñ226 (2018).48. 48.Ali, I. et al. Lung nodule detection via deep reinforcement learning. _Front.Oncol._ **8** , 108 (2018).49. 49.Padmanabhan, R., Meskin, N. & Haddad, W. M. Reinforcement learning-basedcontrol of drug dosing for cancer chemotherapy treatment. _Math. Biosci._**293** , 11äóñ20 (2017).50. 50.Tseng, H. H. et al. Deep reinforcement learning for automated radiationadaptation in lung cancer. _Med. Phys._ **44** , 6690äóñ6705 (2017).51. 51.Mahmud, M., Kaiser, M. S., Hussain, A. & Vassanelli, S. Applications of deeplearning and reinforcement learning to biological data. _IEEE Trans. NeuralNetw. Learn. Syst._ **29** , 2063äóñ2079 (2018).52. 52.Girardi, D. et al. Interactive knowledge discovery with the doctor-in-the-loop: a practical example of cerebral aneurysms research. _Brain Inform._**3** , 133äóñ143 (2016).53. 53.Pearl, J. _Causality: Models, Reasoning and Inference_ (Cambridge UniversityPress, Cambridge, England, 2000).54. 54.Yoon, J., Jordon, J. & Van der Schaar, M. GANITE: estimation of individualizedtreatment effects using generative adversarial nets. In _InternationalConference on Learning Representations._<https://openreview.net/forum?id=ByKWUeWA> (2018).55. 55.Alaa, A. M. & Van der Schaar, M. AutoPrognosis: automated clinical prognosticmodeling via Bayesian optimization with structured Kernel learning. Preprintat <https://arxiv.org/abs/1802.07207> (2018).",Artificial Intelligence,0.889,Artificial intelligence for precision oncology: beyond patient stratification,standard24,https://news.ycombinator.com/item?id=19704661,conversations_hackernewsnew,15.684645,[],AI,"# Must-read books to learn Artificial Intelligence in 2019We have reviewed the top 5 best Artificial Intelligence books available on theInternet. And to be honest, these books were _really_ hard to find. Betweenthe äóìA.I conspiracy booksäóù and the äóìhow to make money off A.I booksäóù, therewas really wasnäó»t much left to choose from. These resources are weighted basedoff trusted community reviews and the quality of the content itself. Becausewhy waste your time on bad content? You wonäó»t ever truly understand the fieldof Artificial Intelligence, nor will you be able to even apply it very well.These books will cover topics like Neural Networks, MathematicalOptimizations, Logic, Probability, and Economics äóî which are all _extremely_useful in todayäó»s modern world.### 1\. Artificial Intelligence: A Modern Approach[Artificial Intelligence: A ModernApproach](https://www.amazon.com/gp/product/9332543518/ref=as_li_tl?ie=UTF8&tag=zeroequalsfal-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=9332543518&linkId=d01c6c70480b5bb97712ea8d4f488ded)provides AI algorithm techniques in-detail, from pathfinding to intelligent AIAgent design. If you are looking for one of the best books on A.I, then thisis surely a top pick. There is detailed information on building Agents, graphalgorithms incl. A* Search, and how to navigate in areas of uncertainty. Greatbook with lots of content and examples.### 2\. Deep Learning[DeepLearning](https://www.amazon.com/gp/product/0262035618/ref=as_li_qf_asin_il_tl?ie=UTF8&tag=zeroequalsfal-20&creative=9325&linkCode=as2&creativeASIN=0262035618&linkId=84568d05237b805c578061aed460c18f)is written by a famous ex-Googler, providing a rich and detailed guide intoone of A.Iäó»s most exciting sub-fields, äóìMachine Learningäóù. In this book youwill learn about Neural Networks and how to construct them for various use-cases. Itäó»s been backed by our industry thought-leaders such as Elon Musk whohas commented on how comphresive this book truly is.### 3\. Pattern Recognition and Machine Learning (Information Science andStatistics)[Pattern Recognition and Machine Learning (Information Science andStatistics)](https://www.amazon.com/gp/product/0387310738/ref=as_li_qf_asin_il_tl?ie=UTF8&tag=zeroequalsfal-20&creative=9325&linkCode=as2&creativeASIN=0387310738&linkId=af136812be38f404cda557c135deaabc)is a speciality book on the field of pattern recognition. This is a no bs*book that covers scientific topics such as Bayesian methods to build A.Iagents. It is a truly an outstanding book for itäó»s time, and first publishedback in 2006.### 4\. Deep Learning with Python[Deep Learning withPython](https://www.amazon.com/gp/product/1617294438/ref=as_li_qf_asin_il_tl?ie=UTF8&tag=zeroequalsfal-20&creative=9325&linkCode=as2&creativeASIN=1617294438&linkId=e875897693499f3ed3137e2cb2c10493)combines Deep Learning techniques together with the Python programminglanguage. Python is generally the preferred language for building AI models äóîas it is highly recognised by many large companies and it supports someexceptional A.I libraries such as Tensorflow to construct A.I agents. Thisbook will get you up to speed with building A.I using Deep Learning. Priorknowledge of Python may be advised.### 5\. The Elements of Statistical Learning: Data Mining, Inference, andPrediction, Second Edition (Springer Series in Statistics)[The Elements of Statistical Learning: Data Mining, Inference, and Prediction,Second Edition (Springer Series inStatistics)](https://www.amazon.com/gp/product/0387848576/ref=as_li_qf_asin_il_tl?ie=UTF8&tag=zeroequalsfal-20&creative=9325&linkCode=as2&creativeASIN=0387848576&linkId=514d719391cc3e1ddb4c8e1ebefeb8c0)might be one of the best books to gain a solid foundation of statistics, whichreally is the backbone of many A.I based applications. Stats helps to drivethe decision-making process of AI such that smart decisions are made. Thisbook is comphresive and covers Data Mining, Inference, and Prediction äóî allrelevant and highly applicable today.Thanks for reading!",Artificial Intelligence,1,Best Books to Learn Artificial Intelligence in 2019,standard59,github-issue-415950367,conversations_github_issues,1,[],,å_‚ä»_kotlinå’Œjetpackç›¸ç»§é—®ä¸–ï_Œæ˜¯ä¸çæ˜¯è¯¥æ›´æ–°ä¸€æ_¢Androidæ_€æœ¯äº†_Ÿ˜Å,DevOps:JAMstack,éƒ_å_§å©¶æ˜¯ä¸çæ˜¯åœ¨å‡†å_‡æ–°ä__äº†,standard,60,github-issue-415950760,conversations_github_issues,1,[],DevOps,"This fails to generate the correct url:```rst.. code-include:: {filename}/src/my_file.py```This generates `contents/pages/{filename}/src/my_file.py`.",DevOps,code_include does not work with the {filename} specification,standard,61,github-issue-415950621,conversations_github_issues,1,[],Containers,This will parse `Dockerfile` and run `docker create` with all the ports and volumes.,Containers,Add the create command,standard,62,github-issue-415955322,conversations_github_issues,1,[],,"```when(ci) {0 -> Green0..5 -> Amber> 5 -> Red}```",DevOps,Update CI indicator's criteria,standard,63,github-issue-415959013,conversations_github_issues,1,[],,,,åê_ç«¯æ_¥åè£æœçå_¡ä¸çè¡Œå•_,standard,64,github-issue-415960205,conversations_github_issues,1,[],Customer experience,"When Xdata is more, is PNChart can scroll?",DevOps:Analytics,PNChart can scroll,standard,65,github-issue-415963003,conversations_github_issues,1,"['html-parser', 'html-parser-library', 'typescript', 'dom', 'javascript']",,https://github.com/inikulin/parse5,DevOps:JAMstack:Frontend tools,Tests from parse5,standard,66,github-issue-415969575,conversations_github_issues,1,[],"DevOps, Networking","According to https://github.com/gin-contrib/cors/blob/master/cors.go#L151, `OPTIONS` aren't allowed by default. Should it be allowed?When performing certain types of cross-domain Ajax requests, modern browsers that support CORS will insert an extra [preflight request](https://developer.mozilla.org/en-US/docs/Glossary/Preflight_request) to determine whether they have permission to perform the action. The preflight request is using the `OPTIONS` method.",DevOps:Frontend tools:Networking:JAMstack,Add OPTIONS to allowed methods in default config,standard,67,github-issue-415968808,conversations_github_issues,1,[],,There must be a new contact created on HubSpot.,DevOps,From Hubspot,standard,68,github-issue-415970470,conversations_github_issues,1,[],DevOps,"componentWillReceiveProps ä»¥ä¸‹ç®€ç§°receivePropsæœ€è¿‘åœ¨å·¥ä_œä¸_éÅ‡åˆ°äº†ä¸€è¡Œä»£ç Åä¸_ç_„é—®é¢˜ï_Œåœ¨receivePropsæ–_æ_•å†…éƒ¨è°ƒç”¨this.aç_„æ—¶å€™å‡ºç_°æ•°æç®ä¸¢å_±é—®é¢˜ï_Œå…·ä_“ä»£ç Åå_‚ä¸‹ï__```jsconstructor(props) {super(props);this.key = null;} componentWillReceiveProps(nextProps) {if (nextProps.status !== this.props.status) {this.doSomething(nextProps.status);}}eventListener(key) { // æŸêä¸ªäº‹ä»¶è§_åè‘this.key =key;this.props.updateStatus(2); // è°ƒç”¨reduxæ–_æ_•ï_Œä¿®æ”_propså†…å®_};```eventListerè§_åè‘--ã€‹receiveProps--ã€‹åœ¨doSomethingæ–_æ_•é‡Œè°ƒç”¨ this.keyã€‚**å¥‡æ€ªç_„æ˜¯ï_Œç¬¬ä¸€æ¬¡è°ƒç”¨doSomethingåè¯ä»¥è®¿é—®åˆ°this.key ç¬¬äºŒæ¬¡è°ƒç”¨åç´ä__è¿”å›_null;**okä¸ºäº†è§£å†_é—®é¢˜ï_Œæˆ‘ä»¬å…ˆçœ‹ä¸‹å®˜æ–_æ–‡æ¡£ä¸_å¯_receivePropsæ–_æ_•ç_„è§£é‡_```jsUNSAFE_componentWillReceiveProps()UNSAFE_componentWillReceiveProps(nextProps)NoteThis lifecycle was previously named componentWillReceiveProps. That name will continue to work until version 17. Use the rename-unsafe-lifecycles codemod to automatically update your components.Note:Using this lifecycle method often leads to bugs and inconsistenciesIf you need to perform a side effect (for example, data fetching or an animation) in response to a change in props, use componentDidUpdate lifecycle instead.If you used componentWillReceiveProps for re-computing some data only when a prop changes, use a memoization helper instead.If you used componentWillReceiveProps to â€œresetâ€ù some state when a prop changes, consider either making a component fully controlled or fully uncontrolled with a key instead.For other use cases, follow the recommendations in this blog post about derived state.UNSAFE_componentWillReceiveProps() is invoked before a mounted component receives new props. If you need to update the state in response to prop changes (for example, to reset it), you may compare this.props and nextProps and perform state transitions using this.setState() in this method.Note that if a parent component causes your component to re-render, this method will be called even if props have not changed. Make sure to compare the current and next values if you only want to handle changes.React doesnâ€™t call UNSAFE_componentWillReceiveProps() with initial props during mounting. It only calls this method if some of componentâ€™s props may update. Calling this.setState() generally doesnâ€™t trigger UNSAFE_componentWillReceiveProps().Googleç¿»è¯‘ä¸€ä¸‹ã€‚ã€‚æ_¨æ„èï__ä_¿ç”¨æ__ç”Ÿå‘_å‘¨æœŸæ–_æ_•é€_å¸¸ä__å¯_è‡´é”™è¯¯å’Œä¸çä¸€è‡´å_‚æ_œæ‚¨éœ€è_Åæ‰§è¡Œå‰¯ä_œç”¨ï_ˆä_‹å_‚ï_Œæ•°æç®æèêåè–æˆ–å_¨ç”»ï_‰ä»¥å“çåº”propsä¸_ç_„æ›´æ”_ï_Œè¯·æ”_ç”¨componentDidUpdateç”Ÿå‘_å‘¨æœŸã€‚å_‚æ_œæ‚¨ä»…åœ¨propæ›´æ”_æ—¶ä_¿ç”¨componentWillReceivePropsé‡çæ–°è®¡ç®—æŸêäº›æ•°æç®ï_Œè¯·ä_¿ç”¨memoization helperã€‚å_‚æ_œæ‚¨åœ¨propæ›´æ”_æ—¶ä_¿ç”¨componentWillReceivePropsâ€œé‡çç_®â€ùæŸêäº›ç_¶æ€Åï_Œè¯·è€ƒè™‘ä_¿ç”¨ç»„ä»¶å®Œå…¨æ_§åˆ¶ç»„ä»¶æˆ–å®Œå…¨ä¸çåè—æ_§åˆ¶ã€‚å¯_äº_å…¶ä»–ç”¨ä_‹ï_Œè¯·éÅµå_ªæ__åç_å®¢æ–‡ç« ä¸_æœ‰å…_æ´_ç”Ÿç_¶æ€Åç_„å»ºè®®ã€‚åœ¨å®‰è£…ç_„ç»„ä»¶æ_¥æ”¶æ–°ç_„propsä_‹å‰çè°ƒç”¨UNSAFE_componentWillReceivePropsï_ˆï_‰ã€‚å_‚æ_œæ‚¨éœ€è_Åæ›´æ–°ç_¶æ€Åä»¥å“çåº”propæ›´æ”_ï_ˆä_‹å_‚ï_Œé‡çç_®å®ƒï_‰ï_Œæ‚¨åè¯ä»¥æ¯”è_ƒthis.propså’ŒnextPropså_¶ä_¿ç”¨æ__æ–_æ_•ä¸_ç_„this.setStateï_ˆï_‰æ‰§è¡Œç_¶æ€Åè_¬æç¢ã€‚è¯·æ_¨æ„èï_Œå_‚æ_œçˆ¶ç»„ä»¶å¯_è‡´ç»„ä»¶é‡çæ–°æ¸_æŸ“ï_Œåç_ä_¿propsæ_¡æœ‰æ›´æ”_ï_Œä_Ÿä__è°ƒç”¨æ__æ–_æ_•ã€‚å_‚æ_œæ‚¨åèªæƒ_å_„çê†æ›´æ”_ï_Œè¯·å_¡å¿…æ¯”è_ƒå_“å‰çå€_å’Œä¸‹ä¸€ä¸ªå€_ã€‚åœ¨å®‰è£…è¿‡ç¨‹ä¸_ï_ŒReactä¸çä__ä_¿ç”¨åˆùå§‹éÅ“å…·è°ƒç”¨UNSAFE_componentWillReceivePropsï_ˆï_‰ã€‚å_‚æ_œæŸêäº›ç»„ä»¶ç_„éÅ“å…·åè¯èƒ_ä__æ›´æ–°ï_Œå®ƒåèªä__è°ƒç”¨æ__æ–_æ_•ã€‚è°ƒç”¨this.setStateï_ˆï_‰é€_å¸¸ä¸çä__è§_åè‘UNSAFE_componentWillReceivePropsï_ˆï_‰ã€‚```æ–‡æ¡£ä¸_è¯´ï_Œä_¿ç”¨æ__ç”Ÿå‘_å‘¨æœŸæ–_æ_•é€_å¸¸ä__å¯_è‡´é”™è¯¯å’Œä¸çä¸€è‡´ï_Œé‚£åˆ°åº•æ˜¯ä»€ä_ˆå_Ÿå› å¯_è‡´ç_„é”™è¯¯å’Œä¸çä¸€è‡´å‘¢ï_Œæˆ‘ä»¬æù¥çœ‹ä¸‹receivePropsæ˜¯æ€_ä_ˆå®_ç_°ç_„```js```",DevOps:Frontend tools,çê†è§£Reactç”Ÿå‘_å‘¨æœŸcomponentWillReceiveProps,standard,69,github-issue-415970379,conversations_github_issues,1,"['enhancement', 'cmake', 'python', 'c-plus-plus']",Customer Experience,"The idea is to make a ""git pull"" if possible, maybe once a day",DevOps:Customer Experience (UI + UX),rb.py could check if it is not outdated,standard,70,github-issue-415971754,conversations_github_issues,1,"['data-visualization', 'd3', 'webgl', 'plotly', 'charting-library', 'charts', 'visualization', 'plotly-dash', 'regl']","DevOps, analytics","For plotly scatter mal plots:I'd wish the package also offered the possibility to use a self defined style.json like when you use Mapbox natively in JS. Using this specification (and using OSM tiles) makes the mapbox_access_token obsolete.https://github.com/plotly/plotly.py/issues/1431",DevOps:Analytics,[Feature Request] use own style.json instead of mapbox access token,standard,71,github-issue-415973026,conversations_github_issues,1,"['swift', 'music', 'youtube', 'wrapper', 'mac', 'macos', 'osx']",Customer Experience,@steve228uk love the app! Have a request: would it be possible to reduce the minimal window width and height? YT music has a good mobile layout and it would make it a little less large on the desktop. I tried doing it myself by can't get the cocoapods dependencies to resolve properly...,DevOps:Customer Experience (UI + UX),Mobile size window,standard,72,github-issue-415976226,conversations_github_issues,1,[],Customer Experience,"The mailchimp widget included in this theme is:- dependent upon INN's mailchimp tools library- dependent upon an external plugin- therefore, out of dateAnd it should be replaced with a different solution when possible.",DevOps,For later: Replace their mailchimp widget with plain Mailchimp embedded form,standard,73,github-issue-415975860,conversations_github_issues,1,[],,"- [ ] æ”¯æŒÅè_· Keyã€‚- [ ] æ”¯æŒÅåˆ†äº«åˆ†æ__ Shareã€‚- [ ] æ”¯æŒÅè¨‚è__ Bookã€‚",DevOps,Steam Key,standard,74,github-issue-415986276,conversations_github_issues,1,"['design', 'linux', 'ux', 'ui', 'c-plus-plus', 'vala', 'gtk', 'cmake']",Customer Experience,"I would love to see variable fonts support. It's in freetype and harfbuzz, and all major browsers.",DevOps:Customer Experience (UI + UX),Variable Fonts,standard,75,github-issue-415986154,conversations_github_issues,1,[],,,,åè¯ä»¥åè¯ä»¥,standard,76,github-issue-416001826,conversations_github_issues,1,['bug'],DevOps,dialog model has to be reload from actual value in LineEdit,DevOps,Multi-value Properties - Dialog values change after cancel,standard,77,github-issue-415023561,conversations_github_issues,1,"['docs', 'good first issue']",,https://whitesmith.github.io/qnorr-styles/#objects-css-[data-flitem*=%22justify%22],DevOps:JAMstack:Frontend tools,"[data-flitem*=""justify""]Â annotations should go into base selector [data-flitem]",standard,78,github-issue-415022369,conversations_github_issues,1,"['c: functionality', 'e: all editions', 'p: medium', 't: defect']",,NullPointerException on Routine.toString() when routine is not attached to a Configuration - Merge [#8355],standard,,,79,github-issue-415025858,conversations_github_issues,1,[],"DevOps, Cloud","Greetings, I figured I would try to implement this on this [aws-example](https://github.com/aws-samples/aws-amplify-graphql). After installing and running it successfully, I followed the steps in this repo.- Unauthenticated role which is accepted:```json{""Version"": ""2012-10-17"",""Statement"": [{""Effect"": ""Allow"",""Action"": [""appsync:GraphQL""],""Resource"": [""arn:aws:appsync:eu-west-1:073051392232:apis/gucbk3owp5h4dihdqd2qdiqbwa/types/Query/fields/listPictures""]}]}```""Authenticated role"" throws following error```An error occurredYour request has a problem. Please see the following details.The policy failed legacy parsing ```When trying this json:```json{""Version"": ""2012-10-17"",""Statement"": [{""Effect"": ""Allow"",""Action"": [""appsync:GraphQL""],""Resource"": [""arn:aws:appsync:eu-west-1:073051392232:apis/gucbk3owp5h4dihdqd2qdiqbwa/types/Query/fields/listPictures"",""arn:aws:appsync:eu-west-1:073051392232:apis/gucbk3owp5h4dihdqd2qdiqbwa/types/Mutation/fields/createPicture""]}]}```Searched around on stackoverflow and web but cant find what the proper format should be to fix the parsing error.```json{""Version"": ""2012-10-17"",""Statement"": [{""Effect"": ""Allow"",""Action"": [""appsync:GraphQL""],""Resource"": [""arn:aws:appsync:eu-west-1:073051392232:apis/gucbk3owp5h4dihdqd2qdiqbwa/types/Query/fields/listPictures"",""arn:aws:appsync:eu-west-1:073051392232:apis/gucbk3owp5h4dihdqd2qdiqbwa/types/Mutation/fields/createPicture""]}]}```Any guidance would be greatly appreciated. Thank you.",DevOps:Databases:Cloud,Getting ` The policy failed legacy parsing ` error.,standard,80,github-issue-415023663,conversations_github_issues,1,[],,There must be a new contact created on HubSpot.,DevOps,From Hubspot,standard,81,github-issue-415026890,conversations_github_issues,1,['triage'],DevOps,"# Bug reportIf you've installed MenuCRUD as a package using our docs, a ```composer update``` , ```composer require``` or ```composer self-update``` might trigger the following warning:```Deprecation warning: require.backpack/MenuCRUD is invalid, it should not contain uppercase characters. Please use backpack/menucrud instead. Make sure you fix this as Composer 2.0 will error.```",DevOps,Composer install / update / self-update warning,standard,82,github-issue-415033949,conversations_github_issues,1,"['bug', 'wordpress', 'wordpress-plugin', 'amp']","DevOps, Mobile services","This is something I noticed personally on a site while working on AMP integration, and was just reported at https://wordpress.org/support/topic/php-fatal-error-enabling-paired-mode/ as well.Basically, it's possible to call `is_amp_endpoint()` after `parse_query` but without there being a global `$wp_query` (e.g. because of some plugin `doing_it_wrong`).This will lead to a fatal error in this part of the code:https://github.com/ampproject/amp-wp/blob/286aef5d61e838d4b4b5797acd583a5e0e947f90/includes/amp-helper-functions.php#L273-L277Where `get_query_var()` will fail to access `$wp_query->get()`.I feel like we can just return `false` in when `$wp_query` is not set, perhaps even with an additional `_doing_it_wrong()` to say that there's no global query.",DevOps:Frontend tools:JAMstack,Check for WP_Query global in is_amp_endpoint(),standard,83,github-issue-415030167,conversations_github_issues,1,[],Customer Experience,"I don't see any way around having an editable treeview for the entire NeXus structure, but other suggestions are welcome.A tab setup to switch between the basic component list and a full treeview seems like a nice idea.This ticket needs to be discussed and planned in more detail before starting any coding!",DevOps:Customer Experience (UI + UX),Detailed editing of entire NeXus Structure,standard,84,github-issue-415024444,conversations_github_issues,1,"['sylius', 'symfony', 'php', 'cms', 'ecommerce', 'ckeditor', 'sylius-plugin']",DevOps,"Hi,I stumble upon this error when searching for products in the Admin.Pages grid.""message"": ""Too few arguments to function Doctrine\\ORM\\EntityRepository::__construct(), 0 passed in /sylius/var/cache/dev/ContainerIcERujV/getBitbagSyliusCmsPlugin_Controller_Action_Admin_ProductSearchService.php on line 15 and exactly 2 expected""Any ideas? Thanks!",DevOps,Edit page: Product autocomplete causes error,standard,85,github-issue-415024699,conversations_github_issues,1,"['/', 'gitalk']",,https://smiletolove.github.io/,DevOps:JAMstack:Frontend tools,Xiao Cheng,standard,86,github-issue-415024604,conversations_github_issues,1,[],,,,fix: fzf preview toggle fullscreen and tmux toggle terminal,standard,87,github-issue-415030871,conversations_github_issues,1,"['follow', 'github', 'organization', 'subscription', 'notification']",,"## Paste the link of the GitHub organisation below and submithttps://github.com/socialblade---###### Please subscribe to this thread to get notified when a new repository is created",DevOps,Subscribe to Social Blade LLC,standard,88,github-issue-415039152,conversations_github_issues,1,[],Customer Experience,Add options for aligning text,DevOps:Customer Experience (UI + UX),Add alignment options,standard,89,github-issue-415036655,conversations_github_issues,1,['bug'],Customer Experience,"When the Publish button is disabled because there are no items to publish or user lacks publishing permissions, the disabled Publish button should be replaced with ""Create Issue"" (blue background, no dropdown menu). Note also that two different ways are used to disable the publish button on the screenshots below. ![image](https://user-images.githubusercontent.com/11612490/53478748-d4f73200-3a77-11e9-9f14-fd62e5241bbd.png)![image](https://user-images.githubusercontent.com/11612490/53481429-82b90f80-3a7d-11e9-9725-bc21d5a27675.png)**NB! Exception is the case when publishing is prevented by invalid items in the list - in this case Publish button should be visible but disabled (like now).**",DevOps:Frontend tools:Customer Experience (UI + UX),"""Create Issue"" button should replace ""Publish"" when publishing is not allowed",standard,90,github-issue-415035979,conversations_github_issues,1,[],,,,Improve error messages,standard,91,github-issue-415038493,conversations_github_issues,1,"['machine-learning', 'python', 'statistics', 'data-science', 'data-analysis']","DevOps, Machine Learning","#### Description<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->#### Steps/Code to Reproduce```In [1]: import imblearn In [2]: from imblearn.pipeline import make_pipeline, Pipeline In [3]: from imblearn.under_sampling import RandomUnderSampler In [4]: rus = RandomUnderSampler() In [5]: p = make_pipeline(rus, None) In [6]: from sklearn.datasets import make_blobs In [7]: X, y = make_blobs() In [8]: p.fit_resample(X, y) ---------------------------------------------------------------------------AttributeError Traceback (most recent call last)<ipython-input-8-cfa4a4b54c83> in <module>----> 1 p.fit_resample(X, y)~/repos/scikit-learn/sklearn/utils/metaestimators.py in __get__(self, obj, type)108 continue109 else:--> 110 getattr(delegate, self.attribute_name)111 break112 else:AttributeError: 'str' object has no attribute 'fit_resample'```#### Expected ResultsNo error#### Actual ResultsError#### Versions<!--Please run the following snippet and paste the output below.import platform; print(platform.platform())import sys; print(""Python"", sys.version)import numpy; print(""NumPy"", numpy.__version__)import scipy; print(""SciPy"", scipy.__version__)import sklearn; print(""Scikit-Learn"", sklearn.__version__)import imblearn; print(""Imbalanced-Learn"", imblearn.__version__)-->```Linux-4.19.0-2-amd64-x86_64-with-debian-buster-sidPython 3.7.1 (default, Dec 14 2018, 19:28:38) [GCC 7.3.0]NumPy 1.15.4SciPy 1.1.0Scikit-Learn 0.21.dev0Imbalanced-Learn 0.4.3```<!-- Thanks for contributing! -->",DevOps,Error when last stage is None and fit_resample is called,standard,92,github-issue-415038535,conversations_github_issues,1,[],,There must be a new contact created on HubSpot.,DevOps,From Hubspot,standard,93,github-issue-415038488,conversations_github_issues,1,[],DevOps,"All events end up in bad when we enable the application context (`builder?.setApplicationContext(true)`). When we decode the line, we find that the context is sent like this:```{""schema"": ""iglu:com.snowplowanalytics.mobile\/application\/jsonschema\/1-0-0"",""data"": {""118"": ""build"",""2.6.1"": ""version""}}```As far as we know, there is nothing in our specific implementation that causes this to happenâ€“suggesting it's a bug in the SDK. If it is indeed, it's severe as it will cause all events to fail validation.We suspect it's related to these 2 lines in the SDK: https://github.com/snowplow/snowplow-objc-tracker/blob/master/Snowplow/SPTracker.m#L212-L213",DevOps,Events fail validation when the application context is added,standard,94,github-issue-415041731,conversations_github_issues,1,"[':canvas', ':design', 'discuss', 'kibana', 'elasticsearch', 'visualizations', 'metrics']",Customer Experience,"Now it's easy to accidentally hit the Delete or Duplicate hotspots in the Canvas page sorter, in part because it's part of the mini slide (clicking on the entire box is designed mostly to switch to that page) and in part because the hotspots are active before they show up, and the user may have initiated the clicking before the appearance of the hotspots would have a chance of canceling the click action (users may frustratingly witness the click + hotspot collision as ""their own failing"")![deletepage](https://user-images.githubusercontent.com/1548516/53482353-4f778000-3a7f-11e9-88f9-87c106d31279.gif)Not sure what the ideal solution would be, though disabling the fade-in transition would help (controls to show up immediately), and it would not be a deep change.",DevOps:Frontend tools:Customer Experience (UI + UX),[Canvas][Minor] Immediately display `Delete` and `Duplicate` hotspots,standard,95,github-issue-415040791,conversations_github_issues,1,"['enhancement', 'svn', 'subversion', 'searching', 'jstree']",,Add username/password parameters to svn config,standard,,,96,github-issue-415052487,conversations_github_issues,1,"['ogc', 'ogc-sos', 'sensorobservationservice', 'ogc-swe', 'sensorweb', 'rstats', 'r']",DevOps,Running the tests depends on #78,DevOps,Write testthast tests for the convience functions,standard,97,github-issue-415056232,conversations_github_issues,1,[],Customer Experience,"it should possible to show more infos on the delivery email like:- customer email and other infos- deliveryContext stuff",DevOps:Customer Experience (UI + UX),add more infos to the delivery email,standard,98,github-issue-415055871,conversations_github_issues,1,[],,There must be a new contact created on HubSpot.,DevOps,From Hubspot,standard,99,github-issue-415055373,conversations_github_issues,1,[],DevOps,"Project description on EL Showcase states that:> 100% unit-testable code _Ÿ”¥There is a shared build scheme that should run tests, but I didn't find any tests. Moreover, the unit tests target referred by the scheme is missing _Ÿ‘€",DevOps,Missing tests,standard,100,github-issue-415054214,conversations_github_issues,1,"['0a66a50353bba700c40341a6efae161b', 'gitalk']",Frontend tools,"http://blog.yangyong.io/2019/02/27/html/textarea/ JavaScriptæ˜¯ä¸–ç•Œä¸_ç¬¬äºŒå¥_ç_„è¯_è¨€_Ÿ˜è",DevOps:Frontend tools,textarea | Angus,standard,101,github-issue-415053086,conversations_github_issues,1,"['machine-learning', 'generative-adversarial-network', 'music-generation', 'multi-track', 'piano-roll']",Machine Learning,"In the museGAN paper, the negative critic loss is said to be converging around 10 to the power of 8.I am training the model on my own, and my generator loss is oscillating around zero, an e.g. history is```console1 | 104/ 180 | 67.79 | 60.750362 | -16.1817931 | 105/ 180 | 67.73 | 62.146736 | 40.1340071 | 106/ 180 | 67.83 | 74.526474 | 33.8560941 | 107/ 180 | 67.66 | 59.294342 | -21.4019321 | 108/ 180 | 67.92 | 115.995209 | 94.3445431 | 109/ 180 | 68.00 | 156.438293 | 224.5999451 | 110/ 180 | 68.32 | 54.108414 | -35.459709```sometimes it is positive, and sometimes negative. And also discriminator loss (negative) is coming around 50-100.So my question is?1. Where should ideally generator and discriminator losses converge?2. I am sensing, I should increase the parameter of the generator, is it a right approach?Thank you in advance. :)",DevOps,discriminator and generator loss convergence,standard,102,github-issue-415057486,conversations_github_issues,1,['bug'],Customer Experience,The menu on IE is not displayed very good.,DevOps:Customer Experience (UI + UX),menu on IE,standard,103,github-issue-415061427,conversations_github_issues,1,['bug'],,![screen shot 2019-02-27 at 10 48 33](https://user-images.githubusercontent.com/52113/53485145-5dc49c80-3a7d-11e9-8687-17715ba01ac1.png),DevOps,Bug in the descriptions in the main boxes.,standard,104,github-issue-415066178,conversations_github_issues,1,[],,,,Je travaille sur library/subprocess.po,standard,105,github-issue-415064743,conversations_github_issues,1,[],,,,Je travaille sur library/string.po,standard,106,github-issue-415063946,conversations_github_issues,1,[],,"<blockquote class=""twitter-tweet""><p lang=""ro"" dir=""ltr"" xml:lang=""ro"">KUB-UAV : le nouveau drone de Kalachnikov - IT Social <a href=""https://t.co/fobO4JldFV"">https://t.co/fobO4JldFV</a> <a href=""https://twitter.com/hashtag/actu?src=hash&amp;ref_src=twsrc%5Etfw"">#actu</a> <a href=""https://twitter.com/hashtag/drone?src=hash&amp;ref_src=twsrc%5Etfw"">#drone</a></p>&mdash; Les drones (@les_drones) <a href=""https://twitter.com/les_drones/status/1100710454643617794?ref_src=twsrc%5Etfw"">February 27, 2019</a></blockquote><br><br>February 27, 2019 at 11:53AM<br>",DevOps,KUB-UAV : le nouveau drone de Kalachnikov - IT Social https://t.co/fobO4JldFV #actu #drone,standard,107,github-issue-415060780,conversations_github_issues,1,[],,,,Je travaille sur library/queue.po,standard,108,github-issue-415061807,conversations_github_issues,1,[],,,,Port https://go-review.googlesource.com/c/go/+/163861,standard,109,github-issue-415065644,conversations_github_issues,1,[],,,,PLease add redmi note 7 support,standard,110,github-issue-415060300,conversations_github_issues,1,"['language', 'wiki', 'notes']",,"## åê_ç«¯æ_¶æ_„- [é˜¿é‡Œã€_åê_ç«¯æ_¶æ_„å¸ˆæ_€æœ¯å›_è°±ã€‹](https://github.com/xingshaocheng/architect-awesome)",DevOps:Frontend tools:JAMstack,æ_€æœ¯æˆêé•¿è·¯çº¿,standard,111,github-issue-415066640,conversations_github_issues,1,"['node', 'express', 'particula', 'framework']",DevOps,We need an easy way to test particula-based apps in granular way (i.e. testing routes separately since testing the whole app is already possible),DevOps,Tools for testing Particula-based apps,standard,112,github-issue-415083573,conversations_github_issues,1,[],Frontend tools,"Move **Morphy Tsai** from Community to Design category. New title ""Designer""",DevOps,Contributors: Move Morphy Tsai to Design,standard,113,github-issue-415083282,conversations_github_issues,1,"['wordpress-plugin', 'feature-flags', 'feature-toggle', 'wordpress-deployment', 'wordpress-theme-development', 'wordpress']",DevOps,Are you able to tag a new stable release? Or a `0.x` if not willing to commit to `1.x`+?,DevOps,Tag release,standard,114,github-issue-415084510,conversations_github_issues,1,['new'],,See [here](https://github.com/Luolc/AdaBound).,DevOps:JAMstack,AdaBound optimizer,standard,115,github-issue-415084471,conversations_github_issues,1,[],,"- é‡‡é›†æ˜_åç¡- Airserver ä_¿ç”¨æ_•å±èè_¯ä»¶ï_Œç„¶åê_å†ççª—åè£æç•è_·åç_åè¯",DevOps,OBSå_‚ä_•æ_¨æµÅåè°ç”µè„‘ç_„å±èå_•,standard,116,github-issue-415103804,conversations_github_issues,1,"['community 1.10.4', 'bug', 'production']",DevOps,"## Error in Faveo Community**Symfony\Component\Debug\Exception\FatalThrowableError** in **GET /gcdhelpdesk/public/ticket/tooltip**Call to a member function purify() on null[View on Bugsnag](https://app.bugsnag.com/ladybird-web-solution-pvt-ltd/faveo-community/errors/5c768484f0d68100194589ed?event_id=5c76848400358f7be3780000&i=gh&m=ci)## Stacktrace\app\Http\Controllers\Agent\helpdesk\TicketController.php:2403 - App\Http\Controllers\Agent\helpdesk\TicketController::getTooltip\app\Http\Middleware\Authenticate.php:54 - App\Http\Middleware\Authenticate::handle\app\Http\Middleware\CheckUpdate.php:35 - App\Http\Middleware\CheckUpdate::handle\app\Http\Middleware\LanguageMiddleware.php:33 - App\Http\Middleware\LanguageMiddleware::handle\app\Http\Middleware\VerifyCsrfToken.php:33 - App\Http\Middleware\VerifyCsrfToken::handle[View full stacktrace](https://app.bugsnag.com/ladybird-web-solution-pvt-ltd/faveo-community/errors/5c768484f0d68100194589ed?event_id=5c76848400358f7be3780000&i=gh&m=ci)*Created automatically via Bugsnag*",DevOps,Symfony\Component\Debug\Exception\FatalThrowableError in GET /gcdhelpdesk/public/ticket/tooltip,standard,117,github-issue-415107880,conversations_github_issues,1,[],,"https://zrbabbler.hatenablog.com/entry/2019/02/27/203738 ãÅ®ã€Œupnmlminr-hã‚’å_‰æè›ãÅ—ã‚ˆãÅ†ãÅ¨ãÅ™ã‚‹ãÅ¨ã‚¢ãƒ¬ã€çã‚’ãÅŸãÅ_ãÅŸãÅ_èª_ã‚“ãÅ ãÅ®ãÅ§ãÅ™ãÅŒï_Œ> japanese-otfãƒ‘ãƒƒã‚±ãƒ_ã‚¸ãÅ®upTeXç”¨ãÅ®ãƒ•ã‚©ãƒ_ãƒˆãÅ®ä¸€ãÅ_ãÅ§ãÅ‚ã‚‹upnmlminr-hãÅ®ä¸_èº«ã‚’èª¿ãÅ_ãÅŸãÅ„ãÅ¨æ€ùãÅ£ãÅ_ã€Åvf2zvpã‚’ä_¿ãÅ_ãÅ†ãÅ¨ãÅ—ãÅ_ã‚‚ã€Åã‚¨ãƒ©ãƒ_ãÅŒå‡ºãÅ_å_‰æè›ãÅ¯å_±æ•—ãÅ—ãÅ_ãÅ—ãÅ_ãÅ†ã€‚ãÅ¨ãÅ‚ã‚_ãÅ_ãÅ™ã€‚ç¢ºãÅ‹ãÅ«ï_Œ```$ jfmutil vf2zvp --uptool upnmlminr-hjfmutil: CHARWD value mismatch: code 00B7```.tfmãÅ¨.vfãÅ«è¨˜éŒ_ãÅ•ã‚ŒãÅŸå€_ãÅŒé£ŸãÅ„éÅ•ãÅ£ãÅ_ãÅ„ã‚‹ãÅ®ãÅ¯å__å¯†ãÅ«ãÅ¯æ_£ãÅ—ãÅèãÅªãÅ„ã‚ˆãÅ†ãÅ§ãÅ™ã€‚ä¸€å¿œ> å®Ÿé_›ãÅ®éÅ‹ç”¨ãÅ¨ãÅ—ãÅ_ãÅ¯ã€ÅãÅ©ãÅ®DVIã‚_ã‚§ã‚¢ã‚‚ã€Œ.vfãÅ®æ–_ãÅ®æ–‡å_—å_…ãÅ®æƒ…å ±ãÅ¯ç„¡è_–ãÅ™ã‚‹ã€çãÅ¨ãÅ„ãÅ†å‹•ä_œã‚’ãÅ™ã‚‹ã€‚ãÅªãÅ®ãÅ§upnmlminr-hãÅ¯ã€Œå®Ÿé_›ãÅ«ãÅ¯ãÅ¡ã‚ƒã‚“ãÅ¨ä_¿ãÅˆã‚‹ã€çãÅ®ãÅ§ãÅ‚ã‚‹ã€‚ãÅ¨ãÅ‚ã‚_ãÅ_ãÅ™ãÅŒï_Œå¿µãÅ®ãÅŸã‚Åèª¿ãÅ_ãÅ_ãÅ¿ãÅ_ãÅ™ã€‚",DevOps,upnmlminr-h.vf ãÅ®å_—å_…,standard,118,github-issue-415107996,conversations_github_issues,1,"['vssue', 'en']",,[Vssue] Comments for docs,standard,,,119,github-issue-415108583,conversations_github_issues,1,"['janeway', 'journal', 'publishing']",Customer Experience,We should consider whether it would be useful for Janeway to collect and report information on co-reviewers (additional people who help a referee perform peer review).,Analytics:DevOps,Co-reviewers,standard,120,github-issue-415111817,conversations_github_issues,1,"['bug', 'bareflank', 'stl', 'hypervisor', 'linux', 'windows', 'cxx', 'gsl']",DevOps,We ended up with a regression where the targets are always linking every time you execute make. This isn't a big deal on Linux as its fast but on Windows this is terrible.,DevOps,[BUG]: Targets always linking,standard,121,github-issue-415112875,conversations_github_issues,1,[],,"<blockquote class=""twitter-tweet""><p lang=""fr"" dir=""ltr"" xml:lang=""fr"">Livraison par drone &agrave; venir au Japon via un accord entre Rakuten et <a href=""https://t.co/yx9Y1ghzCw"">https://t.co/yx9Y1ghzCw</a> - La Revue du digital <a href=""https://t.co/KABE7oDZBI"">https://t.co/KABE7oDZBI</a> <a href=""https://twitter.com/hashtag/actu?src=hash&amp;ref_src=twsrc%5Etfw"">#actu</a> <a href=""https://twitter.com/hashtag/drone?src=hash&amp;ref_src=twsrc%5Etfw"">#drone</a></p>&mdash; Les drones (@les_drones) <a href=""https://twitter.com/les_drones/status/1100741892017451010?ref_src=twsrc%5Etfw"">February 27, 2019</a></blockquote><br><br>February 27, 2019 at 01:58PM<br>",DevOps,Livraison par drone Ã venir au Japon via un accord entre Rakuten et https://t.co/yx9Y1ghzCw - La Revue du digital https://t.co/KABE7oDZBI #actu #drone,standard,122,github-issue-415127256,conversations_github_issues,1,"['opsys-windows', 'enhancement', 'python', 'c', 'monitoring', 'ps', 'top', 'netstat', 'cpu', 'memory', 'memory-analysis', 'freebsd', 'osx', 'windows', 'netbsd', 'openbsd', 'linux', 'disk', 'sensors', 'windows-service', 'system-monitoring', 'process-monitor']",,"This is basically a note to self:https://docs.microsoft.com/en-us/sysinternals/downloads/pstools",DevOps,Look into sys-internals for ideas re. Windows,standard,123,github-issue-415078571,conversations_github_issues,1,"['podman', 'libpod', 'debian', 'runc', 'cni', 'cri-o', 'debian-package']",DevOps,"/assign <!-- @ the release branch manager or the person who cuts the release -->Scheduled to happen <!-- Tue, 2019-02-26 -->Any bumps or issues encountered along the way, post here. /close when the release has been cut.<!-- add & remove items of the checklist as you see fit -->- [ ] screen shot master's (unhealthy) testgrid boards and add them as a comment- [ ] stage & release- [ ] collect metrics, links, ... and add them as a comment <!-- example: https://github.com/kubernetes/sig-release/issues/506#issuecomment-465202113 -->- [ ] notify [#sig-release](https://kubernetes.slack.com/messages/C2C40FMNF/) <!-- e.g. https://kubernetes.slack.com/archives/C2C40FMNF/p1551205263064000 -->- [ ] build & publish packages (debs & rpms)- [ ] send notification mail/milestone <!-- v1.14 -->/sig release/area release-team<!--Example template for screenshots comment:----### Testgrid status#### master blocking<details><summary>`gce-cos-master-default`</summary></p>... paste image here ...</p></details>#### master-upgrade<details><summary>`gce-new-master-upgrade-master`</summary><p>... paste image here ...... paste multiple images here ...</p></details>------>",DevOps,"Cut 1.x.y-{alpha,beta,rc}.z release",standard,124,github-issue-415082244,conversations_github_issues,1,"['follow', 'github', 'organization', 'subscription', 'notification']",,"## Paste the link of the GitHub organisation below and submithttps://github.com/opensourcedesign---###### Please subscribe to this thread to get notified when a new repository is created",DevOps,Subscribe to Open Source Design,standard,125,github-issue-415101240,conversations_github_issues,1,"['flutter', 'flutter-desktop', 'dart']","DevOps, Customer Experience","HidebugDefaultTargetPlatformOverride = TargetPlatform.fuchsia; // for desktop embedderwhy you make target fuchsia?and is fuchsia apps will work in the future as cross-platform apps from day one?",DevOps:Customer Experience (UI + UX),Target Fuchsia,standard,126,github-issue-415102162,conversations_github_issues,1,"['julia', 'julia-wrapper', 'robotics']",DevOps,"I am on a Mac and MuJoCo by itself and every other part of the demo code in the readme works except these lines of code and their following errors:**mj.set(m, :opt, :timestep, -0.002)**ERROR: MethodError: no method matching update_ptr(::Base.RefValue{mjModel}, ::UInt64, ::Float64)Closest candidates are:update_ptr(::Ptr, ::Integer, ::Float64) at /Users/powers/.julia/packages/MuJoCo/GRTFJ/src/mjextra.jl:152update_ptr(::Ptr, ::Integer, ::Integer) at /Users/powers/.julia/packages/MuJoCo/GRTFJ/src/mjextra.jl:149update_ptr(::Ptr, ::Integer, ::StaticArrays.SArray{Tuple{S},T,1,S} where T where S) at /Users/powers/.julia/packages/MuJoCo/GRTFJ/src/mjextra.jl:155Stacktrace:[1] set(::jlModel, ::Symbol, ::Symbol, ::Float64) at /Users/powers/.julia/packages/MuJoCo/GRTFJ/src/mjextra.jl:204[2] top-level scope at none:0**m.m[].opt.timestep = 0.002**ERROR: setfield! immutable struct of type mjOption cannot be changedStacktrace:[1] setproperty!(::mjOption, ::Symbol, ::Float64) at ./sysimg.jl:19[2] top-level scope at none:0**d.qpos[:] = rand(nq)**ERROR: UndefVarError: nq not definedStacktrace:[1] top-level scope at none:0",DevOps,Demo in README not working,standard,127,github-issue-415115135,conversations_github_issues,1,['app: projects'],,Repo list should be alphabetical order,standard,,,128,github-issue-415121740,conversations_github_issues,1,"['refarm-site', 'pdd', 'ecommerce', 'django']",DevOps,"The puzzle `465-ed5b16ac` from #465 has to be resolved: https://github.com/fidals/stroyprombeton/blob/ae45c60f5dd7dfabfe5c10a4f00c6613cfc2bf7f/stroyprombeton/views/catalog.py#L140-L141The puzzle was created by duker33 on 27-Feb-19. If you have any technical questions, don't ask me, submit new tickets instead. The task will be \""done\"" when the problem is fixed and the text of the puzzle is _removed_ from the source code. Here is more about [PDD](http://www.yegor256.com/2009/03/04/pdd.html) and [about me](http://www.yegor256.com/2017/04/05/pdd-in-action.html).",DevOps,catalog.py:140-141: Implement `Tags.filter_by_options`...,standard,129,github-issue-415121922,conversations_github_issues,1,[],DevOps,"We will support Java inner class.Example is below.- Test.java```javaimport OuterTest.*;class Test{public static void main(String[] args){System.out.println(OuterTest.helloWorld());}}```- OuterTest.java```javaclass OuterTest{public static string helloWorld(){return ""Hello World!"";}}```",DevOps,Support to load outer class,standard,130,github-issue-415122000,conversations_github_issues,1,[],,,,filter_plugins.xml is not availalbe in install space,standard,131,github-issue-415131750,conversations_github_issues,1,"['18f-human-services', 'apd', 'eapd', 'cms', 'hhs', 'medicaid', 'hitech']","DevOps, Customer Experience","_Broken up for implementation from #1269______In order to build out the transition to the CMS design system, add the pieces needed to implement the systemâ€™s new components and styling for the State Medicaid Director and Medicaid Office Address section: https://hitech-apd.app.cloud.gov/apd#apd-state-profile-office### This task is done when...[ ] The section is not contained in an accordion- [ ] The headline typography is stylistically and semantically correct- [ ] The [Form label component](https://design.cms.gov/components/form-label/) has been added and used for each field- [ ] The [Text field component](https://design.cms.gov/components/text-field/) has been added and used for each field- [ ] The [Select component](https://design.cms.gov/components/select/) has been added and used for the `State` field- [ ] Overall layout and interaction matches the [InVision prototype](https://gsa.invisionapp.com/share/BJQNX2KXQVW#/348460956_State_Profile_Contact)![screen shot 2019-02-27 at 12 01 24 am](https://user-images.githubusercontent.com/11636908/53468246-67231a00-3a27-11e9-87be-21daff5d7f64.png)",DevOps,Implement CMS Design System in State Medical Director child section,standard,132,github-issue-415130628,conversations_github_issues,1,[],DevOps,"Using the component with the [Quasar framework](https://quasar-framework.org/) I have the following error below.**`Failed to mount component: template or render function not defined.`**![captura de tela 2019-02-27 as 10 37 24](https://user-images.githubusercontent.com/3280420/53494267-18539f80-3a7c-11e9-9978-ddde9a96a341.png)Created plugin quasar > > import VueAnimateNumber from 'vue-animate-number'> > export default ({ app, router, Vue }) => {> Vue.use(VueAnimateNumber)> }> and add in quasar.conf.js`plugins: [''vue-animate-number'']`",DevOps,Failed to mount component: template or render function not defined.,standard,133,github-issue-415134197,conversations_github_issues,1,"['scraping', 'e2e-testing', 'webdriver', 'selenium', 'selenium-webdriver', 'symfony', 'php', 'chromedriver']",DevOps,"Hello,I have a small phar, built using humbug/box. It's a symfony/console based application, where I wanted to use panther.Trying to execute command with Client::createChromeClient() results in:```sh: 1: exec: phar:///home/wojciechem/myphar-testing/myphar.phar/vendor/symf ony/panther/src/ProcessManager/../../chromedriver-bin/chromedriver_linux64: not found```When I extract the phar contents, the file does exist in expected path.PHP 7.2.15-1+ubuntu16.04.1+deb.sury.org+1 (cli). I have zip extension installed.Am I missing something trivial here? Should I use external chromedriver?",DevOps,[Question] How to run inside phar,standard,134,github-issue-415134212,conversations_github_issues,1,[],Frontend tools,"Feedback from Peter Durham:HTMLPage Has a crawlerDepth property in the schema, is this describing the depth in the crawl at which the current page was found? In a crawl scenario, would we expect results in a single AXRL instance or spread across multiple instances?",DevOps,crawlerDepth question,standard,135,github-issue-415134545,conversations_github_issues,1,[],"DevOps, Customer Experience","Feedback from Peter Durham:In the multiple test example, HTMLPage is used as the container, with a tests property not in the schema. Will there be a way to include tests of different pages in one structure?",DevOps,"Define ""tests"" as a reverse ""subject"" relationship",standard,136,github-issue-415156191,conversations_github_issues,1,[],,,,"Na lista de ferramenta tirar: "" Gerador de """,standard,137,github-issue-415159463,conversations_github_issues,1,[],,There must be a new contact created on HubSpot.,DevOps,From Hubspot,standard,138,github-issue-415163478,conversations_github_issues,1,"['pf4', 'accessibility', 'reactjs', 'bootstrap3', 'patternfly']","DevOps, Customer Experience","To meet AA WCAG 2.1 compliance, Tooltips should be persistent on hover (i.e. when you move the mouse from the tooltip toggle to the tooltip contents.The WCAG item with more details about this: [WCAG 1.4.13](https://www.w3.org/WAI/WCAG21/quickref/#content-on-hover-or-focus)",DevOps:Frontend tools:Customer Experience (UI + UX),Tooltip - persistent on hover,standard,139,github-issue-415185489,conversations_github_issues,1,"['java-shell', 'java-interpreter', 'scripting-language', 'beanshell', 'java', 'beanshell-scripting-language']",DevOps,"DEBUG in the `Interpreter` class was changed to a ThreadLocal, but the BSFEngine implementation in the bsh-bsf-engine sub-project assumes that it is a Boolean.",DevOps,bsh-bsf-engine sub-project build fails,standard,140,github-issue-415187523,conversations_github_issues,1,"['api', 'refactor']",DevOps,"## overviewThe deck calibration CLI instructions are incorrect and should be added to a README so that the information is correct.Also, there are some to-do's listed in the `test_cli.py` to prevent against fragility of the CLI tool. There were some bugs found that would have been caught with some simple tests.",DevOps,API: Add instructions for Deck Calibration CLI Tool and extra tests,standard,141,github-issue-415187567,conversations_github_issues,1,[],DevOps,Is the conversion function not terminating? Is that what is causing the coroutine event loop not to finish? Should there be an `await` on the Popen handle?,DevOps,conversion function not terminating?,standard,142,github-issue-415156514,conversations_github_issues,1,[],,There must be a new contact created on HubSpot.,DevOps,From Hubspot,standard,143,github-issue-415155907,conversations_github_issues,1,[],,,,"BotÃ£o ferramenta (no topo) para escolher entre: n-grama, concordanciador e lista de palavras",standard,144,github-issue-415163013,conversations_github_issues,1,[],,äººé¡_ç_„å_©æ€§æ˜¯åêˆä_œã€‚è€Œå_‚æ_œèƒ_åœ¨ç__æœƒä¸_æœ‰ä¸€ä»_ã€Œå·¥ä_œã€çï_Œä¸çç®¡å__ç°¡å–®éƒ_æ_’é—œä¿‚ã€‚å·¥ä_œå°±æ˜¯åœ¨é€™å€‹é«”åˆ¶è£¡è·Ÿåˆ¥äººåÅ_äº_æ˜“ï_Œé‚£éº_å°±æ˜¯ç__æœƒåŒ–äº†ï_Å,DevOps:JAMstack:Mobile Services,å·¥ä_œç_„æœ¬è_ªæ˜¯é«”åˆ¶å…§ç_„ç__æœƒåŒ–,standard,145,github-issue-415182574,conversations_github_issues,1,[],DevOps,Error when compiling to PDF,DevOps,Package xkeyval Error: `width' undefined in families `Gin'.,standard,146,github-issue-415186786,conversations_github_issues,1,[],,There must be a new contact created on HubSpot.,DevOps,From Hubspot,standard,147,github-issue-415186650,conversations_github_issues,1,[],DevOps,"This command from the [lesson](https://datacarpentry.org/wrangling-genomics/03-trimming/index.html) doesn't work:```cp ~/miniconda3/pkgs/trimmomatic-0.38-0/share/trimmomatic-0.38-0/adapters/NexteraPE-PE.fa .```because of how the things are installed. This works instead:```cp /opt/conda/pkgs/trimmomatic-0.38-0/share/trimmomatic-0.38-0/adapters/NexteraPE-PE.fa .```",DevOps,Path issue with Illumina adapters for Trimmomatic,standard,148,github-issue-415157252,conversations_github_issues,1,"['civic-tech', 'transparency', 'parliamentary-monitoring', 'politics', 'politicians', 'uk', 'parliament', 'civictech', 'mysociety', 'democracy']",DevOps,"PHP7 introduces [new error handling](http://php.net/manual/en/language.errors.php7.php), which the current version of the Whoops error handler [doesn't understand](https://github.com/filp/whoops/issues/341). Whoops 2 fixes this and other problems as a result of moving to PHP7.",DevOps,Upgrade Whoops to version 2,standard,149,github-issue-415160659,conversations_github_issues,1,[],DevOps,"Rust's [NonNull has been stabilized](https://doc.rust-lang.org/std/ptr/struct.NonNull.html) for a while now. Double check for each usage that the covariance rule makes sense, however.",DevOps,Use NonNull,standard,150,github-issue-415161311,conversations_github_issues,1,"['graphql', 'ide', 'graphiql', 'prisma', 'graphql-playground']",DevOps,"#### This issue pertains to the following package(s):- [ ] GraphQL Playground - Electron App- [ ] GraphQL Playground HTML- [ ] GraphQL Playground- [ ] GraphQL Playground Express Middleware- [ ] GraphQL Playground Hapi Middleware- [ ] GraphQL Playground Koa Middleware- [ ] GraphQL Playground Lambda Middleware- [X ] GraphQL Playground React#### What OS and OS version are you experiencing the issue(s) on?macOS Mojave#### What version of graphql-playground(-electron/-middleware) are you experiencing the issue(s) on?""graphql-playground-react"": 1.7.20#### What is the expected behavior?returning a valid ApolloLink as specified in createApolloLink should work, but the types are wrong, createApolloLink should return object { link: ApolloLink } not ApolloLink!#### What is the actual behavior?SchemaFetcher.js fetchSchema fails to get ApolloLink, because it looks for it in a object { link: xxx }. #### What steps may we take to reproduce the behavior?my playground setup which produces an error when fetching schema:```js<Playgroundendpoint={graphqlEndpoint || ''}subscriptionEndpoint={subsEndpoint}workspaceName={'Test'}createApolloLink={(session: Session, subscriptionEndpoint?: string): ApolloLink => {const myLink = ApolloLink.from([errorLink, authLink, httpLink]);console.log('link', myLink);myLink;}}/>```from PlaygroundWrapper.d.ts we can see that I'm supposed to return a ApolloLink:```export interface PlaygroundWrapperProps {endpoint?: string;...createApolloLink?: (session: Session, subscriptionEndpoint?: string) => ApolloLink;...}```but this should probably be:```export interface PlaygroundWrapperProps {endpoint?: string;...createApolloLink?: (session: Session, subscriptionEndpoint?: string) => { link: ApolloLink; subscriptionClient?: SubscriptionClient };...}```temporary workaround using any type passing correct type works!:```js<Playgroundendpoint={graphqlEndpoint || ''}subscriptionEndpoint={subsEndpoint}workspaceName={'Test'}createApolloLink={(session: Session, subscriptionEndpoint?: string): any => {const myLink = ApolloLink.from([errorLink, authLink, httpLink]);return { link: myLink }; // this works, but get compile error if I remove any type}}/>```code where it fails to get link:https://github.com/prisma/graphql-playground/blob/a220dc00b7bb0fa3c35f13b8691c66be8ef49a82/packages/graphql-playground-react/src/state/sessions/fetchingSagas.ts_Please provide a gif or image of the issue for a quicker response/fix._",DevOps,"Typescript bug, createApolloLink wrong return type",standard,151,github-issue-415161023,conversations_github_issues,1,[],DevOps,reason: `callbag-subscribe` not transpiled to es5,DevOps,useMount breaks in IE11,standard,152,github-issue-415179777,conversations_github_issues,1,[],DevOps,"Missing arguments `visible` and `active` for loop `lang`Missing arguments `visible` for loop `currency`",DevOps,Missing arguments for loop lang and currency,standard,153,github-issue-415181284,conversations_github_issues,1,[],Customer Experience,Can the regions be ordered in the filter in the same order as in the charts? (north to south),DevOps,Order of regions in Filter,standard,154,github-issue-415184543,conversations_github_issues,1,[],,è°¢è°¢ã€‚,DevOps:JAMstack:Mobile Services,æœ‰æ_¡æœ‰cuda8.0ç_„ç‰ˆæœ¬å•_ï_Ÿ,standard,155,github-issue-415184560,conversations_github_issues,1,[],,,,User Story: Sample Magento 2 store,standard,156,github-issue-415188054,conversations_github_issues,1,[],"DevOps, Customer Experience","Add a modifier class to `.pf-c-page__main-section` that removes padding from the main section, to allow cards to sit on the edges of the viewport. This is necessary for the data table and data list demos.",DevOps:Customer Experience (UI + UX):Analytics,Add modifier class to `.pf-c-page__main-section`,standard,157,github-issue-415188170,conversations_github_issues,1,['admin'],"DevOps, Frontend tools","### Basic requirement- [ ] Create react app- [ ] React admin- [ ] Typescript (or Javascript) ???",DevOps,Initialize admin side,standard,158,github-issue-415188582,conversations_github_issues,1,['testing'],"DevOps, Frontend tools","### Basic requirement- [ ] Cypress- [ ] Typescript",DevOps,Initialize end-to-end testing,standard,159,github-issue-415188164,conversations_github_issues,1,"['enhancement', 'warehouse']",,AbhÃ_ngig von der Implementierung der Transaktionen fÃ_r BestandsÃ_nderungen,DevOps,"Lager kann nur gelÃ¶lscht werden, wenn es leer ist",standard,160,github-issue-415076420,conversations_github_issues,1,[],,There must be a new contact created on HubSpot.,DevOps,From Hubspot,standard,161,github-issue-415079718,conversations_github_issues,1,[],Customer Experience,"There are conventions and approaches that are not clear. They could be solved with a more intuitive UI, but for the time being would be good to document them.",DevOps:Customer Experience (UI + UX),"Document ssml mapping, conventions and approaches",standard,162,github-issue-415086532,conversations_github_issues,1,['question'],,,,Evaluate returning unmodifiable list for queries,standard,163,github-issue-415086971,conversations_github_issues,1,"['follow', 'github', 'organization', 'subscription', 'notification']",,"## Paste the link of the GitHub organisation below and submithttps://github.com/flexport---###### Please subscribe to this thread to get notified when a new repository is created",DevOps,Subscribe to Flexport,standard,164,github-issue-415103163,conversations_github_issues,1,"['convolutional-neural-networks', 'inference-engine', 'caffe', 'android', 'ios', 'arm-neon']",DevOps,"I can see there is toolchain for aarch64 arm device, so we can build in RPiB3 with 64-bit OS. How about raspbian os which has armv7l. How can I build in such device. Thanks.",DevOps,Does this also support RPi3B with Raspbian OS (armv7l),standard,165,github-issue-415101902,conversations_github_issues,1,[],DevOps,"I was able to resolve Blockhound with version ""1.0.0.BUILD-SNAPSHOT"" (the README says ""v1.0.0.BUILD-SNAPSHOT"" via an embedded image).",DevOps,"Bug in README: version does not have ""v"" prefix",standard,166,github-issue-415107442,conversations_github_issues,1,[],"DevOps, JAMstack","The API returns this as a top-level key titled ""`isHighSchool`""",DevOps,Add checkbox for high school only,standard,167,github-issue-415110951,conversations_github_issues,1,"['qt', 'libraries']",,"OpenRPT Report WriterLink: [Here](https://github.com/xtuple/openrpt)",DevOps,Request: add OpenRPT,standard,168,github-issue-415123238,conversations_github_issues,1,[],,,,Imitate java.util.*,standard,169,github-issue-415131043,conversations_github_issues,1,"['trezor', 'micropython', 'bitcoin']",Customer Experience,"It seems that Dash did a hard fork which introduces a new TX format. The old TX format still works, but there might be some trouble while spending the new format because Dash doesn't use BIP-143 and needs to stream prev-txs.",DevOps,Investigate the new Dash TX format,standard,170,github-issue-415135384,conversations_github_issues,1,['bug'],Customer Experience,"If you add a big piece of text which you try to split in multiple paragraphs by hitting the return key, then it adds the ssml closing paragraph tag at the end.",DevOps:Customer Experience (UI + UX):Frontend tools,Splitting existing text to paragraphs closes all paragraphs at end,standard,171,github-issue-415137635,conversations_github_issues,1,['task'],Analytics,"Produce a spreadsheet with all by all attribute comparison between HCA and ArrayExpress metadata.**Description**The metadata team needs to understand how attributes in our schema align with other resources. Initially we will map to ArrayExpress using work already done by Robert Petryszak.If possible we could extend this analysis to include mapping to the following:- SCEA/ArrayExpress- NeMo- GEO/SRA- CIRM**Acceptance Criteria**- [ ] Make a table of attributes and map usage across schemas.- [ ] Categories attributes based on MINSEQE definitions.- [ ] Assessment of how long it will take to include the other archives/projects.- [ ] Add frequency of use information covered in ticket https://github.com/HumanCellAtlas/metadata-schema/issues/847",DevOps,Mapping of attributes across single cell RNA seq archives,standard,172,github-issue-415137059,conversations_github_issues,1,"['high priority', 'refactor:api', 'refactor:front', 'webapp', 'tool', 'visual', 'bookmark', 'colllect', 'collection', 'collaboration']",,https://symfony.com/doc/current/security/guard_authentication.html,DevOps:JAMstack:Frontend tools:Mobile Services,Refactor auth layer from JWT to API key,standard,173,github-issue-415137717,conversations_github_issues,1,"['high priority', 'refactor:front', 'webapp', 'tool', 'visual', 'bookmark', 'colllect', 'collection', 'collaboration']",,Upgrade node deps,standard,,,174,github-issue-415137369,conversations_github_issues,1,[],,,,"Disable the ""Apply to council"" button if an applicant tries to stake more than they have on a balance",standard,175,github-issue-415140006,conversations_github_issues,1,"['customer-products', 'component']","DevOps, Customer Experience","Currently if the `nht gtg [url]` command fails, it doesn't exit properly, it just gives the error:(node:220) UnhandledPromiseRejectionWarning: _Ÿ˜¢ http://ft-next-arti-remove-clu-yxgglg.herokuapp.com/__gtg did not respond with an ok response within two minutes.eg, https://circleci.com/gh/Financial-Times/next-article/16102---This is probably where changes need to be made: https://github.com/Financial-Times/n-heroku-tools/blob/master/tasks/gtg.js#L10Example of how exit is done in `nht`: https://github.com/Financial-Times/n-heroku-tools/blob/0ada2df0ff875a7f3fdc81a743359193557e4193/bin/n-heroku-tools.js#L13-L16Good time to add tests for the `gtg` command too _Ÿ˜„",DevOps,Good to go (gtg) failures should exit with an error code,standard,176,github-issue-415139178,conversations_github_issues,1,"['bugs', 'issues', 'tracker', 'fabric', 'crashlitics', 'kodular', 'firebase']","DevOps, Mobile Services","#### in android.view.ViewRootImpl.performTraversals* Number of crashes: 1* Impacted devices: 1There's a lot more information about this crash on crashlytics.com:[https://fabric.io/makeroid4/android/apps/io.makeroid.application/issues/5c769815f8b88c29634e3a48?utm_medium=service_hooks-github&utm_source=issue_impact](https://fabric.io/makeroid4/android/apps/io.makeroid.application/issues/5c769815f8b88c29634e3a48?utm_medium=service_hooks-github&utm_source=issue_impact)",DevOps:Mobile Services,ViewRootImpl.java line 2047,standard,177,github-issue-415077377,conversations_github_issues,1,[],,"Moving from godotengine/godot#14553, see details there.",DevOps,"GDNative linear indexing of Basis returns rows, expected columns (axis)",standard,178,github-issue-415090346,conversations_github_issues,1,"['bug', 'production']","DevOps, Containers","## Error in Faveo Community**Symfony\Component\Debug\Exception\FatalErrorException** in **vendor/laravel/framework/src/Illuminate/Container/Container.php:698**Maximum execution time of 30 seconds exceeded[View on Bugsnag](https://app.bugsnag.com/ladybird-web-solution-pvt-ltd/faveo-community/errors/5c767c5105bbe70018a1702d?event_id=5c767c510035ab9097410000&i=gh&m=ci)## Stacktracevendor/laravel/framework/src/Illuminate/Container/Container.php:698 - [main][View full stacktrace](https://app.bugsnag.com/ladybird-web-solution-pvt-ltd/faveo-community/errors/5c767c5105bbe70018a1702d?event_id=5c767c510035ab9097410000&i=gh&m=ci)*Created automatically via Bugsnag*",DevOps,Symfony\Component\Debug\Exception\FatalErrorException in vendor/laravel/framework/src/Illuminate/Container/Container.php:698,standard,179,github-issue-415090313,conversations_github_issues,1,['enhancement'],,All the objects and parts should be written according to https://www.neos.io/blog/neos-best-practices-1-0.html,DevOps,Refactor fusion code to fully use afx and components,standard,180,github-issue-415106768,conversations_github_issues,1,"['amiko', 'fachinfo', 'java', 'android', 'opensource']",Customer Experience,"The font in the package chooser pop-up should be smaller (as big as the font of the packages, that are show in the search result list.",DevOps:Customer Experience (UI + UX),Make font in package chooser smaller,standard,181,github-issue-415106375,conversations_github_issues,1,[],,There must be a new contact created on HubSpot.,DevOps,From Hubspot,standard,182,github-issue-415110306,conversations_github_issues,1,"['enhancement', 'network-analysis', 'missing-data', 'stochastic-block-model', 'network-dataset']",Networking,Probably in file networkSampling_fit-Class.R,DevOps,Task: Add a class for fitting dyad and node sampling in the presence of covariates,standard,183,github-issue-415127692,conversations_github_issues,1,"['bug', 'zsh', 'git', 'prompt']",DevOps,"```thread 'main' panicked at 'no ahead behind stats found for = Some(RemoteBranch { remote_branch: ""upstream/master"", remote_branch_name: ""master"", remote_name: ""upstream"" })', src/models.rs:252:17note: Run with `RUST_BACKTRACE=1` for a backtrace.```p-g-p should likely just not print anything in case such a thing happens",DevOps,error when there is no shared history,standard,184,github-issue-415132309,conversations_github_issues,1,"['app: projects', 'quick-fix']",,![image.png](https://images.zenhubusercontent.com/5ab2c6f24b5806bc2bcbd667/1e6de5ec-662b-476e-ac43-9da64f16e81c),DevOps:Mobile Services:JAMstack,Review Work - line height of issue title should match Review Application panel,standard,185,github-issue-415141495,conversations_github_issues,1,"['r', 'data-science', 'teaching-materials', 'educational-materials', 'workshop-materials']",DevOps,"Love the slides! I thought helpful additions here would be to talk about R types (ints, numerics, strings, etc) and data structured (lists, named lists, etc), and cover the stringr package a bit. also, the concept of ""vectorization"" which is so pervasive in R. sqrt(c(1,2,3,4,5)) etc.",DevOps,Talk a bit about types and vectorization,standard,186,github-issue-415148801,conversations_github_issues,1,"['bug', 'abap', 'abapgit', 'sap', 'git-client']",DevOps,"installhttps://github.com/abapGit-tests/DCLSorhttps://github.com/hardyp/AbapToTheFuture03and click uninstall, it gives errorrelated: https://github.com/larshp/abapGit/issues/2464",DevOps:Frontend tools,"DCLS vs dependencies, error while uinstalling",standard,187,github-issue-415083179,conversations_github_issues,1,[],,There must be a new contact created on HubSpot.,DevOps,From Hubspot,standard,188,github-issue-415082769,conversations_github_issues,1,[],DevOps,"https://github.com/python/python-docs-fr/issues/602see issue above and black. So we can add it to `python-docs-fr` travis",DevOps:Customer Experience (UI + UX),Return 1 if a file has been modified in a CI environement,standard,189,github-issue-415089852,conversations_github_issues,1,"['enhancement', 'good first issue', 'docker', 'kopano']",Containers,This way a secure connection to kopano_ical can be made quite trivial to setup.,Blockchain,Add reverse proxy configuration for /caldav,standard,190,github-issue-415095889,conversations_github_issues,1,['enhancement'],DevOps,The README should reflect the nature of this project because it is the first message people will see.,DevOps,Improve Readme,standard,191,github-issue-415098996,conversations_github_issues,1,"['feature request', 'progressive-delivery', 'istio', 'kubernetes', 'prometheus', 'canary', 'gitops', 'continuous-deployment']",Analysis,The metrics validation should be extended to support any kind of metric by allowing custom Prometheus queries in the canary analysis spec.,DevOps,Support custom Prometheus metrics,standard,192,github-issue-415099115,conversations_github_issues,1,[],,,,Make language selectable in editor,standard,193,github-issue-415106119,conversations_github_issues,1,[],,There must be a new contact created on HubSpot.,DevOps,From Hubspot,standard,194,github-issue-415114676,conversations_github_issues,1,['task'],,"__Ñ‚_° _·_°_´_°Ñ‡_°Â â€” _¿___´_·_°_´_°Ñ‡_° #185. _ŸÑ€__Ñ†_¸Ñ‚_¸Ñ€ÑƒÑ_ _¿_°Ñ€_°__Ñ€_°Ñ„ __Ñ‚Ñ‚Ñƒ_´_°.> _______¿_¸_»ÑèÑ†_¸Ñè Ñ€_µ_·Ñƒ_»ÑŒÑ‚_°Ñ‚__Ñ‹Ñ… __Ñ‹Ñ€_°_¶_µ___¸__ __ _ _µÑ„_°_»_µ-05 __Ñ‚_»_¸Ñ‡_°_µÑ‚ÑÅÑè __Ñ‚ _ _µÑ„_°_»_°-5Î» _´__Ñƒ__Ñè ______Ñˆ_µÑÅÑ‚___°___¸:> * _˜ÑÅ_¿___»ÑŒ_·Ñƒ_µÑ‚ÑÅÑè _´___¿ÑƒÑ‰_µ___¸_µ __ _¿__ÑÅ_»_µ_´_____°Ñ‚_µ_»ÑŒ______ Ñ€_°ÑÅ_¿Ñ€_µ_´_µ_»_µ___¸_¸ _¿_°__ÑèÑ‚_¸ __ ÑÅ_¿_¸ÑÅ_º_µ ÑÅ_____±___´__Ñ‹Ñ… Ñƒ_·_»____.> * __Ñƒ___ºÑ†_¸_¸ __Ñ‹_´_µ_»_µ___¸Ñè _¿_°__ÑèÑ‚_¸ __ÑÅ_µ___´_° ÑƒÑÅ_¿_µÑˆ__Ñ‹.> _”___¿ÑƒÑ‰_µ___¸_µ __ _¿__ÑÅ_»_µ_´_____°Ñ‚_µ_»ÑŒ______ Ñ€_°ÑÅ_¿Ñ€_µ_´_µ_»_µ___¸_¸ Ñƒ_·_»____ _´_»Ñè ___µ__Ñè (@Mazdaywik) ___µ ________ â€” Ñè Ñ‚_°_º _´_µ_»_°_» _¸ __ _œ___´Ñƒ_»ÑŒ______ _ _µÑ„_°_»_µ.> _¡ÑƒÑ‚ÑŒ _µ____ ÑÅ__ÑÅÑ‚___¸Ñ‚ __ Ñ‚____, Ñ‡Ñ‚__ ÑÅ_»_µ_´ÑƒÑ_Ñ‰_¸_µ _´Ñ€Ñƒ__ _·_° _´Ñ€Ñƒ______ ___¿_µÑ€_°Ñ†_¸_¸ Ñ€_°ÑÅ_¿Ñ€_µ_´_µ_»_µ___¸Ñè _¿_°__ÑèÑ‚_¸ ÑÅ___·_´_°Ñ_Ñ‚ __ ÑÅ_¿_¸ÑÅ_º_µ ÑÅ_____±___´__Ñ‹Ñ… Ñƒ_·_»____ _·___°Ñ‡_µ___¸Ñè, Ñ€_°ÑÅ_¿___»_°___°Ñ_Ñ‰_¸_µÑÅÑè _¿__ÑÅ_»_µ_´_____°Ñ‚_µ_»ÑŒ____ __ Ñ‚____ _¶_µ _¿__Ñ€Ñè_´_º_µ. _Ÿ__ÑÅ_»_µ Ñ‡_µ____ __ ÑÅ_¿_µÑ†_¸_°_»ÑŒ____ _·_°____Ñ‚_____»_µ____Ñ‹_µ ___µÑÅÑ‚_° (__ ÑÅ_¿_¸ÑÅ___º ÑÅ_____±___´__Ñ‹Ñ… Ñƒ_·_»____) _¿_µÑ€_µ____ÑÅÑèÑ‚ÑÅÑè _·___°Ñ‡_µ___¸Ñè _¿_µÑ€_µ___µ____Ñ‹Ñ… _¸ __ Ñ€_µ_·Ñƒ_»ÑŒÑ‚_°Ñ‚_µ Ñ‚_°__ _¿___»ÑƒÑ‡_°_µÑ‚ÑÅÑè ___±Ñ€_°_· Ñ€_µ_·Ñƒ_»ÑŒÑ‚_°Ñ‚________ __Ñ‹Ñ€_°_¶_µ___¸Ñè. ___±Ñ€_°_· _¿_µÑ€_µ____ÑÅ_¸Ñ‚ÑÅÑè __ _¿___»_µ _·Ñ€_µ___¸Ñè _º_____°___´____ `splice_from_freelist`, _º__Ñ‚__Ñ€_°Ñè Ñ€_°_·___µÑ‰_°_µÑ‚ _µ____ _¿_µÑ€_µ_´ __Ñ‚_ºÑ€Ñ‹___°Ñ_Ñ‰_µ__ Ñƒ___»________ ÑÅ_º___±_º____. _—_°Ñ‚_µ__ __ÑÅÑ‚_°Ñ‚___º __Ñ‚ __Ñ‹_·_____° Ñ„Ñƒ___ºÑ†_¸_¸ _¿_µÑ€_µ____ÑÅ_¸Ñ‚ÑÅÑè __ ÑÅ_¿_¸ÑÅ___º ÑÅ_____±___´__Ñ‹Ñ… Ñƒ_·_»____ _º_____°___´____ `splice_to_freelist`.> _£ ÑçÑ‚______ _¿___´Ñ…___´_° _´___° _¿Ñ€_µ_¸__ÑƒÑ‰_µÑÅÑ‚___°: _¿____Ñ‹Ñˆ_°_µÑ‚ÑÅÑè _±Ñ‹ÑÅÑ‚Ñ€___´_µ__ÑÅÑ‚___¸_µ _¸ Ñƒ_¿Ñ€__Ñ‰_°_µÑ‚ÑÅÑè ___µ___µÑ€_°Ñ†_¸Ñè _º___´_°. _˜ Ñ‚__, _¸ _´Ñ€Ñƒ_____µ ___±_µÑÅ_¿_µÑ‡_¸___°_µÑ‚ÑÅÑè __Ñ‚ÑÅÑƒÑ‚ÑÅÑ‚___¸_µ__ _º_____°___´ _¿_µÑ€_µ____ÑÅ_° _¿__ÑÅÑ‚Ñ€___µ____Ñ‹Ñ… Ñƒ_·_»____ _¿__ __Ñ‚_´_µ_»ÑŒ____ÑÅÑ‚_¸.> _’ Ñ‚_µ_ºÑƒÑ‰_µ__ Ñ€_µ_°_»_¸_·_°Ñ†_¸_¸ _ _µÑ„_°_»_°-5Î» Ñ„Ñƒ___ºÑ†_¸_¸ Ñ€_°ÑÅ_¿Ñ€_µ_´_µ_»_µ___¸Ñè _¿_°__ÑèÑ‚_¸ _____·__Ñ€_°Ñ‰_°Ñ_Ñ‚ _±Ñƒ_»_µ__ÑÅ_º___µ _·___°Ñ‡_µ___¸_µ: _¸ÑÅÑ‚_¸__Ñƒ, _µÑÅ_»_¸ _¿_°__ÑèÑ‚ÑŒ Ñ€_°ÑÅ_¿Ñ€_µ_´_µ_»_¸Ñ‚ÑŒ Ñƒ_´_°_»__ÑÅÑŒ, _¸ _»___¶ÑŒ, _µÑÅ_»_¸ _¿_°__ÑèÑ‚_¸ ___º_°_·_°_»__ÑÅÑŒ ___µ_´__ÑÅÑ‚_°Ñ‚__Ñ‡____. _’ _º__Ñ€Ñ€_µ_ºÑ‚__Ñ‹Ñ… _¿Ñ€____Ñ€_°_____°Ñ… _¸ÑÅÑ‚_¸___° _____·__Ñ€_°Ñ‰_°_µÑ‚ÑÅÑè __ÑÅ_µ___´_°, _° _·___°Ñ‡_¸Ñ‚, __ÑÅ_µ___´_° __Ñ‹_¿___»__Ñè_µÑ‚ÑÅÑè _¸_·_±Ñ‹Ñ‚__Ñ‡___°Ñè _¿Ñ€_____µÑ€_º_°. _•ÑÅ_»_¸ ___° ÑÅÑ‚_°_´_¸_¸ Ñ€_°ÑÅ_¿Ñ€_µ_´_µ_»_µ___¸Ñè _¿_°__ÑèÑ‚_¸ _¿_°__ÑèÑ‚_¸ ___º_°_·_°_»__ÑÅÑŒ ___µ_´__ÑÅÑ‚_°Ñ‚__Ñ‡____, Ñ‚__ Ñ„Ñƒ___ºÑ†_¸Ñè _____·__Ñ€_°Ñ‰_°_µÑ‚ _¿Ñ€_¸_·___°_º __Ñˆ_¸_±_º_¸ `cNoMemory`, __ __Ñ‚___µÑ‚ ___° _º__Ñ‚__Ñ€Ñ‹__ Ñ€_°__Ñ‚_°____ __Ñ‹_____´_¸Ñ‚ _´_°___¿ _¸ _°___°Ñ€_¸______ __ÑÅÑ‚_°___°___»_¸___°_µÑ‚ _¿Ñ€____Ñ€_°____Ñƒ.> _’ _ _µÑ„_°_»_µ-05 ___¿_µÑ€_°Ñ†_¸_¸ Ñ€_°ÑÅ_¿Ñ€_µ_´_µ_»_µ___¸Ñè _¿_°__ÑèÑ‚_¸ __Ñ‹_¿___»__ÑèÑ_Ñ‚ÑÅÑè __ÑÅ_µ___´_° ÑƒÑÅ_¿_µÑˆ____ â€” __ API ___µ _¿Ñ€_µ_´ÑƒÑÅ____Ñ‚Ñ€_µ____ ___¸_º_°_º____ _¿Ñ€_____µÑ€_º_¸ ___° ___µ_´__ÑÅÑ‚_°Ñ‚___º _¿_°__ÑèÑ‚_¸. _•ÑÅ_»_¸ ___¿_µÑ€_°Ñ†_¸Ñè _°_»_»___º_°Ñ†_¸_¸ _¿_°__ÑèÑ‚ÑŒ __Ñ‹_´_µ_»_¸Ñ‚ÑŒ ___µ ÑÅ_______»_°, Ñ‚__ _____° ÑÅ_°___° _¿Ñ€_µÑ€Ñ‹___°_µÑ‚ _¿Ñ€____Ñ€_°____Ñƒ ÑÅ __Ñ‹_____´____ _°___°Ñ€_¸__________ _´_°___¿_°.> __Ñ‚__Ñ‚ _¿___´Ñ…___´ Ñ€_°_·Ñƒ______ _¿_µÑ€_µ___µÑÅÑ‚_¸ ___° _ _µÑ„_°_»-5Î». __ÑÅÑ‚_°___°___»_¸___°Ñ‚ÑŒ _»_¸ _¿Ñ€____Ñ€_°____Ñƒ __ ÑÅ_°______ Ñ„Ñƒ___ºÑ†_¸_¸ Ñ€_°ÑÅ_¿Ñ€_µ_´_µ_»_µ___¸Ñè _¸_»_¸ _¸ÑÅ_¿___»ÑŒ_·_____°Ñ‚ÑŒ ___µ_»___º_°_»ÑŒ__Ñ‹__ _¿_µÑ€_µÑ…___´ (_¸ÑÅ_º_»Ñ_Ñ‡_µ___¸_µ _¸_»_¸ `longjmp()`) â€” ÑÅ_»_µ_´Ñƒ_µÑ‚ Ñ€_µÑˆ_¸Ñ‚ÑŒ._’Â _ _µÑ„_°_»_µ-05 _¿Ñ€_¸ __Ñˆ_¸_±_º_µ __Ñ‚___¶_´_µÑÅÑ‚___»_µ___¸Ñè _¸Â ___µ_´__ÑÅÑ‚_°Ñ‚_º_µ _¿_°__ÑèÑ‚_¸ _¿Ñ€____Ñ€_°_____° _°___°Ñ€_¸______ __ÑÅÑ‚_°___°___»_¸___°_µÑ‚ÑÅÑè. _ _µÑ„_°_»-5Î» ___µÂ _____¶_µÑ‚ _¿___·_____»_¸Ñ‚ÑŒ ÑÅ_µ_±_µ Ñ‚_°_ºÑƒÑ_ Ñ€__ÑÅ_º__ÑˆÑŒÂ â€” __Â ÑÅ_»ÑƒÑ‡_°_µ _°___°Ñ€_¸________ ÑÅ_¸Ñ‚Ñƒ_°Ñ†_¸_¸ _¿Ñ€____Ñ€_°_____° _____¶_µÑ‚ _¿Ñ€___´___»_¶_°Ñ‚ÑŒ __Ñ‹_¿___»__ÑèÑ‚ÑŒÑÅÑè, ___°_¿Ñ€_¸___µÑ€, _µÑÅ_»_¸ _¿Ñ€____Ñ€_°_____° _¸___µ_µÑ‚ ___µÑÅ_º___»ÑŒ_º__ _´_____µ______ _¸_»_¸ ___µÑÅ_º___»ÑŒ_º__ ___¸Ñ€Ñ‚Ñƒ_°_»ÑŒ__Ñ‹Ñ… ___°Ñˆ_¸__. _ù_°_¿Ñ€_¸___µÑ€, _____¶_µÑ‚ _±Ñ‹Ñ‚ÑŒ Ñ€_µ_°_»_¸_·_____°___° Ñ„Ñƒ___ºÑ†_¸Ñè-_¿_µÑÅ__Ñ‡___¸Ñ†_°, _º__Ñ‚__Ñ€_°Ñè _·_°_¿ÑƒÑÅ_º_°_µÑ‚ _´Ñ€Ñƒ__ÑƒÑ_ Ñ„Ñƒ___ºÑ†_¸Ñ_ _¸Â _____·__Ñ€_°Ñ‰_°_µÑ‚ _»_¸_±__ Ñ€_µ_·Ñƒ_»ÑŒÑ‚_°Ñ‚ Ñ€_°_±__Ñ‚Ñ‹, _»_¸_±__ _¿Ñ€_¸_·___°_º __Ñˆ_¸_±_º_¸._Ÿ__ÑçÑ‚____Ñƒ __ÑÅÑ‚_°___°___»_¸___°Ñ‚ÑŒ _¿Ñ€____Ñ€_°____Ñƒ ___µ_»ÑŒ_·Ñè, __Ñƒ_¶____ _º_°_º ___¸___¸__Ñƒ__ _´_µ_»_°Ñ‚ÑŒ ___µ_»___º_°_»ÑŒ__Ñ‹__ _¿_µÑ€_µÑ…___´ (_¸ÑÅ_º_»Ñ_Ñ‡_µ___¸_µ _¸_»_¸ `longjmp()`). _ _°ÑÅ_¿Ñ€_µ_´_µ_»_µ___¸_µ _¿_°__ÑèÑ‚_¸ _____¶_µÑ‚ __Ñ‹_¿___»__ÑèÑ‚ÑŒÑÅÑè _¸Â _¸_·Â _¿___»ÑŒ_·_____°Ñ‚_µ_»ÑŒÑÅ_º_¸Ñ… ___°Ñ‚_¸____Ñ‹Ñ… Ñ„Ñƒ___ºÑ†_¸__. _êÂ Ñ‚_°__ ______ÑƒÑ‚ Ñ€_°ÑÅ_¿Ñ€_µ_´_µ_»ÑèÑ‚ÑŒÑÅÑè Ñ€_µÑÅÑƒÑ€ÑÅÑ‹, Ñ‚Ñ€_µ_±ÑƒÑ_Ñ‰_¸_µ _´_°_»ÑŒ___µ__Ñˆ_µ____ __ÑÅ_____±___¶_´_µ___¸Ñè (___°_¿Ñ€_¸___µÑ€, _¿_°__ÑèÑ‚ÑŒ _¸_»_¸ __Ñ‚_ºÑ€Ñ‹Ñ‚Ñ‹_µ Ñ„_°___»Ñ‹)._•ÑÅ_»_¸ _¸ÑÅ_¿___»ÑŒ_·_____°Ñ‚ÑŒ _¸ÑÅ_º_»Ñ_Ñ‡_µ___¸Ñè _¸Â RAII-Ñ€_µÑÅÑƒÑ€ÑÅÑ‹, Ñ‚__Â ___°Â _¿_µÑ€__Ñ‹__ ___·___»Ñè_´ _¿Ñ€___±_»_µ__ ___µÑ‚. __Ñ€_____µ ÑÅ_»ÑƒÑ‡_°Ñè _¸ÑÅ_¿___»ÑŒ_·_____°___¸Ñè Ñ€_°_·__Ñ‹Ñ… DLL ÑÅÂ Ñ€_°_·__Ñ‹___¸ Ñ€_°__Ñ‚_°_____°___¸ _¡_¸++._Ÿ__ÑçÑ‚____Ñƒ _¿__-_¿Ñ€_µ_¶___µ__Ñƒ __ÑÅÑ‚_°Ñ_Ñ‚ÑÅÑè __Ñƒ_¶__Ñ‹ API-Ñ„Ñƒ___ºÑ†_¸_¸ ÑÅ___·_´_°___¸Ñè Ñç_»_µ___µ__Ñ‚____, _º__Ñ‚__Ñ€Ñ‹_µ ______ÑƒÑ‚ _____·__Ñ€_°Ñ‰_°Ñ‚ÑŒ _¿Ñ€_¸_·___°_º ÑƒÑÅ_¿_µÑˆ____ÑÅÑ‚_¸. _êÂ Ñ‚_°_º_¶_µ __Ñƒ_¶____ ÑÅ__Ñ…Ñ€_°___¸Ñ‚ÑŒ `return â€_NO_MEMORY;`. __ÑÅÑ‚_°Ñ‚_¸, _¿Ñ€_¸_·___°_º ÑƒÑÅ_¿_µÑˆ____ÑÅÑ‚_¸ ___µÂ ___±Ñè_·_°Ñ‚_µ_»ÑŒ____ _´___»_¶_µ__ _±Ñ‹Ñ‚ÑŒ _º___´____ _____·__Ñ€_°Ñ‚_°. __Ñƒ___ºÑ†_¸Ñè _____¶_µÑ‚ _____·__Ñ€_°Ñ‰_°Ñ‚ÑŒ `void`, ____ _¿Ñ€_¸ ÑçÑ‚____ __Â API _____¶_µÑ‚ _±Ñ‹Ñ‚ÑŒ _¸Â Ñ„Ñƒ___ºÑ†_¸Ñè ÑÅÂ _¸___µ___µ__ __Ñ€___´_µ `bool alloc_failed()`, _º__Ñ‚__Ñ€_°Ñè _____·__Ñ€_°Ñ‰_°_µÑ‚ `true`, _µÑÅ_»_¸ ___´_¸__ _¸_·Â _¿Ñ€_µ_´Ñ‹_´ÑƒÑ‰_¸Ñ… __Ñ‹_·________ _¿_°__ÑèÑ‚ÑŒ __Ñ‹_´_µ_»_¸Ñ‚ÑŒ ___µÂ ÑÅ______._¢_°_º_¸__ ___±Ñ€_°_·____, _____¶____ __Ñ‹_´_µ_»_¸Ñ‚ÑŒ _´___µ _¿___´_·_°_´_°Ñ‡_¸:* [ ] _ _µ_°_»_¸_·_°Ñ†_¸Ñè API _±_µ_·__Ñ‚_º_°_·________ __Ñ‹_´_µ_»_µ___¸Ñè _¿_°__ÑèÑ‚_¸.* [ ] _“_µ___µÑ€_°Ñ†_¸Ñè _º___´_°, ÑƒÑ‡_¸Ñ‚Ñ‹___°Ñ_Ñ‰_°Ñè _¿__ÑÅ_»_µ_´_____°Ñ‚_µ_»ÑŒ_____µ Ñ€_°ÑÅ_¿Ñ€_µ_´_µ_»_µ___¸_µ Ñƒ_·_»____.",DevOps,_Ÿ__ÑÅÑ‚Ñ€___µ___¸_µ Ñ€_µ_·Ñƒ_»ÑŒÑ‚_°Ñ‚__Ñ‹Ñ… __Ñ‹Ñ€_°_¶_µ___¸__ _º_°_º __Â _ _µÑ„_°_»_µ-05,standard,