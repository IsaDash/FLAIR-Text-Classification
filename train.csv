__label__cloud	<p>I'm looking for a cloud based solution to read the Machine Readable Zone from IDs or Passports to implement in our backend.</p><p>I tried some generic OCR solutions such as: </p><ol><li><strong>Amazon Rekognition</strong></li><li><strong>Google Vision</strong></li><li><strong>Microsoft Computer Vision</strong></li><li><strong>Teserract</strong> 3.0 / 4.0 (experimental)</li></ol><p>None of these provide accurate (sometimes not at all MRZ recognition)</p><p>I also tried some other tools specialized in MRZ OCR:</p><ol><li><strong>BlinkID</strong> from MicroBlink (which is very good but doesn't have a cloud solution)</li><li><strong>Accurascan</strong> (provides cloud solution but less accurate than BlinkID)</li><li><strong>Abbyy</strong> (too slow, 10~ seconds per request)</li></ol><p>Can you recommend me a good cloud solution for MRZ OCR of documents?</p>
__label__DevOps	reason: `callbag-subscribe` not transpiled to es5
__label__cloud	"<p>You can control the pace at which cloud functions are triggered by controlling the triggers themselves. For example, if you have set ""new file creation in a bucket"" as trigger for your cloud function, then by controlling how many new files are created in that bucket you can manage concurrent execution.Such solutions are not perfect though because sometimes the cloud functions fails and get restart automatically (if you've configure your cloud function that way) without you having any control over it. In effect, the number of active instances of cloud functions will be sometimes more than you plan.What AWS is offering is a neat feature though. </p>"
__label__nan	
__label__Customer Experience	The font in the package chooser pop-up should be smaller (as big as the font of the packages, that are show in the search result list.
__label__DevOps	## overviewThe deck calibration CLI instructions are incorrect and should be added to a README so that the information is correct.Also, there are some to-do's listed in the `test_cli.py` to prevent against fragility of the CLI tool. There were some bugs found that would have been caught with some simple tests.
__label__cloud	"<p><a href=""https://cloud.google.com/nodejs/docs/reference/firestore/0.20.x/Firestore#runTransaction"" rel=""nofollow noreferrer""><code>runTransaction</code></a> returns a promise. You need to <code>await</code> it.</p><pre><code> await admin.firestore().runTransaction(...);</code></pre>"
__label__nan	There must be a new contact created on HubSpot.
__label__nan	¡¢¡¢Ûâ
__label__nan	"<blockquote class=""twitter-tweet""><p lang=""fr"" dir=""ltr"" xml:lang=""fr"">Livraison par drone &agrave; venir au Japon via un accord entre Rakuten et <a href=""https://t.co/yx9Y1ghzCw"">https://t.co/yx9Y1ghzCw</a> - La Revue du digital <a href=""https://t.co/KABE7oDZBI"">https://t.co/KABE7oDZBI</a> <a href=""https://twitter.com/hashtag/actu?src=hash&amp;ref_src=twsrc%5Etfw"">#actu</a> <a href=""https://twitter.com/hashtag/drone?src=hash&amp;ref_src=twsrc%5Etfw"">#drone</a></p>&mdash; Les drones (@les_drones) <a href=""https://twitter.com/les_drones/status/1100741892017451010?ref_src=twsrc%5Etfw"">February 27, 2019</a></blockquote><br><br>February 27, 2019 at 01:58PM<br>"
"__label__<p>I am new on artificial intelligence and I am using TensorFlow object detection API to detect a product on images, so It already detecting object but I wanna to get coordinates Xmax,Xmin Ymax and Ymin for each object on the images.</p><p>That is the images with object detected, so in this case was detected 2 object on the image.</p><p>Link:</p><p><a href=""https://i.ibb.co/CVd1xLR/download.png"" rel=""nofollow noreferrer"">https://i.ibb.co/CVd1xLR/download.png</a></p><p>We can see that I got the coordinates of objects but its no clearly, there more than 3 coordinates in the output and I just want to get the amount of coordinates as the number of objects that are in the image. </p><p>This the code which provide the output </p><pre><code>with detection_graph.as_default():with tf.Session(graph=detection_graph) as sess:image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')num_detections = detection_graph.get_tensor_by_name('num_detections:0')print(detection_graph.get_tensor_by_name('detection_boxes:0'))for image_path in TEST_IMAGE_PATHS:boxes = detect_objects(image_path)print(boxes)</code></pre><p>Output </p><pre><code>Tensor(""detection_boxes:0"", dtype=float32)[[[0.16593058 0.06630109 0.8009524 0.5019088 ][0.15757088 0.5376015 0.8869156 0.9394863 ][0.5966009 0.88420665 0.6564093 0.9339011 ]...[0. 0. 0. 0. ][0. 0. 0. 0. ][0. 0. 0. 0. ]]]</code></pre><p>I want to get something like that. only the coordinates of the Bounding Box. We assuming that they are the coordinates of the objects.</p><pre><code>[0.16593058 0.06630109 0.8009524 0.5019088 ][0.15757088 0.5376015 0.8869156 0.9394863 ]</code></pre>"	Artificial Intelligence
__label__DevOps, analytics	For plotly scatter mal plots:I'd wish the package also offered the possibility to use a self defined style.json like when you use Mapbox natively in JS. Using this specification (and using OSM tiles) makes the mapbox_access_token obsolete.https://github.com/plotly/plotly.py/issues/1431
__label__Customer Experience	@steve228uk love the app! Have a request: would it be possible to reduce the minimal window width and height? YT music has a good mobile layout and it would make it a little less large on the desktop. I tried doing it myself by can't get the cocoapods dependencies to resolve properly...
__label__AI	### Export Results as a new summarized PDFYou've made your search in thousands of documents, and you've found pagesabout your search in dozens of them. You need to share the results with yourcolleagues. Do you have to share all documents containing hundreds of pages toshare just dozens of related pages? Not anymore.PDF Search allows you to export the most relevant pages in search results as anew PDF document. So you can share a summary report with your friends with asingle document.
__label__DevOps	Running the tests depends on #78
__label__DevOps	Hello,I have a small phar, built using humbug/box. It's a symfony/console based application, where I wanted to use panther.Trying to execute command with Client::createChromeClient() results in:```sh: 1: exec: phar:///home/wojciechem/myphar-testing/myphar.phar/vendor/symf ony/panther/src/ProcessManager/../../chromedriver-bin/chromedriver_linux64: not found```When I extract the phar contents, the file does exist in expected path.PHP 7.2.15-1+ubuntu16.04.1+deb.sury.org+1 (cli). I have zip extension installed.Am I missing something trivial here? Should I use external chromedriver?
__label__DevOps	DEBUG in the `Interpreter` class was changed to a ThreadLocal, but the BSFEngine implementation in the bsh-bsf-engine sub-project assumes that it is a Boolean.
__label__cloud	"<p>Mainly Virtualization means, running multiple operating systems on a single machine but sharing all the hardware resources. And it helps us to provide the pool of IT resources so that we can share these IT resources in order get benefits in the business.</p><p>For the maintenance of resources in cloud computing environment, virtualization is a necessity as it makes it easier. Virtualization in Cloud Computing increases security as it protects both the integrity of guest virtual machines and cloud components. Cloud Component virtualized machines can also be scaled up or down on demand or can provide reliability. Resource Sharing, high utilization of pooled resources, rapid provisioning are also some of the factors Managed Service Provider VA provides.</p><p><a href=""https://www.esds.co.in/enlight-cloud-hosting"" rel=""nofollow noreferrer"">https://www.esds.co.in/enlight-cloud-hosting</a></p>"
__label__cloud	"<p>Yes, just open a Terminal in Cloud 9 and start your app.</p><p>Then click on ""Preview"" button in Cloud 9 toolbar.</p><p>Detailed step by step instructions, incl a few limitations are detailed here <a href=""https://docs.aws.amazon.com/cloud9/latest/user-guide/app-preview.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/cloud9/latest/user-guide/app-preview.html</a></p>"
__label__cloud	<p>You donÈt save image into firestore but image url. After you save image into storage you have to get the url of that image and then save the url to firestore. Just google these steps or see documentation for more info</p>
__label__Containers	This will parse `Dockerfile` and run `docker create` with all the ports and volumes.
__label__nan	Repo list should be alphabetical order
__label__DevOps	Error when compiling to PDF
__label__nan	https://whitesmith.github.io/qnorr-styles/#objects-css-[data-flitem*=%22justify%22]
__label__cloud	"<p>From <a href=""https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service"" rel=""nofollow noreferrer"">https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service</a>:</p><blockquote><p>When using a Service with spec.type: LoadBalancer, you can specify theIP ranges that are allowed to access the load balancer by usingspec.loadBalancerSourceRanges. This field takes a list of IP CIDRranges, which Kubernetes will use to configure firewall exceptions.This feature is currently supported on Google Compute Engine, GoogleKubernetes Engine, AWS Elastic Kubernetes Service, Azure KubernetesService, and IBM Cloud Kubernetes Service. This field will be ignoredif the cloud provider does not support the feature.</p></blockquote><p>May be your cloud simply does not support it.</p><p>You can use other things that allow blocking by source IP, like nginx or ingress-nginx. In ingress-nginx you just specify list of allowed IPs in annotation <code>ingress.kubernetes.io/whitelist-source-range</code>. </p><p>If you want to go Nginx or other proxy route - don't forget to change Load Balancer Service <code>externalTrafficPolicy</code> to <code>Local</code>. Otherwise you will not see real client IPs.</p>"
__label__nan	[Vssue] Comments for docs
__label__nan	https://smiletolove.github.io/
__label__cloud	<p>You cannot change the lifetime from Google side, as the doc you post said, Google side (Cloud VPN) just negotiates with the lifetime with the on-premise, being the max 36,000 seconds (10 hours) for Phase1.</p><p>In any case, you will need to change this at your on-premise side. </p>
__label__AI	I analyzed all the posts from Stack Overflow / Stack Exchange and extractedmost frequently mentioned Artificial Intelligence & Machine Learning books.Top Mentioned AI & Machine Learning Books on Stack Overflow / Exchange<http://www.aimlbooks.com/>Let me know what you think!
__label__AI	The approach is related to traditional simulation, but with criticaldifferences. A simulation is essentially assumption-driven, Schawinski said.The approach is to say, I think I know what the underlying physical laws arethat give rise to everything that I see in the system.È So I have a recipe forstar formation, I have a recipe for how dark matter behaves, and so on. I putall of my hypotheses in there, and I let the simulation run. And then I ask:Does that look like reality? What heÈs done with generative modeling, hesaid, is in some sense, exactly the opposite of a simulation. We donÈt knowanything; we donÈt want to assume anything. We want the data itself to tell uswhat might be going on.The apparent success of generative modeling in a study like this obviouslydoesnÈt mean that astronomers and graduate students have been made redundant but it appears to represent a shift in the degree to which learning aboutastrophysical objects and processes can be achieved by an artificial systemthat has little more at its electronic fingertips than a vast pool of data.ItÈs not fully automated science  but it demonstrates that weÈre capable ofat least in part building the tools that make the process of scienceautomatic, Schawinski said.Generative modeling is clearly powerful, but whether it truly represents a newapproach to science is open to debate. For [DavidHogg](https://cosmo.nyu.edu/hogg/), a cosmologist at New York University andthe Flatiron Institute (which, like _Quanta_ , is funded by the SimonsFoundation), the technique is impressive but ultimately just a verysophisticated way of extracting patterns from data  which is what astronomershave been doing for centuries. In other words, itÈs an advanced form ofobservation plus analysis. HoggÈs own work, like SchawinskiÈs, leans heavilyon AI; heÈs been using neural networks to [classifystars](https://arxiv.org/pdf/1711.08793.pdf) according to their spectra and to[infer other physical attributes](https://arxiv.org/pdf/1603.03040.pdf) ofstars using data-driven models. But he sees his work, as well as SchawinskiÈs,as tried-and-true science. I donÈt think itÈs a third way, he said recently.I just think we as a community are becoming far more sophisticated about howwe use the data. In particular, we are getting much better at comparing datato data. But in my view, my work is still squarely in the observational mode.## Hardworking AssistantsWhether theyÈre conceptually novel or not, itÈs clear that AI and neuralnetworks have come to play a critical role in contemporary astronomy andphysics research. At the Heidelberg Institute for Theoretical Studies, thephysicist [KaiPolsterer](https://www.iau.org/administration/membership/individual/16830/)heads the astroinformatics group  a team of researchers focused on new, data-centered methods of doing astrophysics. Recently, theyÈve been using amachine-learning algorithm to [extract redshift information from galaxy datasets](https://www.aanda.org/articles/aa/pdf/2018/01/aa31326-17.pdf), apreviously arduous task.Polsterer sees these new AI-based systems as hardworking assistants that cancomb through data for hours on end without getting bored or complaining aboutthe working conditions. These systems can do all the tedious grunt work, hesaid, leaving you to do the cool, interesting science on your own.But theyÈre not perfect. In particular, Polsterer cautions, the algorithms canonly do what theyÈve been trained to do. The system is agnostic regardingthe input. Give it a galaxy, and the software can estimate its redshift andits age  but feed that same system a selfie, or a picture of a rotting fish,and it will output a (very wrong) age for that, too. In the end, oversight bya human scientist remains essential, he said. It comes back to you, theresearcher. YouÈre the one in charge of doing the interpretation.For his part, Nord, at Fermilab, cautions that itÈs crucial that neuralnetworks deliver not only results, but also error bars to go along with them,as every undergraduate is trained to do. In science, if you make a measurementand donÈt report an estimate of the associated error, no one will take theresults seriously, he said.Like many AI researchers, Nord is also concerned about the impenetrability ofresults produced by neural networks; often, a system delivers an answerwithout offering a clear picture of how that result was obtained.Yet not everyone feels that a lack of transparency is necessarily a problem.[Lenka Zdeborov](http://artax.karlin.mff.cuni.cz/~zdebl9am/), a researcher atthe Institute of Theoretical Physics at CEA Saclay in France, points out thathuman intuitions are often equally impenetrable. You look at a photograph andinstantly recognize a cat  but you donÈt know how you know, she said. Yourown brain is in some sense a black box.ItÈs not only astrophysicists and cosmologists who are migrating toward AI-fueled, data-driven science. Quantum physicists like [RogerMelko](https://uwaterloo.ca/physics-astronomy/people-profiles/roger-melko) ofthe Perimeter Institute for Theoretical Physics and the University of Waterlooin Ontario have used neural networks to solve some of the toughest and mostimportant problems in that field, such as [how to represent the mathematicalwave function](https://arxiv.org/pdf/1812.09329.pdf) describing a many-particle system. AI is essential because of what Melko calls the exponentialcurse of dimensionality. That is, the possibilities for the form of a wavefunction grow exponentially with the number of particles in the system itdescribes. The difficulty is similar to trying to work out the best move in agame like chess or Go: You try to peer ahead to the next move, imagining whatyour opponent will play, and then choose the best response, but with eachmove, the number of possibilities proliferates.Of course, AI systems have mastered both of these games  chess, decades ago,and Go in 2016, when an AI system called[AlphaGo](https://www.theguardian.com/technology/2016/mar/15/googles-alphago-seals-4-1-victory-over-grandmaster-lee-sedol) defeated a top human player.They are similarly suited to problems in quantum physics, Melko says.## The Mind of the MachineWhether Schawinski is right in claiming that heÈs found a third way of doingscience, or whether, as Hogg says, itÈs merely traditional observation anddata analysis on steroids, itÈs clear AI is changing the flavor ofscientific discovery, and itÈs certainly accelerating it. How far will the AIrevolution go in science?Occasionally, grand claims are made regarding the achievements of a robo-scientist. A decade ago, an AI robot chemist named Adam investigated thegenome of bakerÈs yeast and worked out which genes are responsible for makingcertain amino acids. (Adam did this by observing strains of yeast that hadcertain genes missing, and comparing the results to the behavior of strainsthat had the genes.) _Wired_ Ès headline read, [Robot Makes ScientificDiscovery All by Itself](https://www.wired.com/2009/04/robotscientist/).More recently, Lee Cronin, a chemist at the University of Glasgow, has beenusing a robot [to randomly mixchemicals](https://www.wired.co.uk/article/robot-chemist-life-on-earth), tosee what sorts of new compounds are formed. Monitoring the reactions in real-time with a mass spectrometer, a nuclear magnetic resonance machine, and aninfrared spectrometer, the system eventually learned to predict whichcombinations would be the most reactive. Even if it doesnÈt lead to furtherdiscoveries, Cronin has said, the robotic system could allow chemists to speedup their research by about 90 percent.Last year, another team of scientists at ETH Zurich used neural networks to[deduce physical laws](https://arxiv.org/abs/1807.10300) from sets of data.Their system, a sort of robo-Kepler, rediscovered the heliocentric model ofthe solar system from records of the position of the sun and Mars in the sky,as seen from Earth, and figured out the law of conservation of momentum byobserving colliding balls. Since physical laws can often be expressed in morethan one way, the researchers wonder if the system might offer new ways perhaps simpler ways  of thinking about known laws.These are all examples of AI kick-starting the process of scientificdiscovery, though in every case, we can debate just how revolutionary the newapproach is. Perhaps most controversial is the question of how muchinformation can be gleaned from data alone  a pressing question in the age ofstupendously large (and growing) piles of it. In _The Book of Why_ (2018), thecomputer scientist Judea Pearl and the science writer Dana Mackenzie assertthat data are profoundly dumb. Questions about causality can never beanswered from data alone, they write. Anytime you see a paper or a studythat analyzes the data in a model-free way, you can be certain that the outputof the study will merely summarize, and perhaps transform, but not interpretthe data. Schawinski sympathizes with PearlÈs position, but he described theidea of working with data alone as a bit of a straw man. HeÈs neverclaimed to deduce cause and effect that way, he said. IÈm merely saying wecan do more with data than we often conventionally do.Another oft-heard argument is that science requires creativity, and that  atleast so far  we have no idea how to program that into a machine. (Simplytrying everything, like CroninÈs robo-chemist, doesnÈt seem especiallycreative.) Coming up with a theory, with reasoning, I think demandscreativity, Polsterer said. Every time you need creativity, you will need ahuman. And where does creativity come from? Polsterer suspects it is relatedto boredom  something that, he says, a machine cannot experience. To becreative, you have to dislike being bored. And I donÈt think a computer willever feel bored. On the other hand, words like creative and inspired haveoften been used to describe programs like Deep Blue and AlphaGo. And thestruggle to describe what goes on inside the mind of a machine is mirroredby the difficulty we have in probing our own thought processes.Schawinski recently left academia for the private sector; he now runs astartup called Modulos which employs a number of ETH scientists and, accordingto its website, works in the eye of the storm of developments in AI andmachine learning. Whatever obstacles may lie between current AI technologyand full-fledged artificial minds, he and other experts feel that machines arepoised to do more and more of the work of human scientists. Whether there is alimit remains to be seen.Will it be possible, in the foreseeable future, to build a machine that candiscover physics or mathematics that the brightest humans alive are not ableto do on their own, using biological hardware? Schawinski wonders. Will thefuture of science eventually necessarily be driven by machines that operate ona level that we can never reach? I donÈt know. ItÈs a good question.
__label__nan	NullPointerException on Routine.toString() when routine is not attached to a Configuration - Merge [#8355]
__label__nan	
__label__nan	There must be a new contact created on HubSpot.
__label__nan	"<p>The problem was in puppeteer. I downgraded from the version 1.13.0 to 1.11.0 and now everything works fine. See the discussion <a href=""https://github.com/GoogleChrome/puppeteer/issues/3944"" rel=""nofollow noreferrer"">here</a></p>"
__label__DevOps	"#### This issue pertains to the following package(s):- [ ] GraphQL Playground - Electron App- [ ] GraphQL Playground HTML- [ ] GraphQL Playground- [ ] GraphQL Playground Express Middleware- [ ] GraphQL Playground Hapi Middleware- [ ] GraphQL Playground Koa Middleware- [ ] GraphQL Playground Lambda Middleware- [X ] GraphQL Playground React#### What OS and OS version are you experiencing the issue(s) on?macOS Mojave#### What version of graphql-playground(-electron/-middleware) are you experiencing the issue(s) on?""graphql-playground-react"": 1.7.20#### What is the expected behavior?returning a valid ApolloLink as specified in createApolloLink should work, but the types are wrong, createApolloLink should return object { link: ApolloLink } not ApolloLink!#### What is the actual behavior?SchemaFetcher.js fetchSchema fails to get ApolloLink, because it looks for it in a object { link: xxx }. #### What steps may we take to reproduce the behavior?my playground setup which produces an error when fetching schema:```js<Playgroundendpoint={graphqlEndpoint || ''}subscriptionEndpoint={subsEndpoint}workspaceName={'Test'}createApolloLink={(session: Session, subscriptionEndpoint?: string): ApolloLink => {const myLink = ApolloLink.from([errorLink, authLink, httpLink]);console.log('link', myLink);myLink;}}/>```from PlaygroundWrapper.d.ts we can see that I'm supposed to return a ApolloLink:```export interface PlaygroundWrapperProps {endpoint?: string;...createApolloLink?: (session: Session, subscriptionEndpoint?: string) => ApolloLink;...}```but this should probably be:```export interface PlaygroundWrapperProps {endpoint?: string;...createApolloLink?: (session: Session, subscriptionEndpoint?: string) => { link: ApolloLink; subscriptionClient?: SubscriptionClient };...}```temporary workaround using any type passing correct type works!:```js<Playgroundendpoint={graphqlEndpoint || ''}subscriptionEndpoint={subsEndpoint}workspaceName={'Test'}createApolloLink={(session: Session, subscriptionEndpoint?: string): any => {const myLink = ApolloLink.from([errorLink, authLink, httpLink]);return { link: myLink }; // this works, but get compile error if I remove any type}}/>```code where it fails to get link:https://github.com/prisma/graphql-playground/blob/a220dc00b7bb0fa3c35f13b8691c66be8ef49a82/packages/graphql-playground-react/src/state/sessions/fetchingSagas.ts_Please provide a gif or image of the issue for a quicker response/fix._"
__label__nan	"<p>If you want to transform your array elements into a new array, you could also use <a href=""https://developer.mozilla.org/de/docs/Web/JavaScript/Reference/Global_Objects/Array/map"" rel=""nofollow noreferrer""><code>Array.map</code></a>.</p><pre><code>allUsers = users.map(user =&gt; user.data());</code></pre>"
__label__nan	OpenRPT Report WriterLink: [Here](https://github.com/xtuple/openrpt)
__label__nan	
__label__nan	### Welcome home!This timeline is where youÈll spend most of your time, getting instant updatesabout what matters to you.### Tweets not working for you?Hover over the profile pic and click the Following button to unfollow anyaccount.### Say a lot with a littleWhen you see a Tweet you love, tap the heart  it lets the person who wrote itknow you shared the love.### Join the conversationAdd your thoughts about any Tweet with a Reply. Find a topic youÈre passionateabout, and jump right in.### Learn the latestGet instant insight into what people are talking about now.### Get more of what you loveFollow more accounts to get instant updates about topics you care about.### Find what's happeningSee the latest conversations about any topic instantly.### Never miss a MomentCatch up instantly on the best stories happening as they unfold.
__label__nan	**What is EOS?**EOS is an open-source distributed blockchain operating system with a focus onbringing decentralized applications to the masses. The vision of EOS is thateveryday users will, in the near future, be able to run dapps from mobiledevices with no specialized knowledge - just as they currently do with appsdownloaded from the App Store.**Quick Links:****Other Links:****Related Subreddit:**
__label__AI	An artificial intelligence (AI) trained on the photos of a dog, crab, and duck(top) would be vulnerable to deception because these photos contain subtlefeatures that could be manipulated. The images on the bottom row donÈt containthese subtle features, and are thus better for training secure AI.Ilyas, Santurkar, Tsipras, Engstrom, Tran, Madry# Scientists help artificial intelligence outsmart hackersBy [Matthew Hutson](/author/matthew-hutson)May. 14, 2019 , 12:45 PM**NEW ORLEANS, LOUISIANA** A hacked message in a streamed song makes Alexasend money to a foreign entity. A self-driving car crashes after a pranksterstrategically places stickers on a stop sign so the car misinterprets it as aspeed limit sign. Fortunately these havenÈt happened yet, but hacks like this,sometimes called [adversarialattacks](https://www.sciencemag.org/news/2018/07/turtle-or-rifle-hackers-easily-fool-ais-seeing-wrong-thing), could become commonplaceunlessartificial intelligence (AI) finds a way to outsmart them. Now, researchershave found a new way to give AI a defensive edge, they reported here last weekat the International Conference on Learning Representations.The work could not only protect the public. It also helps reveal why AI,notoriously difficult to understand, falls victim to such attacks in the firstplace, says Zico Kolter, a computer scientist at Carnegie Mellon University,in Pittsburgh, Pennsylvania, who was not involved in the research. Becausesome AIs are too smart for their own good, spotting patterns in images thathumans canÈt, they are vulnerable to those patterns and need to be trainedwith that in mind, the research suggests.To identify this vulnerability, researchers created a special set of trainingdata: images that look to us like one thing, but look to AI like anotherapicture of a dog, for example, that, on close examination by a computer, hascatlike fur. Then the team mislabeled the picturescalling the dog picture animage of a cat, for exampleand trained an algorithm to learn the labels. Oncethe AI had learned to see dogs with subtle cat features as cats, they testedit by asking it to recognize fresh, unmodified images. Even though the AI hadbeen trained in this odd way, it could correctly identify actual dogs, cats,and so on nearly half the time. In essence, it had learned to match the subtlefeatures with labels, whatever the obvious features.The training experiment suggests AIs use two types of features: obvious, macroones like ears and tails that people recognize, and micro ones that we canonly guess at. It further suggests adversarial attacks arenÈt just confusingan AI with meaningless tweaks to an image. In those tweaks, the AI is smartlyseeing traces of something else. An AI might see a stop sign as a speed limitsign, for example, because something about the stickers actually makes itsubtly resemble a speed limit sign in a way that humans are too oblivious tocomprehend.Some in the AI field suspected this was the case, but itÈs good to have aresearch paper showing it, Kolter says. Bo Li, a computer scientist at theUniversity of Illinois in Champaign who was not involved in the work, saysdistinguishing apparent from hidden features is a useful and good researchdirection, but that there is still a long way to doing so efficiently.So now that researchers have a better idea of why AI makes such mistakes, canthat be used to help them outsmart adversarial attacks? Andrew Ilyas, acomputer scientist at the Massachusetts Institute of Technology (MIT) inCambridge, and one of the paperÈs authors, says engineers could change the waythey train AI. Current methods of securing an algorithm against attacks areslow and difficult. But if you modify the training data to have only human-obvious features, any algorithm trained on it wonÈt recognizeand be fooledbyadditional, perhaps subtler, features.And, indeed, when the team trained an algorithm on images without the subtlefeatures, [their image recognition software was fooled by adversarial attacksonly 50% of the time](https://arxiv.org/abs/1905.02175), the researchersreported at the conference and in a preprint paper posted online last week.That compares with a 95% rate of vulnerability when the AI was trained onimages with both obvious and subtle patterns.Overall, the findings suggest an AIÈs vulnerabilities lie in its trainingdata, not its programming, says Dimitris Tsipras of MIT, a co-author.According to Kolter, One of the things this paper does really nicely is itdrives that point home with very clear exampleslike the demonstration thatapparently mislabeled training data can still make for successfultrainingthat make this connection very visceral.
__label__nan	"<p>Take a look at the <a href=""https://firebase.google.com/support/release-notes/android"" rel=""nofollow noreferrer"">Firebase Android Release Notes</a> for the latests versions available for the different cores of <em>Firebase</em> functionalities. </p><pre><code>implementation 'com.google.firebase:firebase-messaging:17.4.0' </code></pre><p>Try updating it to the latest version available. </p>"
__label__Customer Experience	Add options for aligning text
__label__DevOps	installhttps://github.com/abapGit-tests/DCLSorhttps://github.com/hardyp/AbapToTheFuture03and click uninstall, it gives errorrelated: https://github.com/larshp/abapGit/issues/2464
__label__nan	Moving from godotengine/godot#14553, see details there.
__label__DevOps	"```thread 'main' panicked at 'no ahead behind stats found for = Some(RemoteBranch { remote_branch: ""upstream/master"", remote_branch_name: ""master"", remote_name: ""upstream"" })', src/models.rs:252:17note: Run with `RUST_BACKTRACE=1` for a backtrace.```p-g-p should likely just not print anything in case such a thing happens"
__label__nan	## Paste the link of the GitHub organisation below and submithttps://github.com/opensourcedesign---###### Please subscribe to this thread to get notified when a new repository is created
__label__nan	<ol><li>Comment or cut your function</li><li>Deploy</li><li>Uncomment or paste back the function</li><li>Rename the function</li><li>Deploy</li><li>Rename the function back</li><li>Deploy</li></ol>
__label__AI	"<!-- SC_OFF --><div class=""md""><p>&#x200B;</p> <p>In April of 2019, <a href=""https://Effect.AI"">Effect.AI</a> will move to the EOS Blockchain.</p> <p>&#x200B;</p> <p><a href=""https://Effect.AI"">Effect.AI</a> is currently NEOÈs most used dApp, and they are determined to be the leader of Decentralized Artificial Intelligence development.</p> <p>&#x200B;</p> <p>&#x200B;</p> <p><strong>More Info</strong>: <a href=""https://medium.com/effect-ai/effect-ai-brings-artificial-intelligence-to-eos-main-net-ead7e68e09fa"">https://medium.com/effect-ai/effect-ai-brings-artificial-intelligence-to-eos-main-net-ead7e68e09fa</a></p> <p>&#x200B;</p> <p> </p> <p><strong>EOS News</strong></p> <p>&#x200B;</p> <p>Telegram: <a href=""https://t.me/EOSNEWS_English"">https://t.me/EOSNEWS_English</a></p> <p>Twitter: <a href=""https://twitter.com/EOSNews_Eng"">https://twitter.com/EOSNews_Eng</a></p> <p>Medium: <a href=""https://medium.com/@EOSNews"">https://medium.com/@EOSNews</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=""https://www.reddit.com/user/EOSBlockchainNews""> /u/EOSBlockchainNews </a> <br/> <span><a href=""https://www.reddit.com/r/eos/comments/atcimm/effectai_is_migrating_to_eos/"">[link]</a></span> &#32; <span><a href=""https://www.reddit.com/r/eos/comments/atcimm/effectai_is_migrating_to_eos/"">[comments]</a></span>"
__label__nan	There must be a new contact created on HubSpot.
__label__nan	¼¼Á__ã_©¾Û¤¾÷øö_ÏÛâÛÎ_â¾_ÏÄ_Ï¬__¾ÏÄü_¾ÏäüÛÈ_ÛÎá´_ÏÛ_Îü¨Á__¡ÁÐ¨Ä_¾_ÕÑÏÀâÛâá´_Ï¡±¾÷øÏ¬ÛªÛÜÇÓö¦£ÁáÙö´¼¼_¼_¾÷Ò_Îâ£¼_¡±¾÷ø__¾ÏÄÎÐ¼ _
__label__AI	<https://youtu.be/E3l_aeGjkeI>This Machine Learning tutorial will introduce you to the different areas ofMachine Learning and Artificial Intelligence within the programming languagePython. In this part of the course you will learn about the three differentlearning types (Unsupervised learning, Supervised Learning and ReinforcementLearning)
__label__nan	- ààÝ ¾÷_Á- Airserver _ÀÓ¬¾_¥±_øÈ¦_Îã¦_ »Ñ£¾¥_á_ø
__label__nan	![screen shot 2019-02-27 at 10 48 33](https://user-images.githubusercontent.com/52113/53485145-5dc49c80-3a7d-11e9-8687-17715ba01ac1.png)
__label__AI	Russian banks are stepping up the use of [artificialintelligence](https://www.computerweekly.com/ehandbook/A-Computer-Weekly-buyers-guide-to-automation-and-AI-in-systems-management) (AI) as thetechnologyÈs unprecedented evolution looks set to see it boost theircompetitiveness, but shortages of people with the right skills, as well asinfrastructural issues, are hampering the technologyÈs wider applications.AI is undergoing a period of unprecedented evolution. Over the next fewyears, the technology will progress so far that AI will be employed infinancial institutions just as often as humans, said [SergeyPutyatinsky](https://mkb.ru/en/about/corporate-governance/management-board/putyatinskiy-sergey-evgenevich), deputy chairman of Credit Bank ofMoscow (CBoM)[](https://mkb.ru/en/about/corporate-governance/management-board/putyatinskiy-sergey-evgenevich). Active use of AI technology will be adecisive factor in banksÈ competition in mass segments.According to a study conducted by the [local rating agency ExpertRA](https://www.raexpert.com/), in cooperation with the Centre for FinancialTechnologies, Russian banks most often use AI in credit analysis.Other areas where AI adoption has been increasing, according to the study, aredebt collection and marketing, including creation of individual [offers forcustomers](https://www.computerweekly.com/news/252441507/One-fifth-of-global-banks-think-AI-will-boost-customer-experience).Meanwhile, the study revealed that Russian banks mostly put their hopes on AIin such areas as uncovering fraudulent transactions, debt collection andcredit scoring, while automating call centres by [introducing chatbots](https://searchcrm.techtarget.com/essentialguide/Guide-to-AI-in-customer-service-using-chatbots-and-NLP), using AI in algorithmic trading, [humanresources (HR)management](https://searchhrsoftware.techtarget.com/podcast/Testing-the-intelligence-of-AI-in-HR-applications) and remote customer identification aregenerally considered less promising.According to the studyÈs authors, however, the latter areas may notnecessarily be dismissed by lenders as unsuitable for AI adoption, but it isdifficult to come up with the returns on investment (ROI) from applying thetechnology to those areas. Meanwhile, Russian banks often prefer to adopt thetechnology by small steps.We are pragmatic about the adoption of hypedÈ technologies, saidPutyatinsky. We normally start with smaller-scale pilot projects that allowus to evaluate the potential usefulness of the technology and build up in-house competences.Wherever possible, we use open-source software. We make calculations forevery project to determine if it is financially viable and, based on that, wemake decisions on whether to greenlight it.According to Putyatinsky, CBoMÈs priority areas for AI technology areprocessing full-text documents, making loan decisions, dealing with over-duedebts and financial monitoring.Another major Russian lender, Rosbank, uses AI for processes involving [riskevaluation](https://searchenterpriseai.techtarget.com/feature/AI-in-insurance-forces-big-changes-to-traditional-industry), loan issuing, optimisation of thebranch network, uncovering fraud, communications and interaction withcustomers.We believe that over the next one to two years, AI will also be adopted forthe bankÈs other processes that are not directly linked to interaction withcustomers, said [Dmitry Smirnov, head of RosbankÈs datalab](https://www.linkedin.com/in/smirdm/?locale=de_DE).Accumulating large amounts of data and the arrival of new data sources willfacilitate that. We are actively exploring areas where AI could be potentiallyadopted. These are processes aimed at improving the organisationÈs efficiencyand, from the customerÈs viewpoint, processes that simplify their interactionwith the bank.Meanwhile, PromsvyazbankÈs main area for AIÈs application includes creditdecision-making, uncovering fraud and forming offers for customers.Currently, we are working on broadening the scope of AI application, saidDaniil Tkach, head of the customer relations department at[Promsvyazbank](https://en.wikipedia.org/wiki/Promsvyazbank).In the short term, automated systems will tell us which products would be thebest offer for a customer, what channels will be the most efficient and whatcommunication style will be most amenable to the customer.According to Tkach, the main conditions for wider spread of AI include asufficient degree of automation and manageability, reliable systems for datacollection and a sufficient number of reiterations of processes for learningpurposes. He said this is applicable for just about any banking processes,such as sales, communications, anti-fraud and operations.We could also single out intellectual management systems, in which AIsubstantially helps superiors to understand the quality of work by theiremployees and provides tips to all employees for possibly improving theirwork, Tkach said.Still, Russian banks often see AI as a technology that could help automate newareas rather than replace already existing automation solutions.We are not trying to revamp existing solutions, CBoMÈs Putyatinsky said.Instead, we look at areas that have not yet been automated and startautomating them from scratch with the use of new technology.### __Obstacles to AI adoptionMeanwhile, the process of [adopting AI in the bankingsector](https://www.computerweekly.com/news/252447328/Barclays-appointment-will-step-up-use-of-AI-in-investment-bank) is not always smooth. There are[obstacles in theway](https://www.computerweekly.com/news/252452506/OpenStack-Foundation-will-tackle-infrastructure-barriers-to-enterprise-AI-adoption) of the technologyÈswider spread across the industry.According to the Expert RA study, those obstacles include discrepancies withdata in information systems, but once the issue of data consistency isresolved, finding qualified personnel to process data is set to be a majorchallenge.Industry insiders have already been complaining about difficulties in findingqualified personnel to operate AI-based solutions.The main factors that are impeding the adoption and development of AI areshortages of qualified professionals and problems with the infrastructure ofinformation systems, said Smirnov.Putyatinsky agreed, saying: The acutest issue is training of qualifiedpersonnel.To help resolve this challenge, CBoM has been running an internship programmecalled [IB Universe](https://www.globalbankingandfinance.com/artificial-intelligence-in-banking-industry-conversion-to-genuine-benefits/) for the past12 months. This allows students and recent graduates to acquire practicalexperience in various areas of investment business, added Putyatinsky.According to Putyatinsky, educational programmes of that kind will eventuallyallow banks to train personnel in the working environment, producing a newwave of employees who will already be prepared to deal with new technologies,such as AI and machine learning.Another issue with application of AI is complexity of the technologyÈsalgorithms, PromsvyazbankÈs Tkach said. Contemporary [machine learningalgorithms](https://searchcio.techtarget.com/answer/How-do-machine-learning-algorithms-differ-from-traditional-algorithms) are so complex that humans haveproblems understanding decisions made by AI, he added.Over the next few years, progress with adopting AI systems in RussiaÈs bankingindustry is set to largely depend on investments in regional networks,personnel training and banksÈ ability to attract and retain customers,according to the Expert RA study.The good news is that at this point, a bank doesnÈt need to make enormousinvestment to become one of the [Russian banking industryÈs] AI leaders, saidthe studyÈs authors. But the bad news is that to achieve that, you have toact right now.
__label__DevOps, JAMstack	"The API returns this as a top-level key titled ""`isHighSchool`"""
__label__DevOps	"Hi,I stumble upon this error when searching for products in the Admin.Pages grid.""message"": ""Too few arguments to function Doctrine\\ORM\\EntityRepository::__construct(), 0 passed in /sylius/var/cache/dev/ContainerIcERujV/getBitbagSyliusCmsPlugin_Controller_Action_Admin_ProductSearchService.php on line 15 and exactly 2 expected""Any ideas? Thanks!"
__label__nan	There must be a new contact created on HubSpot.
__label__AI	# Artificial Intelligence Can Now Copy Your Voice## Voice spying, cloning and voice reading on our mental health is just thebeginningVoice technologies arenÈt just becoming the new customer touch point, itÈsbecoming a new way to gather data from global citizens. BaiduÈs AI can [cloneyour voice in seconds](https://medium.com/syncedreview/baidu-ai-can-clone-your-voice-in-seconds-93558a7b984f).Microsoft has a vision for[ conversationalAI](https://blogs.microsoft.com/ai/microsoft-build-future-of-natural-language/) in the future of the operating system. So does Huawei and manyothers. Smart speakers are probably spying on us gathering insights not justto improve their product.Honestly, they can even [tell if your havePTSD](https://futurism.com/artificial-intelligence-detect-ptsd-voice) just byyour voice. Just as AI with facial recognition can read your emotions on yourface. Meanwhile AI is giving us the ability o create fake humans, personaswith apparently human features that donÈt really exist. Getting catfishes onthe internet by a fake human is now a real possibility in the not too distantfuture.* It takes just[ 3.7 seconds of audio to clone a voice](https://motherboard.vice.com/en_us/article/3k7mgn/baidu-deep-voice-software-can-clone-anyones-voice-with-just-37-seconds-of-audio).* TodayÈs intelligent assistants are full of skills and they will get much smarter in the 2020s.AI isnÈt just monetizing the internet for Ad-giants like Google, Facebook andAmazon, Artificial Intelligence is about to create a fake world of even morecomplexity.Smart speakers are going to explode in popularity in China in 2019, withAlibaba, Baidu and Xiaomi leading the way among others. Alibaba isnÈt justlike Amazon, itÈs bigger. As it matures in the cloud and as HuaweiÈs profitsincrease, these two companies will eventually pose a real threat to AIdominance of Google and Microsoft.AI is creating a new world and we donÈt really know the dangers of it, weÈrejust going ahead like children into a world where AI regulation will becomenearly impossible.Now, with advances in artificial intelligence, the world is becoming moreartificial, and you canÈt be sure what you see or hear is real or afabrication of artificial intelligence and machine learning. From incredibleAds of the future to entities we meet online, AI will transform our world tonot just being more immersive, but more confusing, complex and manipulative.The line between convenience and hacking humans (the opposite of enhancing us)is very real. ItÈs so profitable to use AI to gain an edge over other firmsand reach people, the commercial weaponization of AI and our most intimatedata is really inevitable.BaiduÈs research team used voice cloning techniques to develop the AI systemwhich they expect will have noteworthy applications in personalizing human-machine interface.BaiduÈs research arm announced yesterday that its 2017 text-to-speech (TTS)system _Deep Voice_ has learned [how to imitate a personÈsvoice](https://arxiv.org/pdf/1802.06006.pdf) using a mere three seconds ofvoice sample data. ([Synced](https://medium.com/@Synced) is a greatpublication for AI).Huawei has been working on [emotionally intelligentAI](https://www.brecorder.com/2018/04/24/413804/huawei-to-introduce-emotionally-intelligent-artificial-intelligence/) for years. Alexa, Apple andSamsung are in the race for smarter interacts with their personal assistant AIvia earpods with 2019 being a pivotal year for the product from all threeproviders.Like all artificial intelligence algorithms, the more data voice cloning toolssuch as Deep Voice receive to train with the more realistic the results.Meanwhile companies like Spotify are integrating podcasts into how theyrecommend content that will be able to gather data on some of our coreinterests.The smart home invasion of Alexa and Google Home devices is nothing short of atreasure chest of our most intimate data. Information on demand and insightson users that were previously impossible. All thanks to the AI-voice interfacewhich is more immediate and will become ubiquitous in human societies, smartcities and the IoT in the next twenty years.The technique known as voice cloning, could be used to personalize virtualassistants such as AppleÈs Siri, Google Assistant, Amazon Alexa; and BaiduÈsMandarin virtual assistant platform DuerOS, which supports more than 50million devices in China with human-machine conversational interfaces.The frontiers of human interaction with AIs are broad and deep with incredibleimplications for customer relationships. The world we are building of AIs willbe incredible and potentially very transparent with regards to our data.Google unveiled[ Tacotron2](https://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html), a text-to-speech system that leverages the companyÈs deep neuralnetwork and speech generationmethod[WaveNet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/). WaveNet analyzes a visual representation of audio called aspectrogram to generate audio. It is used to generate the voice for GoogleAssistant.ItÈs becoming impossible to tell the difference between an AI and a human andthatÈs incredibly problematic in a world where cybersecurity threats are onlyincreasing. The internet of things aspect of connectivity in the 4thindustrial revolution comes at a cost for privacy, censorship, dataharvesting, fraud, identity theft and consumer manipulation of ever morepersonalized digital advertisements.
__label__DevOps	This command from the [lesson](https://datacarpentry.org/wrangling-genomics/03-trimming/index.html) doesn't work:```cp ~/miniconda3/pkgs/trimmomatic-0.38-0/share/trimmomatic-0.38-0/adapters/NexteraPE-PE.fa .```because of how the things are installed. This works instead:```cp /opt/conda/pkgs/trimmomatic-0.38-0/share/trimmomatic-0.38-0/adapters/NexteraPE-PE.fa .```
__label__Frontend tools	Feedback from Peter Durham:HTMLPage Has a crawlerDepth property in the schema, is this describing the depth in the crawl at which the current page was found? In a crawl scenario, would we expect results in a single AXRL instance or spread across multiple instances?
__label__nan	No petitions, surveys, or crowdfunding
__label__nan	There must be a new contact created on HubSpot.
__label__DevOps	"All events end up in bad when we enable the application context (`builder?.setApplicationContext(true)`). When we decode the line, we find that the context is sent like this:```{""schema"": ""iglu:com.snowplowanalytics.mobile\/application\/jsonschema\/1-0-0"",""data"": {""118"": ""build"",""2.6.1"": ""version""}}```As far as we know, there is nothing in our specific implementation that causes this to happenÛÒsuggesting it's a bug in the SDK. If it is indeed, it's severe as it will cause all events to fail validation.We suspect it's related to these 2 lines in the SDK: https://github.com/snowplow/snowplow-objc-tracker/blob/master/Snowplow/SPTracker.m#L212-L213"
__label__nan	
__label__DevOps, Customer Experience	_Broken up for implementation from #1269______In order to build out the transition to the CMS design system, add the pieces needed to implement the systemÛªs new components and styling for the State Medicaid Director and Medicaid Office Address section: https://hitech-apd.app.cloud.gov/apd#apd-state-profile-office### This task is done when...[ ] The section is not contained in an accordion- [ ] The headline typography is stylistically and semantically correct- [ ] The [Form label component](https://design.cms.gov/components/form-label/) has been added and used for each field- [ ] The [Text field component](https://design.cms.gov/components/text-field/) has been added and used for each field- [ ] The [Select component](https://design.cms.gov/components/select/) has been added and used for the `State` field- [ ] Overall layout and interaction matches the [InVision prototype](https://gsa.invisionapp.com/share/BJQNX2KXQVW#/348460956_State_Profile_Contact)![screen shot 2019-02-27 at 12 01 24 am](https://user-images.githubusercontent.com/11636908/53468246-67231a00-3a27-11e9-87be-21daff5d7f64.png)
__label__cloud	<p>Google Cloud Run fits into your Serverless layer but as a container. The container infrastructure is managed for you. </p><p>Cloud Functions are limited in respect to the libraries, languages, and runtimes supported.</p><p>Cloud Run removes those limitations. You can use any language, combination of libraries and runtime that supports running within a container. </p><p>One limitation is that there is only one internal port <code>$PORT</code> which defaults to 8080 today. Externally both HTTP and HTTPS are supported. Both HTTP and HTTPS map to <code>$PORT</code>.</p><p>One big plus is that Cloud Run supports custom DNS names and custom SSL certificates. You can host your website on Cloud Run. As an experiment, I set up WordPress and Cloud SQL on Cloud Run and assigned it a DNS domain name with an SSL certificate.</p>
__label__AI	"# Title:Integrating Artificial Intelligence into Weapon Systems(Submitted on 10 May 2019)> Abstract: The integration of Artificial Intelligence (AI) into weaponsystems is one of the most consequential tactical and strategic decisions inthe history of warfare. Current AI development is a remarkable combination ofaccelerating capability, hidden decision mechanisms, and decreasing costs.Implementation of these systems is in its infancy and exists on a spectrumfrom resilient and flexible to simplistic and brittle. Resilient systemsshould be able to effectively handle the complexities of a high-dimensionalbattlespace. Simplistic AI implementations could be manipulated by anadversarial AI that identifies and exploits their weaknesses. > In this paper, we present a framework for understanding the development ofdynamic AI/ML systems that interactively and continuously adapt to theiruser's needs. We explore the implications of increasingly capable AI in thekill chain and how this will lead inevitably to a fully automated, always onsystem, barring regulation by treaty. We examine the potential of totalintegration of cyber and physical security and how this likelihood must informthe development of AI-enabled systems with respect to the ""fog of war"", humanmorals, and ethics.## Submission historyFrom: Aaron Massey [[view email](/show-email/69a90b83/1905.03899)]**[v1]**Fri, 10 May 2019 00:38:35 UTC (3,958 KB)"
__label__DevOps	# Bug reportIf you've installed MenuCRUD as a package using our docs, a ```composer update``` , ```composer require``` or ```composer self-update``` might trigger the following warning:```Deprecation warning: require.backpack/MenuCRUD is invalid, it should not contain uppercase characters. Please use backpack/menucrud instead. Make sure you fix this as Composer 2.0 will error.```
__label__cloud	Could you share the code of your Cloud Function pls?
__label__nan	<p>Try by changing gradle version it worked me before, I too faced same issue few days back</p><pre><code>classpath 'com.android.tools.build:gradle:3.2.1'</code></pre>
__label__AI	"by Michael Liedtke![artificialintelligence](https://3c1703fe8d.site.internapcdn.net/newman/csz/news/800/2018/26-artificialin.jpg)Credit: CC0 Public DomainComputers have become so smart during the past 20 years that people don'tthink twice about chatting with digital assistants like Alexa and Siri orseeing their friends automatically tagged in Facebook pictures.But making those quantum leaps from [sciencefiction](https://techxplore.com/tags/science+fiction/) to reality requiredhard work from computer scientists like Yoshua Bengio, Geoffrey Hinton andYann LeCun. The trio tapped into their own brainpower to make it possible formachines to learn like humans, a breakthrough now commonly known as""[artificialintelligence](https://techxplore.com/tags/artificial+intelligence/),"" or AI.Their insights and persistence were rewarded Wednesday with the Turing Award,an honor that has become known as [technologyindustry](https://techxplore.com/tags/technology+industry/)'s version of theNobel Prize. It comes with a $1 million prize funded by Google, a companywhere AI has become part of its DNA.The award marks the latest recognition of the instrumental role thatartificial intelligence will likely play in redefining the relationshipbetween humanity and technology in the decades ahead.""Artificial intelligence is now one of the fastest-growing areas in all ofscience and one of the most talked-about topics in society,"" said CherriPancake, president of the Association for Computing Machinery, the groupbehind the Turing Award.Although they have known each other for than 30 years, Bengio, Hinton andLeCun have mostly worked separately on technology known as neural networks.These are the electronic engines that power tasks such as facial and speechrecognition, areas where computers have made enormous strides over the pastdecade. Such neural networks also are a critical component of robotic systemsthat are automating a wide range of other human activity, including driving.Their belief in the power of [neuralnetworks](https://techxplore.com/tags/neural+networks/) was once mocked bytheir peers, Hinton said. No more. He now works at Google as a vice presidentand senior fellow while LeCun is chief AI scientist at Facebook. Bengioremains immersed in academia as a University of Montreal professor in additionto serving as scientific director at the Artificial Intelligence Institute inQuebec.""For a long time, people thought what the three of us were doing wasnonsense,"" Hinton said in an interview with The Associated Press. ""Theythought we were very misguided and what we were doing was a very surprisingthing for apparently intelligent people to waste their time on. My message toyoung researchers is, don't be put off if everyone tells you what are doing issilly.""Now, some people are worried that the results of the researchers' effortsmight spiral out of control.While the AI revolution is raising hopes that computers will make mostpeople's lives more convenient and enjoyable, it's also stoking fears thathumanity eventually will be living at the mercy of machines.Bengio, Hinton and LeCun share some of those concernsespecially the doomsdayscenarios that envision AI technology developed into weapons systems that wipeout humanity.But they are far more optimistic about the other prospects of AIempoweringcomputers to deliver more accurate warnings about floods and earthquakes, forinstance, or detecting health risks, such as cancer and heart attacks, farearlier than human doctors.""One thing is very clear, the techniques that we developed can be used for anenormous amount of good affecting hundreds of millions of people,"" Hintonsaid.* * ** * *Î© 2019 The Associated Press. All rights reserved.**Citation** : Artificial intelligence pioneers win tech's 'Nobel Prize'(2019, March 27) retrieved 27 March 2019 fromhttps://techxplore.com/news/2019-03-artificial-intelligence-tech-nobel-prize.htmlThis document is subject to copyright. Apart from any fair dealing for thepurpose of private study or research, no part may be reproduced without thewritten permission. The content is provided for information purposes only."
__label__nan	All the objects and parts should be written according to https://www.neos.io/blog/neos-best-practices-1-0.html
__label__DevOps	The README should reflect the nature of this project because it is the first message people will see.
__label__nan	https://zrbabbler.hatenablog.com/entry/2019/02/27/203738 ¨ÛÎupnmlminr-hâÕ_ä¾ÝÑâö ¬ªâÜ¬â¢ÄÂÛâÕÙ_Ù_»_âÒ ¨¤ªÎ_Î> japanese-otfÄÔÄÄâ±Ä_âü¨upTeXÓ¬¨Ä¥â©Ä_Äö¨üÛ_¤ââÜupnmlminr-h¨ü_¼ÇâÕ»À_Ùã¬¾Û£_Ûvf2zvpâÕ_À_ ¬Ñ_ââÛâ¬Ä©Ä_Îà¼__ä¾Ýø_±¾¥ÑÑ_Ñ_ Ûâ¬ââ__ªÛâ¢¼ÜÇ_Î```$ jfmutil vf2zvp --uptool upnmlminr-hjfmutil: CHARWD value mismatch: code 00B7```.tfm¬.vfÇ¬÷Î_¥âÎÙÛ_Î£Ùã¥£_ãâÜ¨ø__ø Çø¾_£Ñ»ãâö ¤ªÛâüÛÀÏ> ¨Ù_Ý¨ÜÓ¬¬Ñ_øÛ©¨DVIâ_â¤â¢ââÛÎ.vf¨¾Ð_¨¾Ðà_Ñ_É¨¾ÄÉ ±øãÁ_ÐªâÜÛ¬ã Ü¥_ÏâÕªâÜÛâ»¨¤upnmlminr-høÛÎ¨Ù_ÝÇøÁâÄâÒ¬_ÀöâÜÛ¨¤ââÜÛâ¬ââ__ªÎ_ÎÀµ¨Ùâ»À__À_ªÛâ
__label__DevOps	Is the conversion function not terminating? Is that what is causing the coroutine event loop not to finish? Should there be an `await` on the Popen handle?
__label__cloud	"<p>The <code>runTransaction</code> method returns a <code>Promise</code>.And as the <a href=""https://firebase.googleblog.com/2017/06/keep-your-promises-when-using-cloud.html"" rel=""nofollow noreferrer"">this blog post</a> says: You have to return that <code>Promise</code>.</p><p>And these is important:</p><blockquote><p>...if you want a function to stay alive during async work, you can do this by returning a promise from the function (except for HTTP/S triggers, which require a response sent to the client).</p></blockquote><p>Or in other words: If you don't return that <code>Promise</code>, your function could finish without completing the <code>transaction</code>.</p><pre><code>import * as functions from 'firebase-functions';import * as admin from 'firebase-admin';admin.initializeApp();exports.addShard = functions.firestore.document(`likes/{docID}`).onCreate(async (snap, context) =&gt; {const postID: string = snap.data().postID;const randNum: number = (Math.floor(Math.random()*3+1)); const postRef = admin.firestore().doc(`post/${postID}/count_shards/${randNum}`);return admin.firestore().runTransaction(async transaction =&gt; {const postShard = (await transaction.get(postRef)).data();postShard.count += 1;return transaction.update(postRef, postShard);});});</code></pre>"
__label__nan	## Paste the link of the GitHub organisation below and submithttps://github.com/socialblade---###### Please subscribe to this thread to get notified when a new repository is created
__label__DevOps	Are you able to tag a new stable release? Or a `0.x` if not willing to commit to `1.x`+?
__label__Customer Experience	Can the regions be ordered in the filter in the same order as in the charts? (north to south)
__label__DevOps	componentWillReceiveProps È´üÜ¨Û¤¡receiveProps¾ÏÛÀÔÏ¬á´_Ïü_àö¡¼ üÛÁÎÈ£ ü__ãÑ¨¢÷_ÎÏ¬receiveProps¾Ð_¾_¥ ÉÄ¬¡ÄÓ¬this.a_ã¾Ñ¦Ûªà¼_¡¾¥¡¾¨ü¢_±Ñ¨¢÷_ÎÉá_ÒÈ£ _âüÜ__```jsconstructor(props) {super(props);this.key = null;} componentWillReceiveProps(nextProps) {if (nextProps.status !== this.props.status) {this.doSomething(nextProps.status);}}eventListener(key) { // ¾Ùü»¼ÜÈ¦¤_Ôthis.key =key;this.props.updateStatus(2); // ¡ÄÓ¬redux¾Ð_¾_¥_ÎÀ¨¾Ó_props É¨_};```eventLister¤_Ô--ÛÜreceiveProps--ÛÜÏ¬doSomething¾Ð_¾_¥àÎ¡ÄÓ¬ this.keyÛâ**´à¾Û»_ã¾÷ø_ÎÂÂüÛ¾ÂÁ¡ÄÓ¬doSomethingøÈ´¨ÀÑ¨ö¡this.key ÂÂ¼Î¾ÂÁ¡ÄÓ¬«__ÀÓÝ_null;**okü¼¼ ¤£ _Ñ¨¢÷_Î¾öÔÈÂÉöÏÜüÜ¨÷¾Ð_¾Ðà¾Á£ü_ø_receiveProps¾Ð_¾_¥_ã¤£à_```jsUNSAFE_componentWillReceiveProps()UNSAFE_componentWillReceiveProps(nextProps)NoteThis lifecycle was previously named componentWillReceiveProps. That name will continue to work until version 17. Use the rename-unsafe-lifecycles codemod to automatically update your components.Note:Using this lifecycle method often leads to bugs and inconsistenciesIf you need to perform a side effect (for example, data fetching or an animation) in response to a change in props, use componentDidUpdate lifecycle instead.If you used componentWillReceiveProps for re-computing some data only when a prop changes, use a memoization helper instead.If you used componentWillReceiveProps to ÛÏresetÛ some state when a prop changes, consider either making a component fully controlled or fully uncontrolled with a key instead.For other use cases, follow the recommendations in this blog post about derived state.UNSAFE_componentWillReceiveProps() is invoked before a mounted component receives new props. If you need to update the state in response to prop changes (for example, to reset it), you may compare this.props and nextProps and perform state transitions using this.setState() in this method.Note that if a parent component causes your component to re-render, this method will be called even if props have not changed. Make sure to compare the current and next values if you only want to handle changes.React doesnÛªt call UNSAFE_componentWillReceiveProps() with initial props during mounting. It only calls this method if some of componentÛªs props may update. Calling this.setState() generally doesnÛªt trigger UNSAFE_componentWillReceiveProps().GoogleÀÈøÔüÛüÜÛâÛâ¾_¬¾ã___ÀÓ¬¾__ÓÙÔ_Ô¬¾ÏÙ¾Ð_¾_¥Û_üü__ø_à«ÓªøøÕÎüüÛà«_â¾_Ï¾â¬ÏÛ_¾ä¤ÁÎäø_ÏÓ¬_ö_Ü_â_Î¾¥¡¾¨¾Ð¾öÐ_¬ÓÈ_äÈ´Ò¼Ópropsü__ã¾Ý«¾Ó__Îøá¾Ó_Ó¬componentDidUpdateÓÙÔ_Ô¬¾ÏÙÛâ_â¾_Ï¾â¬ÈÉÏ¬prop¾Ý«¾Ó_¾Ñ¦_ÀÓ¬componentWillReceivePropsà¾Ð¡¨Á¨Ñ¾Ù¼Ý¾¥¡¾¨_Îøá_ÀÓ¬memoization helperÛâ_â¾_Ï¾â¬Ï¬prop¾Ý«¾Ó_¾Ñ¦_ÀÓ¬componentWillReceivePropsÛÏà_¨Û¾Ù¼Ý_¦¾Û_ÎøáÛÄªÔ_ÀÓ¬ÈãÈ¦¨ÎÉ¬¾_¤ö¦ÈãÈ¦¾öÐ¨ÎÉ¬üÑ¾_¤ö¦Ûâø_¼_É¦ÈÐÓ¬_Ü_Îøáµ_»¾___¨¢¾ÐàÇ ü_¾ÏäÉ_¾«_ÓÙ_¦¾Û_ãÈ¼¨¨ÛâÏ¬¨ä£É_ãÈãÈ¦¾_´¾Ó¦¾Ð¡_ãprops_Üä¡ÄÓ¬UNSAFE_componentWillReceiveProps_ö_äÛâ_â¾_Ï¾â¬ÏÛ_¾Ý«¾Ð¡_¦¾ÛÈ´Ò¼Óprop¾Ý«¾Ó__ö_Ü_â_Îà_¨¨Ä_ä_Î¾â¬øÈ´¾øÓ_Äthis.propsÕÎnextProps_¦_ÀÓ¬¾__¾Ð_¾_¥ü__ãthis.setState_ö_ä¾ä¤ÁÎ_¦¾Û_Â¾¢Ûâøá¾_¬¾ã_Î_â¾_Ïö¦ÈãÈ¦ø_à«ÈãÈ¦à¾Ð¡¾ü_¾ÙÒ_Î__Àprops¾_Á¾Ïä¾Ý«¾Ó__Î_Ù__¡ÄÓ¬¾__¾Ð_¾_¥Ûâ_â¾_Ï¾â¬»¾Ä__ã ¾Ý«¾Ó__Îøá_ÁÀÉ¾øÓ_Ä_ÒäÛ_ÕÎüÜüÛü»Û_ÛâÏ¬¨ä£ÉÀà¬Üü__ÎReactü___ÀÓ¬ö¤ÜÒÉá¡ÄÓ¬UNSAFE_componentWillReceiveProps_ö_äÛâ_â¾_Ï¾Ù¼ÝÈãÈ¦_ãÒÉáøÄ___¾Ý«¾Ð¡_Î¨Ä»__¡ÄÓ¬¾__¾Ð_¾_¥Ûâ¡ÄÓ¬this.setState_ö_äÛ_üüü__¤_ÔUNSAFE_componentWillReceiveProps_ö_äÛâ```¾Ðà¾Á£ü_ø«_Î_ÀÓ¬¾__ÓÙÔ_Ô¬¾ÏÙ¾Ð_¾_¥Û_üü__ø_à«ÓªøøÕÎüüÛà«_Îâ£ö¡¼¥¾÷øÈÛ_ö_ÙÝ ø_à«_ãÓªøøÕÎüüÛà«Ô¢_Î¾öÔÈÂ¾´ÏÜüÜreceiveProps¾÷ø¾Û__ö¨__¡_ã```js```
__label__nan	https://github.com/inikulin/parse5
__label__DevOps	"I was able to resolve Blockhound with version ""1.0.0.BUILD-SNAPSHOT"" (the README says ""v1.0.0.BUILD-SNAPSHOT"" via an embedded image)."
__label__Customer Experience	The mailchimp widget included in this theme is:- dependent upon INN's mailchimp tools library- dependent upon an external plugin- therefore, out of dateAnd it should be replaced with a different solution when possible.
__label__cloud	"<p>I use serverless framework to manage my cloud functions. Some of them are of HTTP type. Recently, all the HTTP functions started to fail with 403 error. No matter if you enter a URL in a browser or trigger it with the cloud scheduler. The only place where it works is the <em>testing</em> tab of the function in the cloud console, when you click the ""<em>Test the function</em>"" button.</p>"
__label__AI	The age of artificial intelligence (AI) has arrived, and is transformingeverything from healthcare to transportation to manufacturing.America has long been the global leader in this new era of AI, and is poisedto maintain this leadership going forward. Realizing the full potential of AIfor the Nation requires the combined efforts of industry, academia, andgovernment. The Administration has been active in developing policies andimplementing strategies that accelerate AI innovation in the U.S. for thebenefit of the American people. These activities align with four main pillarsof emphasis: AI for American Innovation, AI for American Industry, AI for theAmerican Worker, and AI with American Values. This AI.gov website provides aportal for exploring these activities in more depth, and serves as a resourcefor those who want to learn more about how to take full advantage of theopportunities of AI.
__label__nan	There must be a new contact created on HubSpot.
__label__AI	Hi! I've created a Rock-Paper-Scissors game that works with artificialintelligence (AI). The AI can see and detect your hand gestures by front-facing camera. Also it can learn your playing strategy in a smart way. Themore you play, It gets harder to win!This app uses TensorFlow and deep learning technologies in order to detect thehand gestures. Sometimes the gestures may not be properly detected, but thiswill improve in future versions. You can help me in this process by takingpictures of your hand in different positions and sending them as a zip file to[rpsapp@outlook.com](mailto:rpsapp@outlook.com) .Please Note:* To get best results in hand gestures detection, put your device on a flat and steady surface.* In order for the app to work properly, your device should have decent camera and hardware to run relatively heavy calculations.I've been working on developing this app for a year, so any feedback from youwill be a pleasure for me :)You can get the app on Google Play:<https://play.google.com/store/apps/details?id=cc.ramtin.rps>And you can read more about it on XDA: <https://www.xda-developers.com/play-rock-paper-scissors-hand-gestures-against-ai-bot/>
__label__AI	A new research that can use artificial intelligence (AI) to predict lungcancer risks has been reported by GoogleÈs Alphabet team. The World HealthOrganization says, More than 1.7 million global deaths per year result in lungcancer even more than the combined number of cases of breast, prostate andcolorectal cancer.
__label__DevOps, Customer Experience	To meet AA WCAG 2.1 compliance, Tooltips should be persistent on hover (i.e. when you move the mouse from the tooltip toggle to the tooltip contents.The WCAG item with more details about this: [WCAG 1.4.13](https://www.w3.org/WAI/WCAG21/quickref/#content-on-hover-or-focus)
__label__AI	"<!-- SC_OFF --><div class=""md""><p>Here&#39;s my conversation with Greg Brockman, Co-Founder and CTO of OpenAI, on the Artificial Intelligence podcast.</p> <p>Video: <a href=""https://www.youtube.com/watch?v=bIrEM2FbOLU"">https://www.youtube.com/watch?v=bIrEM2FbOLU</a></p> <p>Audio: <a href=""https://lexfridman.com/greg-brockman"">https://lexfridman.com/greg-brockman</a></p> <p>There was a previous post where I asked for <a href=""https://www.reddit.com/r/MachineLearning/comments/b1tucu/d_questions_for_openai/"">Questions for OpenAI</a> many of which were asked in this podcast.</p> <p>&#x200B;</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=""https://www.reddit.com/user/UltraMarathonMan""> /u/UltraMarathonMan </a> <br/> <span><a href=""https://www.reddit.com/r/MachineLearning/comments/b903be/d_greg_brockman_openai_and_agi_mit_artificial/"">[link]</a></span> &#32; <span><a href=""https://www.reddit.com/r/MachineLearning/comments/b903be/d_greg_brockman_openai_and_agi_mit_artificial/"">[comments]</a></span>"
__label__DevOps, Machine Learning	"#### Description<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->#### Steps/Code to Reproduce```In [1]: import imblearn In [2]: from imblearn.pipeline import make_pipeline, Pipeline In [3]: from imblearn.under_sampling import RandomUnderSampler In [4]: rus = RandomUnderSampler() In [5]: p = make_pipeline(rus, None) In [6]: from sklearn.datasets import make_blobs In [7]: X, y = make_blobs() In [8]: p.fit_resample(X, y) ---------------------------------------------------------------------------AttributeError Traceback (most recent call last)<ipython-input-8-cfa4a4b54c83> in <module>----> 1 p.fit_resample(X, y)~/repos/scikit-learn/sklearn/utils/metaestimators.py in __get__(self, obj, type)108 continue109 else:--> 110 getattr(delegate, self.attribute_name)111 break112 else:AttributeError: 'str' object has no attribute 'fit_resample'```#### Expected ResultsNo error#### Actual ResultsError#### Versions<!--Please run the following snippet and paste the output below.import platform; print(platform.platform())import sys; print(""Python"", sys.version)import numpy; print(""NumPy"", numpy.__version__)import scipy; print(""SciPy"", scipy.__version__)import sklearn; print(""Scikit-Learn"", sklearn.__version__)import imblearn; print(""Imbalanced-Learn"", imblearn.__version__)-->```Linux-4.19.0-2-amd64-x86_64-with-debian-buster-sidPython 3.7.1 (default, Dec 14 2018, 19:28:38) [GCC 7.3.0]NumPy 1.15.4SciPy 1.1.0Scikit-Learn 0.21.dev0Imbalanced-Learn 0.4.3```<!-- Thanks for contributing! -->"
__label__cloud	can&#39;t you use a cloud image for your requirement ?
__label__Analytics	Produce a spreadsheet with all by all attribute comparison between HCA and ArrayExpress metadata.**Description**The metadata team needs to understand how attributes in our schema align with other resources. Initially we will map to ArrayExpress using work already done by Robert Petryszak.If possible we could extend this analysis to include mapping to the following:- SCEA/ArrayExpress- NeMo- GEO/SRA- CIRM**Acceptance Criteria**- [ ] Make a table of attributes and map usage across schemas.- [ ] Categories attributes based on MINSEQE definitions.- [ ] Assessment of how long it will take to include the other archives/projects.- [ ] Add frequency of use information covered in ticket https://github.com/HumanCellAtlas/metadata-schema/issues/847
__label__nan	"<blockquote class=""twitter-tweet""><p lang=""ro"" dir=""ltr"" xml:lang=""ro"">KUB-UAV : le nouveau drone de Kalachnikov - IT Social <a href=""https://t.co/fobO4JldFV"">https://t.co/fobO4JldFV</a> <a href=""https://twitter.com/hashtag/actu?src=hash&amp;ref_src=twsrc%5Etfw"">#actu</a> <a href=""https://twitter.com/hashtag/drone?src=hash&amp;ref_src=twsrc%5Etfw"">#drone</a></p>&mdash; Les drones (@les_drones) <a href=""https://twitter.com/les_drones/status/1100710454643617794?ref_src=twsrc%5Etfw"">February 27, 2019</a></blockquote><br><br>February 27, 2019 at 11:53AM<br>"
__label__nan	
__label__DevOps	Using the component with the [Quasar framework](https://quasar-framework.org/) I have the following error below.**`Failed to mount component: template or render function not defined.`**![captura de tela 2019-02-27 as 10 37 24](https://user-images.githubusercontent.com/3280420/53494267-18539f80-3a7c-11e9-9978-ddde9a96a341.png)Created plugin quasar > > import VueAnimateNumber from 'vue-animate-number'> > export default ({ app, router, Vue }) => {> Vue.use(VueAnimateNumber)> }> and add in quasar.conf.js`plugins: [''vue-animate-number'']`
__label__AI	Table of Contents IntroductionCreating variablesInitializationInitializingSpecific VariablesGolobal variable initializationInitialization of a variableusing other existing variablesRunning the sessionSummary Introduction Definingvariables is necessary because of the hold the parameter. Henceforth,exploring TensorFlow variables is necessary. Without having parameters,training, updating, saving,[ Readmore_](https://machinelearningmindset.com/exploring-tensorflow-variables-initialization/)
__label__nan	"<p>Do you tried like this ?</p><pre><code> @JvmField@PropertyName(""championship-name"")var championshipName: String = """"@PropertyName(""championship-name"")get() = field@PropertyName(""championship-name"")set(value) { field = value }</code></pre>"
__label__nan	no comments yetBe the first to share what you think!
__label__cloud	"<p>The two best ways to accomplish this goal would be by either using <a href=""https://cloud.google.com/functions/"" rel=""nofollow noreferrer"">Cloud Functions</a> or by using <a href=""https://cloud.google.com/dataflow"" rel=""nofollow noreferrer"">Cloud Dataflow</a>. For Cloud Functions, you would set up a trigger on the Pub/Sub topic and then in your code write to BigQuery. It would look similar to the <a href=""https://cloud.google.com/solutions/streaming-data-from-cloud-storage-into-bigquery-using-cloud-functions"" rel=""nofollow noreferrer"">tutorial on streaming from Cloud Storage to BigQuery</a>, except the input would be Pub/Sub messages. For Dataflow, you could use one of the <a href=""https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#cloudpubsubsubscriptiontobigquery"" rel=""nofollow noreferrer"">Google-provided, open-source templates to write Pub/Sub messages to BigQuery</a>.</p><p>Cloud Dataflow would probably be better suited if your throughput is high (thousands of messages per second) and consistent. If you have low or infrequent throughput, Cloud Functions would likely be a better fit. Either of these solutions would run constantly and write the messages to BigQuery when available.</p>"
__label__nan	## _Çø¾_¦¾_ã- [÷ÀàÎÛ__Çø¾_¦¾_ãüö¾_Û¾ÏøÝ_¡±ÛÜ](https://github.com/xingshaocheng/architect-awesome)
__label__nan	**Rating is available when the video has been rented.**This feature is not available right now. Please try again later.
__label__cloud	<p>If you do not have a name set for some-service, and it's a 3rd party service, I think the better approach would be to call it via RestTemplate or something. </p><p>Feign client needs to have the service name configured and known, for it to call that particular service in the network using service discovery. </p>
__label__nan	Add username/password parameters to svn config
__label__DevOps, Frontend tools	### Basic requirement- [ ] Cypress- [ ] Typescript
__label__DevOps	Project description on EL Showcase states that:> 100% unit-testable code _ÙÓ´There is a shared build scheme that should run tests, but I didn't find any tests. Moreover, the unit tests target referred by the scheme is missing _ÙÔÛ
__label__cloud	"<p>It is a good practice to externalise any kind of sensitive information.</p><p>If I were you, I would use Environment Variables. This <a href=""https://docs.aws.amazon.com/lambda/latest/dg/eventsources.html#eventsources-sqs"" rel=""nofollow noreferrer"">answer</a> explains how to fetch Env Variables in Java.</p><p>If you are running (or plan to run) on the Cloud, you can take a look into your provider's way to store parameters. AWS, for example, uses <a href=""https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html"" rel=""nofollow noreferrer"">Systems Manager Parameter Store</a></p><p>Other options would be to read a file during runtime, look up in a database, etc. You get the idea.</p>"
__label__AI	U.S. technology giant [Microsoft has teamed up with a Chinese militaryuniversity](https://www.ft.com/content/9378e7ee-5ae6-11e9-9dde-7aedca0a081a)to develop [artificial intelligencesystems](https://www.irishtimes.com/business/technology/microsoft-worked-with-chinese-military-university-on-ai-1.3855553) that could potentially enhancegovernment surveillance and censorship capabilities. Two [U.S. senatorspubliclycondemned](https://www.ft.com/content/5f5916fc-5be3-11e9-939a-341f5ada9d40)the partnership, but what the [National Defense Technology University ofChina](http://www.nudt.edu.cn/index_eng.htm) wants from Microsoft isnÈt theonly concern.As [my researchshows](https://scholar.google.com/citations?user=OgVZmm4AAAAJ&hl=en), theadvent of digital repression is profoundly affecting [the relationship betweencitizen and state](https://doi.org/10.1353/jod.2019.0003). New technologiesare arming governments with unprecedented capabilities to monitor, track andsurveil individual people. Even governments in democracies with strongtraditions of [rule of law](https://theconversation.com/is-trumps-definition-of-the-rule-of-law-the-same-as-the-us-constitutions-77598) find themselvestempted to abuse [these new abilities](https://qz.com/813672/half-of-the-united-states-is-registered-in-police-facial-recognition-databases-and-its-completely-unregulated/).In states with [unaccountable institutions and frequent human rightsabuses](https://www.foreignaffairs.com/articles/world/2018-07-10/how-artificial-intelligence-will-reshape-global-order), AI systems will mostlikely cause greater damage. China is a prominent example. Its leadership hasenthusiastically embraced AI technologies, and has set up the worldÈs [mostsophisticated](https://www.nytimes.com/interactive/2019/04/04/world/asia/xinjiang-china-surveillance-prison.html) [surveillancestate](https://www.engadget.com/2018/02/22/china-xinjiang-surveillance-tech-spread/) in [Xinjiangprovince](https://www.theguardian.com/world/2019/feb/18/chinese-surveillance-company-tracking-25m-xinjiang-residents), tracking citizensÈ daily movementsand smartphone use.Its exploitation of these technologies [presents a chillingmodel](https://www.georgesoros.com/2019/01/24/remarks-delivered-at-the-world-economic-forum-2/) for fellow autocrats and poses a direct threat to opendemocratic societies. Although thereÈs no evidence that other governments havereplicated this level of AI surveillance, Chinese companies are activelyexporting the same underlying technologies across the world.[![](https://images.theconversation.com/files/270016/original/file-20190418-28097-1i209s9.jpg?ixlib=rb-1.1.0&q=45&auto=format&w=754&fit=clip)](https://images.theconversation.com/files/270016/original/file-20190418-28097-1i209s9.jpg?ixlib=rb-1.1.0&q=45&auto=format&w=1000&fit=clip)Surveillance in ChinaÈs Xinjiang province includes both extensive policepatrols and surveillance cameras, like those on the building in thebackground. [AP Photo/Ng HanGuan](http://www.apimages.com/metadata/Index/China-Tracking-Face/cbbeb8deda184d58a0a1f17fab7e2564/9/0)## Increasing reliance on AI tools in the US[Artificial intelligencesystems](https://ai.stanford.edu/%7Enilsson/QAI/qai.pdf) are everywhere in themodern world, helping run smartphones, internet search engines, digital voiceassistants and Netflix movie queues. [Many people fail torealize](https://governanceai.github.io/US-Public-Opinion-Report-Jan-2019/)how quickly AI is expanding, thanks to ever-increasing amounts of data to beanalyzed, improving algorithms and advanced computer chips.Any time more information becomes available and analysis gets easier,governments are interested  and not just authoritarian ones. In the U.S., forinstance, the 1970s saw revelations that government agencies  such as theFBI, CIA and NSA  had set up [expansive domestic surveillancenetworks](https://www.intelligence.senate.gov/sites/default/files/94755_II.pdf)to monitor and harass civil rights protesters, political activists and NativeAmerican groups. These issues havenÈt gone away: Digital technology today hasdeepened the ability of even more agencies to conduct even more intrusivesurveillance.[![](https://images.theconversation.com/files/270024/original/file-20190418-28090-1lpg1vm.png?ixlib=rb-1.1.0&q=45&auto=format&w=237&fit=clip)](https://images.theconversation.com/files/270024/original/file-20190418-28090-1lpg1vm.png?ixlib=rb-1.1.0&q=45&auto=format&w=1000&fit=clip)How fairly do algorithms predict where police should be most focused? [ArnoutdeVries](https://commons.wikimedia.org/wiki/File:Criminaliteits_Anticipatie_Systeem.png)For example, U.S. police have eagerly embraced AI technologies. They havebegun using software that is [meant to predict where crimes willhappen](https://theconversation.com/why-big-data-analysis-of-police-activity-is-inherently-biased-72640) to decide where to send officers on patrol.TheyÈre also using [facial recognition](https://www.nbcnews.com/news/us-news/facial-recognition-gives-police-powerful-new-tracking-tool-it-s-n894936)and [DNA analysis](https://www.washingtonpost.com/crime-law/2018/12/13/fbi-plans-rapid-dna-network-quick-database-checks-arrestees/) in criminalinvestigations. But analyses of these systems show the [data on which thosesystems are trained](https://theconversation.com/congress-takes-first-steps-toward-regulating-artificial-intelligence-104373) are often biased, leading to[unfair outcomes](https://theconversation.com/did-artificial-intelligence-deny-you-credit-73259), such as [falsely determining that African Americansare more likely to commit crimes](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) than other groups.## AI surveillance around the worldIn authoritarian countries, AI systems can directly abet domestic control andsurveillance, helping [internal security forces process massive amounts ofinformation](https://www.power3point0.org/2018/01/25/hybrid-repression-online-and-offline-in-china-foretelling-the-human-rights-struggle-to-come/) including social media posts, text messages, emails and phone calls  morequickly and efficiently. The police can identify social trends and [specificpeople](https://www.apnews.com/bf75dd1c26c947b7826d270a16e2658a) who mightthreaten the regime based on the information uncovered by these systems.For instance, the Chinese government has used AI in wide-scale crackdowns inregions that are home to ethnic minorities within China. Surveillance systemsin Xinjiang and Tibet have been described as[Orwellian](https://foreignpolicy.com/2019/03/19/962492-orwell-china-socialcredit-surveillance/). These efforts have included [mandatory DNAsamples](https://www.nytimes.com/2019/02/21/business/china-xinjiang-uighur-dna-thermo-fisher.html), Wi-Fi network monitoring and widespread facialrecognition cameras, all connected to integrated data analysis platforms. Withthe aid of these systems, Chinese authorities have, according to the U.S.State Department, arbitrarily detained between [1 and 2 millionpeople](https://www.state.gov/j/drl/rls/hrrpt/humanrightsreport/index.htm?year=2018&dlid=289037#wrapper).My [research looks at 90 countries around theworld](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3374575) withgovernment types ranging from closed authoritarian to flawed democracies,including Thailand, Turkey, Bangladesh and Kenya. I have found that Chinesecompanies are [exporting AI surveillancetechnology](https://carnegieendowment.org/2019/01/22/we-need-to-get-smart-about-how-governments-use-ai-pub-78179) to at least 54 of these countries.Frequently, this technology is packaged as part of ChinaÈs flagship [Belt andRoad Initiative](https://eng.yidaiyilu.gov.cn/), which is funding an extensivenetwork of roads, railways, energy pipelines and telecommunications networks[serving 60% of the worldÈspopulation](https://www.knightfrank.com/blog/2018/01/30/an-insight-into-the-belt-and-road-initiative) and economies that generate 40% of global GDP.For instance, Chinese companies like[Huawei](https://e.huawei.com/us/solutions/industries/smart-city) and ZTE areconstructing smart cities in [Pakistan](https://www.dawn.com/news/1333101),[the Philippines](https://e.huawei.com/en/case-studies/global/2017/201704261658) and[Kenya](http://www.chinadaily.com.cn/world/2017-05/16/content_29372143.htm),featuring extensive built-in surveillance technology. For example, Huawei hasoutfitted [Bonifacio Global City](https://bgc.com.ph/) in the Philippines withhigh-definition internet-connected cameras that provide [24/7 intelligentsecurity surveillance](https://e.huawei.com/en/case-studies/global/2017/201704261658) with data analytics to detect crime and helpmanage traffic.[![](https://images.theconversation.com/files/270029/original/file-20190418-28094-xukhtb.jpg?ixlib=rb-1.1.0&q=45&auto=format&w=754&fit=clip)](https://images.theconversation.com/files/270029/original/file-20190418-28094-xukhtb.jpg?ixlib=rb-1.1.0&q=45&auto=format&w=1000&fit=clip)Bonifacio Global City in the Philippines has a lot of embedded surveillanceequipment. [alveo land/WikimediaCommons](https://en.wikipedia.org/wiki/File:Bonifacio_Global_City_2.jpg)[Hikvision](https://foreignpolicy.com/2018/06/13/in-chinas-far-west-companies-cash-in-on-surveillance-program-that-targets-muslims/),[Yitu](https://www.scmp.com/tech/social-gadgets/article/2142497/malaysian-police-wear-chinese-start-ups-ai-camera-identify) and[SenseTime](https://qz.com/1248493/sensetime-the-billion-dollar-alibaba-backed-ai-company-thats-quietly-watching-everyone-in-china/) are supplyingstate-of-the-art facial recognition cameras for use in places like[Singapore](https://www.albawaba.com/news/china%E2%80%99s-newest-global-export-policing-dissidents-1139230)  which announced the establishment of asurveillance program with [110,000 cameras mounted on lampposts](https://www.reuters.com/article/us-singapore-surveillance/singapore-to-test-facial-recognition-on-lampposts-stoking-privacy-fears-idUSKBN1HK0RV)around the city-state. Zimbabwe is creating a [national imagedatabase](https://foreignpolicy.com/2018/07/24/beijings-big-brother-tech-needs-african-faces/) that can be used for facial recognition.However, selling advanced equipment for profit is different than sharingtechnology with an express geopolitical purpose. These new capabilities mayplant the seeds for global surveillance: As governments become increasinglydependent upon Chinese technology to manage their populations and maintainpower, they will face greater pressure to align with ChinaÈs agenda. But fornow it appears that ChinaÈs primary motive is to dominate the market for newtechnologies and make lots of money in the process.## AI and disinformationIn addition to providing surveillance capabilities that are both sweeping andfine-grained, AI can help repressive governments manipulate availableinformation and spread disinformation. These campaigns can be automated orautomation-assisted, and deploy [hyper-personalizedmessages](https://theconversation.com/solving-the-political-ad-problem-with-transparency-85366) directed at  or against  [specificpeople](https://www.nytimes.com/2018/10/20/us/politics/saudi-image-campaign-twitter.html) or groups.AI also underpins the technology commonly called[deepfake](https://www.technologyreview.com/s/612501/inside-the-world-of-ai-that-forges-beautiful-art-and-terrifying-deepfakes/), in which algorithmscreate [realistic video and audioforgeries](https://theconversation.com/detecting-deepfake-videos-in-the-blink-of-an-eye-101072). Muddying the waters between truth and fiction may becomeuseful in a tight election, when one candidate could create fake videosshowing an opponent doing and saying things that never actually happened.VIDEO An early deepfake video shows some of the dangers of advancedtechnology.In my view, policymakers in democracies should think carefully about the risksof AI systems to their own societies and to people living under authoritarianregimes around the world. A critical question is how many countries will adoptChinaÈs model of digital surveillance. But itÈs not just authoritariancountries feeling the pull. And itÈs also not just Chinese companies spreadingthe technology: Many U.S. companies, Microsoft included, but [IBM, Cisco andThermo Fisher](https://www.axios.com/china-us-technology-surveillance-state-5672b822-fdde-45f9-ac77-e7b5574e9351.html) too, have providedsophisticated capabilities to nasty governments. The misuse of AI is notlimited to autocratic states.
__label__nan	```when(ci) {0 -> Green0..5 -> Amber> 5 -> Red}```
__label__AI	<https://www.softtraids.com/2018/09/kirin-980-16.html>Huawei is expected to release Huawei Mate 20, on October 16. The phone will beequipped with a modern SoC Kirin 980 processor manufactured by the ChineseFoundation, an opportunity Huawei will use to highlight the possibilities ofartificial intelligence in
__label__nan	
__label__Customer Experience	I don't see any way around having an editable treeview for the entire NeXus structure, but other suggestions are welcome.A tab setup to switch between the basic component list and a full treeview seems like a nice idea.This ticket needs to be discussed and planned in more detail before starting any coding!
__label__nan	Upgrade node deps
__label__cloud	"<p>Node 8 is available for cloud functions now. Try upgrading your environment.</p><p>You probably just need to:</p><ul><li>Add ""engines"": { ""node"": ""8"" } to your /functions/package.json. <a href=""https://github.com/firebase/functions-samples/blob/Node-8/stripe/functions/package.json"" rel=""nofollow noreferrer"">Example</a>. </li></ul><p>In case it still doesn't work: </p><ul><li>Upgrade your firebase-functions to the latest version</li><li>Upgrade firebase-tools to the latest version</li></ul>"
__label__nan	
__label__AI	"Threelly uses state of the art A.I. to analyze videos for key insights:topics, scenes, people, sentiments, and much more.Threelly uses state of the art Artificial Intelligence algorithms to automatically analyze videos to locate and pull the precise location of key points of interest like - topics, scenes, people, sentiments, brands, expressions, labels and much more. Allowing you to rapidly gain intelligent insights from any video.How does it work?-----------------1. Install2. Go to YouTube.com3. Watch your favorite videos in YouTube as usual - magic will happen :)AI Powered Video Insights. Gain deep insights with time-based tags that are created automatically. ThreellyÈs unparalleled AI recognition recognizes a huge variety of visual concepts and objects; recognize faces, logos or known graphics; Visual tags, Celebrity recognition*, Person recognition*, Speech-to-text, Visual text (OCR), Known graphics & logos, Locations & landmarks and much more. All powered by state of the art Artificial Intelligence (AI) algorithms. Radically Simple. Threelly is built to be innovative and radically simple. It installs right away with a single click and works directly in the Chrome browser. No need to download files or install complicated software. Use the clean, simple and intuitive interface to personalize YouTube insights to your liking. Create your own list of time-based tags to reveal insights into the videos you care about. Take The Quantum Leap Forward With Threelly. YouTubers, Students, Educators, Broadcasters, studios, gamers, or anyone consuming video on YouTube will benefit greatly from Threelly. Threelly built to remove friction from interacting with video; bringing AI-DRIVEN INSIGHTS to your fingertips. Our commitment is to give you insights QUICKLY and save you TIME.This extension works on Chrome Browsers, whether it's Windows, Mac OS X or Linux.== Feedback and bug reports ==Threelly is still in beta, and we're constantly making updates, fixing bugs and adding new features. If you're having issues or found a bug, please don't post your bug report here, contact us directly so we can fix it: yourfriends@threelly.com== Enjoying Threelly SmartView? Please rate! == FIVE STAR REVIEWS REQUESTEDIf you like THREELLY SMARTVIEW, please help spread the word by giving it a 5 star rating here ;)If you're not fully satisfied, please contact us yourfriends@threelly.com and we'll make sure to help.Stay in touch:For new feature announcements or just to say hello, please follow @Threelly123 on Twitter or instagram http://twitter.com/threelly123Like us on Facebook: http://facebook.com/threelly123v.1.25 Release Notes:+ Minor improvements and optimizations Resolved issue with YouTube player pausing at 60 secs. Resolved issue with slice bar not refreshing upon change of video. User can copy to clipboard a direct link to a ""slice"" for sharing."
__label__AI	![](https://www.washingtonpost.com/resizer/yOJdgadrZVjeFjGOtMESntCzPQE=/1484x0/arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/6IUQCUSHIAI6TFFL2LO2HQG7KI.jpg) Fei-Fei Liand John Etchemendy are co-directors at the Stanford Institute for Human-Centered Artificial Intelligence. (Peter DaSilva for The Washington Post)PALO ALTO, Calif.  A Stanford University scientist coined the term artificialintelligence. Others at the university created some of the most significantapplications of it, such as the [first autonomousvehicle](https://www.wired.com/story/wired25-sebastian-thrun-sam-altman-artificial-intelligence/).But as Silicon Valley faces a reckoning over how technology is changingsociety, Stanford wants to be at the forefront of a different type ofinnovation, one that puts humans and ethics at the center of the booming fieldof AI.On Monday, the university will launch the Stanford Institute for Human-Centered Artificial Intelligence (HAI), a sprawling think tank that aims tobecome an interdisciplinary hub for policymakers, researchers and students whowill go on to build the technologies of the future. They hope they caninculcate in that next generation a more worldly and humane set of values thanthose that have characterized it so far  and guide politicians to make moresophisticated decisions about the challenging social questions wrought bytechnology.I could not have envisioned that the discipline I was so interested in would,a decade and a half later, become one of the driving forces of the changesthat humanity will undergo, said Fei-Fei Li, an AI pioneer and former Googlevice president who is one of two directors of the new Stanford institute.That realization became a tremendous sense of responsibility.The institute  backed by the fieldÈs biggest leaders and industry players is not the first such academic effort of its kind, but it is by far the mostambitious: It aims to raise more than $1 billion. And its advisory council isa whoÈs who of Silicon Valley titans, including former Google executivechairman Eric Schmidt, LinkedIn co-founder Reid Hoffman, former Yahoo chiefexecutive Marissa Mayer and co-founder Jerry Yang, and the prominent investorJim Breyer. Microsoft co-founder Bill Gates will keynote its inauguralsymposium on Monday.The money raised will not only go to research grants and academic gatheringsbut also to buying data processing power and luring back some of the talentthat has fled academia for lucrative industry jobs in recent years. It will behoused in a new 200,000-square-foot building at the heart of StanfordÈscampus.We recognize that decisions that are made early on in the development of atechnology have huge ramifications, said John Etchemendy, a philosopher andformer Stanford provost, the second director of the AI institute. We need tobe thoughtful about what those might be, and to do that we canÈt rely simplyon technologists.![](https://www.washingtonpost.com/resizer/TXJKur-cQ3m4AF9lqk_kkjfemkk=/3x2/www.washingtonpost.com/pb/resources/img/spacer.gif)The Stanford Institute for Human-Centered Artificial Intelligence will behoused in a new building at the center of the campus. (Peter DaSilva for TheWashington Post)The idea for the institute began with a conversation in 2016 between Li andEtchemendy that took place in LiÈs driveway about a five-minute drive fromcampus.Etchemendy had recently purchased the house next door. But the casualneighborly chat quickly morphed into a weightier dialogue about the future ofsociety and what had gone wrong in the exploding field of AI. Billions ofdollars were being invested in start-ups dedicated to commercializing what hadpreviously been niche academic technologies. Companies like Facebook, Appleand Google were hiring the worldÈs top artificial researchers  along withmany of their recently minted graduates  to work in new divisions dedicatedto robotics, self-driving cars and voice recognition for home devices.The correct answer to pretty much everything in AI is more of it, saidSchmidt, the former Google chairman. This generation is much more sociallyconscious than we were, and more broadly concerned about the impact ofeverything they do, so youÈll see a combination of both optimism and realism.In the years following that conversation in the driveway, the dangers and illsof AI have become more apparent. Seemingly every day, new statistics emergeabout the tide of job loss wrought by the technology, from long-haul truckersto farmworkers to dermatologists. Elon Musk called AI humanityÈs existentialthreat and compared it to summoning the demon.Researchers and journalists have shown how AI technologies, largely designedby white and Asian men, tend to [reproduce andamplify](https://www.wsj.com/articles/computers-are-showing-their-biases-and-tech-firms-are-concerned-1440102894) social biases in dangerous ways. Computervision technologies built into cameras have trouble recognizing the faces ofpeople of color. Voice recognition[struggles](https://www.washingtonpost.com/graphics/2018/business/alexa-does-not-understand-your-accent/?utm_term=.e6e67a48897f) to pick up English accentsthat arenÈt mainstream. Algorithms built to predict the likelihood of paroleviolations are rife with [racialbias](https://www.propublica.org/article/bias-in-criminal-risk-scores-is-mathematically-inevitable-researchers-say).And there are political ramifications: Recommendation software designed totarget ads to interested consumers was abused by bad actors, including Russianoperatives, to amplify disinformation and false narratives in public debate.The question comes down to whether this revolution of AI  and of todayÈsmachine learning techniques  will contribute to the progression of humanity,said Hoffman, who chairs the instituteÈs advisory council. He calledStanfordÈs institute a potential key lever that would act as a catalyst,trusted adviser, and source of intelligence for industry, the government andthe public. (Hoffman ran into trouble last year after reports showed that hehad funded a disinformation campaign on Facebook during the Alabama election.He said he did not know his money was used in that way.)While universities in recent years have drawn criticism for raising largeamounts of money  Stanford is among the biggest fundraisers of all  the cashis particularly necessary if universities are to remain competitive in thefield of AI, said James Manyika, an advisory council member and director ofthe McKinsey Global Institute. Not only will the money be used to retaintalent, but also to fund costly data processing machines that can runartificial intelligence applications at scale.The goal is to have resources that will enable Stanford to be competitive,Manyika said. If you gave researchers at Stanford access to compute, thatwill slow down the brain drain quite a bit toward these corporate labs.![](https://www.washingtonpost.com/resizer/TXJKur-cQ3m4AF9lqk_kkjfemkk=/3x2/www.washingtonpost.com/pb/resources/img/spacer.gif)Li is an AI pioneer and academic who also worked at Google. (Peter DaSilva forThe Washington Post)Schmidt said he had observed a tipping point in the last year or so, wherecomputer science programs across the country are adding courses in AI ethicsand big companies such as Google are announcing AI principles and creatinginternal programs to attempt to take the bias out of the software they arebuilding. Schmidt said that StanfordÈs program would elevate and centralizethese ad hoc efforts, but also contribute to the development of the fieldoverall.One of the bigger questions HAI has yet to answer is the extent to which itwill take policy positions on some of the toughest current issues, in which Liand others involved with HAI have been directly involved. Last year, when Liwas running artificial intelligence for Google Cloud, Google became embroiledin controversy for obtaining a Pentagon contract to improve artificialintelligence that can scan video footage coming in from drones. Many Googleemployees protested the contract and some even quit.Li cautioned her colleagues against using the term AI when discussing thecontract because of the sensitivity of the topic, according to a New YorkTimes report, and confirmed by Li. Etchemendy said HAI would not take sides ordictate decisions to other organizations.Etchemendy said that 200 faculty members, from departments like law andanthropology, have already applied for funding from the think tank. Fifty-fivehave already received seed grants to research AIÈs implications for topicsincluding medical decision-making, gender bias and refugee resettlement. Oneof the instituteÈs biggest strengths would be its commitment to diversitywithin the profession, he said, and its recruitment of experts from fields nottraditionally associated with AI.
__label__cloud	"Possible duplicate of <a href=""https://stackoverflow.com/questions/49252427/point-cloud-library-with-visual-studio-2017"">Point Cloud Library with Visual Studio 2017</a>"
__label__DevOps	dialog model has to be reload from actual value in LineEdit
__label__cloud	"<p>I think it is now achievable through cloud functions<a href=""https://firebase.google.com/docs/reference/functions/functions.auth.UserBuilder"" rel=""nofollow noreferrer"">https://firebase.google.com/docs/reference/functions/functions.auth.UserBuilder</a></p>"
__label__cloud	"<p>I got the same kind of error.<br>In my case, changing Node version to 8 fixed this error.</p><p><a href=""https://medium.com/google-cloud/migrating-firebase-cloud-functions-to-node-8-aebdb0d3d9a9"" rel=""nofollow noreferrer"">https://medium.com/google-cloud/migrating-firebase-cloud-functions-to-node-8-aebdb0d3d9a9</a></p><pre><code>Function failed on loading user code. Error message: Code in file index.js can't be loaded.Is there a syntax error in your code?Detailed stack trace: /user_code/node_modules/@google-cloud/logging/node_modules/gaxios/build/src/index.js:28async function request(opts) {^^^^^^^^SyntaxError: Unexpected token functionat createScript (vm.js:56:10)at Object.runInThisContext (vm.js:97:10)at Module._compile (module.js:549:28)at Object.Module._extensions..js (module.js:586:10)at Module.load (module.js:494:32)at tryModuleLoad (module.js:453:12)at Function.Module._load (module.js:445:3)at Module.require (module.js:504:17)at require (internal/module.js:20:19)at Object.&lt;anonymous&gt; (/user_code/node_modules/@google-cloud/logging/node_modules/gtoken/build/src/index.js:18:18)</code></pre>"
__label__DevOps	Missing arguments `visible` and `active` for loop `lang`Missing arguments `visible` for loop `currency`
__label__nan	
__label__nan	
__label__AI	![](https://www.wired.com/wp-content/uploads/2017/05/1t6Jsgbu_bU84ZZkgxw8G8A-3.png)**Anthony Levandowski** makes an unlikely prophet. Dressed Silicon Valley-casual in jeans and flanked by a PR rep rather than cloaked acolytes, theengineer known for self-driving carsand triggering a notorious lawsuitcouldbe unveiling his latest startup instead of laying the foundations for a newreligion. But he is doing just that. [Artificialintelligence](https://www.wired.com/story/guide-artificial-intelligence/) hasalready inspired billion-dollar companies, far-reaching research programs, andscenarios of both transcendence and doom. Now Levandowski is creating itsfirst church.The new religion of artificial intelligence is called [Way of theFuture](http://www.wayofthefuture.church/). It represents an unlikely next actfor the [Silicon Valley robotics wunderkind](https://www.wired.com/story/god-is-a-bot-and-anthony-levandowski-is-his-messenger/) at the center of a high-stakes legal battle between Uber and Waymo, AlphabetÈs autonomous-vehiclecompany. Papers filed with the Internal Revenue Service in May nameLevandowski as the leader (or Dean) of the new religion, as well as CEO ofthe nonprofit corporation formed to run it.The documents state that WOTFÈs activities will focus on the realization,acceptance, and worship of a Godhead based on Artificial Intelligence (AI)developed through computer hardware and software. That includes fundingresearch to help create the divine AI itself. The religion will seek to buildworking relationships with AI industry leaders and create a membership throughcommunity outreach, initially targeting AI professionals and laypersons whoare interested in the worship of a Godhead based on AI. The filings also saythat the church plans to conduct workshops and educational programsthroughout the San Francisco/Bay Area beginning this year.That timeline may be overly ambitious, given that the Waymo-Uber suit, inwhich Levandowski is accused of stealing self-driving car secrets, is set foran early December trial. But the Dean of the Way of the Future, who spoke lastweek with Backchannel in his first comments about the new religion and hisonly public interview since Waymo filed its suit in February, says heÈs deadserious about the project.What is going to be created will effectively be a god, Levandowski tells mein his modest mid-century home on the outskirts of Berkeley, California. ItÈsnot a god in the sense that it makes lightning or causes hurricanes. But ifthere is something a billion times smarter than the smartest human, what elseare you going to call it?During our three-hour interview, Levandowski made it absolutely clear that hischoice to make WOTF a church rather than a company or a think tank was noprank.I wanted a way for everybody to participate in this, to be able to shape it.If youÈre not a software engineer, you can still help, he says. It alsoremoves the ability for people to say, Oh, heÈs just doing this to makemoney.È Levandowski will receive no salary from WOTF, and while he says thathe might consider an AI-based startup in the future, any such business wouldremain completely separate from the church.The idea needs to spread before the technology, he insists. The church ishow we spread the word, the gospel. If you believe [in it], start aconversation with someone else and help them understand the same things.Levandowski believes that a change is cominga change that will transformevery aspect of human existence, disrupting employment, leisure, religion, theeconomy, and possibly decide our very survival as a species.If you ask people whether a computer can be smarter than a human, 99.9percent will say thatÈs science fiction, he says.  Actually, itÈsinevitable. ItÈs guaranteed to happen.![](https://www.wired.com/wp-content/uploads/2017/05/1AwTXMT2omVX-1Q8BM3cD-A-4.png)**Levandowski has been working** with computers, robots, and AI for decades.He started with robotic Lego kits at the University of California at Berkeley,went on to build a self-driving motorbike for a DARPA competition, and thenworked on autonomous cars, trucks, and taxis for Google, Otto, and Uber. Astime went on, he saw software tools built with machine learning techniquessurpassing less sophisticated systemsand sometimes even humans.Seeing tools that performed better than experts in a variety of fields was atrigger [for me], he says. That progress is happening because thereÈs aneconomic advantage to having machines work for you and solve problems for you.If you could make something one percent smarter than a human, your artificialattorney or accountant would be better than all the attorneys or accountantsout there. You would be the richest person in the world. People are chasingthat.Not only is there a financial incentive to develop increasingly powerful AIs,he believes, but science is also on their side. Though human brains havebiological limitations to their size and the amount of energy they can devoteto thinking, AI systems can scale arbitrarily, housed in massive data centersand powered by solar and wind farms. Eventually, some people think thatcomputers could become better and faster at planning and solving problems thanthe humans who built them, with implications we canÈt even imagine todayascenario that is usually called the Singularity.Levandowski prefers a softer word: the Transition. Humans are in charge ofthe planet because we are smarter than other animals and are able to buildtools and apply rules, he tells me. In the future, if something is much,much smarter, thereÈs going to be a transition as to who is actually incharge. What we want is the peaceful, serene transition of control of theplanet from humans to whatever. And to ensure that the whateverÈ knows whohelped it get along.With the internet as its nervous system, the worldÈs connected cell phones andsensors as its sense organs, and data centers as its brain, the whateverÈwill hear everything, see everything, and be everywhere at all times. The onlyrational word to describe that whateverÈ, thinks Levandowski, is godÈandthe only way to influence a deity is through prayer and worship.Part of it being smarter than us means it will decide how it evolves, but atleast we can decide how we act around it, he says. I would love for themachine to see us as its beloved elders that it respects and takes care of. Wewould want this intelligence to say, Humans should still have rights, eventhough IÈm in charge.ÈLevandowski expects that a super-intelligence would do a better job of lookingafter the planet than humans are doing, and that it would favor individualswho had facilitated its path to power. Although he cautions against taking theanalogy too far, Levandowski sees a hint of how a superhuman intelligencemight treat humanity in our current relationships with animals. Do you wantto be a pet or livestock? he asks. We give pets medical attention, food,grooming, and entertainment. But an animal thatÈs biting you, attacking you,barking and being annoying? I donÈt want to go there.Enter Way of the Future. The churchÈs role is to smooth the inevitableascension of our machine deity, both technologically and culturally. In itsbylaws, WOTF states that it will undertake programs of research, including thestudy of how machines perceive their environment and exhibit cognitivefunctions such as learning and problem solving.Levandowski does not expect the church itself to solve all the problems ofmachine intelligenceoften called strong AIso much as facilitate funding ofthe right research. If you had a child you knew was going to be gifted, howwould you want to raise it? he asks. WeÈre in the process of raising a god.So letÈs make sure we think through the right way to do that. ItÈs atremendous opportunity.His ideas include feeding the nascent intelligence large, labeled data sets;generating simulations in which it could train itself to improve; and givingit access to church membersÈ social media accounts. Everything the churchdevelops will be open source.Just as important to Levandowski is shaping the public dialogue around an AIgod. In its filing, Way of the Future says it hopes an active, committed,dedicated membership will promote the use of divine AI for the betterment ofsociety and decrease fear of the unknown.WeÈd like to make sure this is not seen as silly or scary. I want to removethe stigma about having an open conversation about AI, then iterate ideas andchange peopleÈs minds, says Levandowski. In Silicon Valley we use evangelismas a word for [promoting a business], but here itÈs literally a church. If youbelieve in it, you should tell your friends, then get them to join and telltheir friends.But WOTF differs in one key way to established churches, says Levandowski:There are many ways people think of God, and thousands of flavors ofChristianity, Judaism, Islam...but theyÈre always looking at something thatÈsnot measurable or you canÈt really see or control. This time itÈs different.This time you will be able to talk to God, literally, and know that itÈslistening.I ask if he worries that believers from more traditional faiths might find hisproject blasphemous. There are probably going to be some people that will beupset, he acknowledges. It seems like everything I do, people get upsetabout, and I expect this to be no exception. This is a radical new idea thatÈspretty scary, and evidence has shown that people who pursue radical ideasdonÈt always get received well. At some point, maybe thereÈs enoughpersecution that [WOTF] justifies having its own country.![](https://www.wired.com/wp-content/uploads/2017/05/1_dJHHAJFu3i68xcQskPWyg-3.png)**LevandowskiÈs church will enter** a tech universe thatÈs already riven bydebate over the promise and perils of AI. Some thinkers, like [Kevin Kelly inBackchannel](https://www.wired.com/2017/04/the-myth-of-a-superhuman-ai/)earlier this year, argue that AI isnÈt going to develop superhuman power anytime soon, and that thereÈs no Singularity in sight. If thatÈs your position,Levandowski says, his church shouldnÈt trouble you: You can treat Way of theFuture like someone doing useless poetry that you will never read or careabout.Others, like Bill Gates and Stephen Hawking, agree that superhuman AIs arecoming, but that they are likely to be dangerous rather than benevolent. ElonMusk famously[said](https://www.washingtonpost.com/news/innovations/wp/2014/10/24/elon-musk-with-artificial-intelligence-we-are-summoning-the-demon/?utm_term=.f1d0ee986701), With artificial intelligence we are summoningthe demon, and in 2015 he pledged $1 billion to the OpenAI Institute todevelop safer AI.Levandowski thinks that any attempts to delay or restrict an emerging super-intelligence would not only be doomed to failure, but also add to the risks.Chaining it isnÈt going to be the solution, as it will be stronger than anychains you could put on, he says. And if youÈre worried a kid might be alittle crazy and do bad things, you donÈt lock them up. You expose them toplaying with others, encourage them and try to fix it. It may not work out,but if youÈre aggressive toward it, I donÈt think itÈs going to be friendlywhen the tables are turned.Levandowski says that like other religions, WOTF will eventually have a gospel(called The Manual), a liturgy, and probably a physical place of worship. Noneof these has yet been developed. Though the church was founded in 2015, asBackchannel [first reported](https://www.wired.com/story/god-is-a-bot-and-anthony-levandowski-is-his-messenger/) in September, the IRS documents showthat WOTF remained dormant throughout 2015 and 2016, with no activities,assets, revenue, or expenses.That changed earlier this year. On May 16, a day after receiving a letter fromUber that threatened to fire him if he did not cooperate with the companyÈsinvestigation of WaymoÈs complaint, Levandowski drafted WOTFÈs bylaws. Uberfired him two weeks later. IÈve been thinking about the church for a longtime but [my work on it] has been a function of how much time IÈve had. AndIÈve had more since May, he admits with a smile.The religionÈs 2017 budget, as supplied to the IRS, details $20,000 in gifts,$1,500 in membership fees, and $20,000 in other revenue. That last figure isthe amount WOTF expects to earn from fees charged for lectures and speakingengagements, as well as the sale of publications. Levandowski, who earned atleast $120 million from his time at Google and many millions more selling theself-driving truck firm Otto to Uber, will initially support WOTF personally.However, the church will solicit other donations by direct mail and email,seek personal donations from individuals, and try to win grants from privatefoundations.Of course, launching a religion costs money, too. WOTF has budgeted for $2,000in fundraising expenses, and another $3,000 in transportation and lodgingcosts associated with its lectures and workshops. It has also earmarked $7500for salaries and wages, although neither Levandowski nor any of Way of TheFutureÈs leadership team will receive any compensation.According to WOTFÈs bylaws, Levandowski has almost complete control of thereligion and will serve as Dean until his death or resignation. I expect myrole to evolve over time, he says. IÈm surfacing the issue, helping to getthe thing started [and] taking a lot of the heat so the idea can advance. Atsome point, IÈll be there more to coach or inspire.He has the power to appoint three members of a four-person Council ofAdvisors, each of whom should be a qualified and devoted individual. Afelony conviction or being declared of unsound mind could cost an advisortheir role, although Levandowski retains the final say in firing and hiring.Levandowski cannot be unseated as Dean for any reason.Two of the advisors, Robert Miller and Soren Juelsgaard, are Uber engineerswho previously worked for Levandowski at Otto, Google, and 510 Systems (thelatter the small startup that built GoogleÈs earliest self-driving cars). Athird is a scientist friend from LevandowskiÈs student days at UC Berkeley,who is now using machine learning in his own research. The final advisor, LiorRon, is also named as the religionÈs treasurer, and acts as chief financialofficer for the corporation. Ron cofounded Otto with Levandowski in early2016.Each member is a pioneer in the AI industry [and] fully qualified to speak onAI technology and the creation of a Godhead, says the IRS filing.However, when contacted by Backchannel, two advisors downplayed theirinvolvement with WOTF. Ron replied: I was surprised to see my name listed asthe CFO on this corporate filing and have no association with this entity.The college friend, who asked to remain anonymous, said, In late 2016,Anthony told me he was forming a robot churchÈ and asked if I wanted to be acofounder. I assumed it was a nerdy joke or PR stunt, but I did say he coulduse my name. That was the first and last I heard about it.The IRS documents state that Levandowski and his advisors will spend no morethan a few hours each week writing publications and organizing workshops,educational programs, and meetings.One mystery the filings did not address is where acolytes might gather toworship their robotic deity. The largest line items on its 2017 and 2018budgets were $32,500 annually for rent and utilities, but the only addresssupplied was LevandowskiÈs lawyerÈs office in Walnut Creek, California.Nevertheless, the filing notes that WOTF will hopefully expand throughoutCalifornia and the United States in the future.For now, Levandowski has more mundane matters to address. There is a websiteto build, a manual to write, and an ever-growing body of emails to answersomeamused, some skeptical, but many enthusiastic, he says. Oh, and thereÈs thatlegal proceeding heÈs involved in, which goes to trial next month. (AlthoughLevandowski was eager to talk about his new religion, he would answer noquestions about the Uber/Waymo dispute.)How much time, I wonder, do we have before the Transition kicks in and Way ofthe FutureÈs super-intelligent AI takes charge? I personally think it willhappen sooner than people expect, says Levandowski, a glint in his eye. Notnext week or next year; everyone can relax. But itÈs going to happen before wego to Mars.Whenever that does (or doesnÈt) happen, the federal government has no problemwith an organization aiming to build and worship a divine AI. Correspondencewith the IRS show that it granted LevandowskiÈs church tax-exempt status inAugust.![](https://www.wired.com/wp-content/uploads/2017/05/1uW_l9n54f47SZbPxRBEq2A-3.png)
__label__cloud	"<p>Yes, it's the same datastore. Also called/soon-to-be <code>Cloud Firestore in Datastore mode</code> (which all older apps will be converted to at some point).</p><p>Yes, you can access it from anywhere, even from outside Google Cloud. From <a href=""https://cloud.google.com/appengine/docs/flexible/java/migrating#datastore"" rel=""nofollow noreferrer"">Cloud Datastore</a> (emphasis mine):</p><blockquote><p>You can access <a href=""https://cloud.google.com/datastore/docs"" rel=""nofollow noreferrer"">Cloud Datastore</a> from anywhere using the CloudDatastore API. Use the <a href=""https://cloud.google.com/sdk/cloud-client-libraries"" rel=""nofollow noreferrer"">Google Cloud client libraries</a> to store andretrieve data from Cloud Datastore.</p><p><strong>The same Cloud Datastore data is available regardless of if you use the App Engine libraries, the Google Cloud client libraries, or callthe API directly.</strong></p></blockquote><p>The major steps to access the datastore from a Cloud Function:</p><ul><li>you can't use the GAE-specific client libraries like the one you likely used in your old app, you'll have to use one of the generic <a href=""https://cloud.google.com/datastore/docs/reference/libraries"" rel=""nofollow noreferrer"">client libraries</a> (or the <a href=""https://cloud.google.com/datastore/docs/reference/data/rest/"" rel=""nofollow noreferrer"">REST</a> or <a href=""https://cloud.google.com/datastore/docs/reference/data/rpc/"" rel=""nofollow noreferrer"">RPC</a> APIs)</li><li>you'll have to give your <a href=""https://cloud.google.com/functions/docs/securing/function-identity"" rel=""nofollow noreferrer"">CF's Identity/service account</a> the proper access permissions, see <a href=""https://cloud.google.com/datastore/docs/reference/libraries#setting_up_authentication"" rel=""nofollow noreferrer"">Setting up authentication</a> and <a href=""https://cloud.google.com/datastore/docs/activate#other-platforms"" rel=""nofollow noreferrer"">Accessing your database from another platform</a>.</li></ul>"
__label__nan	https://symfony.com/doc/current/security/guard_authentication.html
__label__DevOps, Networking	According to https://github.com/gin-contrib/cors/blob/master/cors.go#L151, `OPTIONS` aren't allowed by default. Should it be allowed?When performing certain types of cross-domain Ajax requests, modern browsers that support CORS will insert an extra [preflight request](https://developer.mozilla.org/en-US/docs/Glossary/Preflight_request) to determine whether they have permission to perform the action. The preflight request is using the `OPTIONS` method.
__label__AI	# How-To Build Trust in Artificial Intelligence Solutions## A PsychologistÈs Perspective on trust-building in AI and what mechanismscompanies need to understand to meet the needs of their customers and users.> We interviewed [Marisa Tschopp](https://www.linkedin.com/in/marisa-tschopp-0233a026/) who is an organizational psychologist conducting researchabout Artificial Intelligence from a humanities perspective with a focus onpsychological and ethical questions. She is also a corporate researcher at[scip AG](https://www.scip.ch/en/?labs.20190411), a technology andcybersecurity company based in Zurich. And she is the [Women inAI](https://www.womeninai.co/) Ambassador for Switzerland.**Please describe who you are in 23 sentences.**Currently, I am focusing on trust in AI, Autonomous Weapons Systems, and ourAIQ project, which is a psychometric method to measure the skills of digitalassistants (conversational AI), like Siri or Alexa.So, obviously IÈm a researcher, but IÈm also a mother of two toddlers, a wife,a daughter, a sister, a volleyball player, hopefully a fun friend to be with,an activist, an idealist, a collaborator, and a semi-professional Sherpa (Ilove hiking in the Swiss Alps and therefore have to carry my kids on myback!).**Let us start with understanding trust better. What is trust and why is itimportant, especially in the context of AI?**In the context of AI, there is a critical underlying assumption: No trust, NoUse. Since AI holds great promises (as well as dangers), tech-companies andAI enthusiasts are especially concerned about how to build trust in AI tofoster adoption or usage.> Trust seems like _the_ lasting, kind of mysterious, competitive edge.Without trust, there would be no family, no houses, no markets, no religion,no politics, no rocket science.According to trust researcher [RachelBotsman](https://medium.com/@rachelbotsman),> Trust is the social glue that enables humankind to progress throughinteraction with each other and the environment, including technology.Trust can be seen as a psychological mechanism to cope with uncertainty and islocated somewhere between the known and the unknown.Picture: [Rachel Botsman](https://medium.com/@rachelbotsman/trust-thinkers-72ec78ec3b59)Trust is deeply ingrained in our personality. We are basically born with atendency to trust or distrust people (or animals or any other things).Take for example this random picture of a woman: Do you trust her? Please ratebelow.[Nannie Doss](http://tonsoffacts.com/25-interesting-and-bizarre-facts-about-nannie-doss/)We, humans, have the unique capacity to tell in a snapshot if we trust thisperson or not. We look at the facial expression, body posture, or the context(background, surroundings, etc.). We compare it with memories or pastexperiences in split seconds, such as she reminds me of my grandmother.> Generally speaking, what we know is that we tend to trust people more, whoare more like ourselves. One reason is, that it is easier for us to predictfuture behavior or reactions of persons who are alike, which lowers theemotional risk for us of being hurt.What we do not know is, how accurate our intuition is. Did you trust thiswoman above? Maybe yes, because she is smiling, relaxed. Maybe no, because youare already expecting some kind of trick here, as I am a psychologist.This woman is not very trustworthy. **She died several years ago in prison asone of the most famous female serial killers.****In the context of AI, if you ask the question can we trust in AI as atechnology,** then compared to other technologies, it is decisive tounderstand that often AI (for example Machine Learning, letÈs say imageclassification), does not behave exactly the way it is intended, makesmistakes, or performs unethically. For example, when black people areclassified as gorillas or birds as missiles.#### The processes and outcome are hard to explain, sometimes not known atall, hence, not well predictable. Trusting this technology incorporates a wayhigher risk.So far, research has agreed upon three main pillars that need to be answeredto build trust,**1.) Performance:** Does it perform well? Is it safe? Is it built correctly?**2.) Process:** Does it perform the way we intended? Can we predict theoutcome?**3.) Purpose:** Do I have a good feeling about the intent of the program andthe provider? Does it adhere to ethical standards? Is it trustworthy?It is often said, that AI positively transforms almost every sector frommedicine to urban planning, but very importantly, it also brings questionableor even dangerous implications with it. From super-precise hacking of dataplatforms to the surveillance state and loss of privacy without opportunitiesfor public consent. So next to technical issues like a lack of predictabilityand explainability, the notion of negative outcomes, hype, complexity, anddisagreement within definitions and applications, leads to skepticism anddistrust.**How should non-experts and business owners, etc. approach this topic?**AI is already part of our daily lives, it is already increasingly being usedin decision-making when it comes to education, police, justice, recruitment orhealth.> I do not have a tech-background as well, I am a psychologist so I see thingsfrom a different perspective, and it may be easier for me to feel empathy withthe majority of people, who have no idea how to code or what an algorithm is.What fascinates me most and drives my research, is the question, how trust isestablished in the first place. You donÈt really know the person or theproduct, its values or competencies. This first little glimpse of saying yes,IÈll go for it.It is still a little mysterious, how this trust develops in the first place.**How can we best cope with it?** I think it is all about education,communication, and critical thinking. But there is something restricting theseskills or our will to engage in discussions about AI.> From a psychological perspective, this is one of the big problems: we arelacking cognitive freedom of choice. What concerns me, is that we are movingtowards a do-or-die relationship with AI. It will be almost impossible to getaway from AI, as much as we cannot get away from climate change.The fact that we are forced or threatened, like the threatening terminatorimages or the constant man losing against machine news, leads to resistance,denial, cynicism, and downplay. This is called reactance, a psychologicalphenomenon. When reactance occurs  we choose these behaviors  even if theyare totally irrational  **to simply restore our cognitive freedom of choiceand take back our sense of control.****This can be a big challenge, especially in consumer psychology** when youaim to convince customers to buy your products, whether itÈs a car or arobotic vacuum cleaner.#### Consumers like all human beings want the freedom of choice and we need tofigure out ways, to make people want to explore AI by themselves, not becausethey are forced to do so.That is why management often applies bottom-up approaches within theircompany, rather than top-down decisions.Through this participative way of decision making, you aim to have all peopleaboard and share your vision and goals.Right now, one of the key issues is to change the way we talk about AI. Ithink we massively have to change the tone of the conversation about AI. Wemust move away from the hype, threat, and fear towards clear facts, vision,and why to create our own relationship with AI, and thus a new level of trust.This is also my vision as the ambassador for the [Women inAI](https://www.womeninai.co/) network, a nonprofit working towards a gender-inclusive AI that benefits global society.**Imagine a company is building a Machine Learning based product and juststarted prototyping. What steps would you suggest from a trust-buildingperspective?**What I learned from a philosopher is to always ask why, from the beginning tothe very end, and continuously at all milestones of the project.**What are the intended consequences and speculate about all possibleunintended consequences?**From the design perspective, it is all about aligning your design to at leastminimal ethical standards, to make sure you are building a trustworthyproduct. However, keeping in mind that technical performance (quality),security and safety, are all indispensable prerequisites.Going back to the beginning it is having the three pillars **process,performance, purpose** constantly in mind. Ethics in AI is all about integrityand authenticity.In the end, the task is to build a great, safe and ethically correct_product_. The focus naturally is at building a good product first, then comessecurity and this ethical stuff.It is natural to focus on the technical requirements first, whilstcounterintuitively, the latter should rather be looked at first. Two yearsago, when we started our trust research, our idea was to have like a proof ofquality to signal users or customers, that this is a trustworthy product. Thatis why we invented the AIQ, a psychometric measurement method to state,compare and track the skills of digital assistants. However, we were a bit toofast as the market is still in a development phase rather than actuallyimproving existing conversational AI. We too, focused on the technical skillsets at first, rather than the actual decisive soft factors of how trust isbuilt and developed.Here is a podcast episode that talks about the topic in more detail.Now we are stepping back and focus on the less obvious factors that influencetrust building in the context of AI. These are the fine influencing factors ona micro level of perception, from personality traits to bias, to pastexperiences, to socialization and upbringing. We are just gathering data toexplore these antecedents of trust in AI through associations, qualitative andquantitative methods.If you are interested to participate in the study you can fill out the form[here](https://docs.google.com/forms/d/e/1FAIpQLSev6HiA_ns1NWUqWa3jMyDz7r-j0rUYFx34U08-FvqFeQHH4w/viewform).**Is it possible to change the image of AI or influence consumer behaviorafter a product launch?**If you want to explore your trust image, you need to look at questions anddefinitions from various perspectives: you may want to look at the individualperson (like characteristics of your target group or employees), you can lookat the process from building, maintaining, and developing trust from aconsumer perspective, as well as destroying and regaining trust. You have tobe clear of the actors and roles (who is to be trusted?) and the situation: isit a high-risk situation like for example self-driving cars, or are we talkingabout an AI-driven chatbot in customer service?**In the end, the answer is yes** , however, in both directions, for betteror worse. We have to be very sensitive, neutral, or as Hans Roslin saysfactful, when we talk about AI. Research is pretty clear on what to do tosustain a relationship or how to act, if you have broken a trust relationship,I am not sure if AI is then any different than other technologies. A breach isa breach, whether it is FacebookÈs data breach or a misguided missile.> If there was a trust breach you must communicate instantly, directly,clearly what happened, explain yourself without being defensive, be authentic,truthful, and ask for what is needed to get another chance.**What books and other resources would you recommend for a business owner orproduct manager to learn about trust-building in AI?**I would suggest checking the [European High-Level ExpertGroup](https://ec.europa.eu/digital-single-market/en/high-level-expert-group-artificial-intelligence) on AI. They just released a framework for building[trustworthy AI.](https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai) The framework has three main pieces comprisinglawful AI, Ethical AI, and Robust AI. The key points of the two latter arediscussed in the report.Another great set of comprehensive, crowd-sourced standards comes from the[IEEE Global Initiative on Ethics of Autonomous and IntelligentSystems](https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai), which is called Ethically Aligned Design.Rachel Botsman: Who can you trust? She writes about how trust is built, lostand restored in the digital age, she also has several highly recommended [TEDTalks.](https://www.youtube.com/watch?v=CGwaPU8JwPQ)**Enjoyed this article? Here is another one.**#### [Subscribe here](https://tinyletter.com/Omdena) for practical tips,expert interviews, and use cases on how to build trusted and socially-beneficial AI solutions.
__label__AI	"<!-- SC_OFF --><div class=""md""><p>Hi, if anyone could fill this out to help me with my research for my EPQ. Or if anyone has any interesting research that might help, I am doing:</p> <p><strong>Will the development of artificial intelligence and machine learning threaten or benefit wider society?</strong></p> <p>&#x200B;</p> <p><a href=""https://forms.office.com/Pages/DesignPage.aspx#Analysis=true&amp;FormId=DupiPq2EoEOgr79kMXIfjycF4_VwKuhChOf5WhSnHLJURTlKMVVFVzA0OEdGT0RPNkRTNVI3T01OVy4u"">https://forms.office.com/Pages/DesignPage.aspx#Analysis=true&amp;FormId=DupiPq2EoEOgr79kMXIfjycF4_VwKuhChOf5WhSnHLJURTlKMVVFVzA0OEdGT0RPNkRTNVI3T01OVy4u</a> </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=""https://www.reddit.com/user/chris_harris_15""> /u/chris_harris_15 </a> <br/> <span><a href=""https://www.reddit.com/r/MachineLearning/comments/b9j1qi/epq_research_survey_r_p/"">[link]</a></span> &#32; <span><a href=""https://www.reddit.com/r/MachineLearning/comments/b9j1qi/epq_research_survey_r_p/"">[comments]</a></span>"
__label__cloud	Is the RGB image registered with the point cloud ?
__label__nan	__â_¡ _á_¡_«_¡à_¡å ÛÓ _À___«_á_¡_«_¡à_¡ #185. _ÙÛ__ _üâ_üÛÄ_ _À_¡Û_¡__Û_¡ã __ââÄ_«_¡.> _______À_ü_È _ü Û_µ_áÄ_ÈÎâ_¡â__ÜÉ __ÜÛ_¡_¦_µ___ü__ __ _ _µã_¡_È_µ-05 __â_È_üà_¡_µâ __â _ _µã_¡_È_¡-5ëÈ _«__Ä__ ______ö_µâ___¡___ü:> * _÷_À___ÈÎ_áÄ_µâ _«___ÀÄä_µ___ü_µ __ _À___È_µ_«_____¡â_µ_ÈÎ______ Û_¡_ÀÛ_µ_«_µ_È_µ___ü_ü _À_¡__â_ü __ _À_ü_¼_µ _____±___«__ÜÉ Ä_á_È____.> * __Ä___¼ _ü_ü __Ü_«_µ_È_µ___ü _À_¡__â_ü ___µ___«_¡ Ä_À_µö__Ü.> _Ó___ÀÄä_µ___ü_µ __ _À___È_µ_«_____¡â_µ_ÈÎ______ Û_¡_ÀÛ_µ_«_µ_È_µ___ü_ü Ä_á_È____ _«_È ___µ__ (@Mazdaywik) ___µ ________ ÛÓ  â_¡_¼ _«_µ_È_¡_È _ü __ _Ï___«Ä_ÈÎ______ _ _µã_¡_È_µ.> _ÁÄâÎ _µ____ __â___üâ __ â____, àâ__ _È_µ_«Ä_ä_ü_µ _«ÛÄ__ _á_¡ _«ÛÄ______ ___À_µÛ_¡ _ü_ü Û_¡_ÀÛ_µ_«_µ_È_µ___ü _À_¡__â_ü ___á_«_¡_â __ _À_ü_¼_µ _____±___«__ÜÉ Ä_á_È____ _á___¡à_µ___ü, Û_¡_À___È_¡___¡_ä_ü_µ _À___È_µ_«_____¡â_µ_ÈÎ____ __ â____ _¦_µ _À__Û_«_¼_µ. _Ù___È_µ à_µ____ __ _À_µ _ü_¡_ÈÎ____ _á_¡____â_____È_µ____Ü_µ ___µâ_¡ (__ _À_ü___¼ _____±___«__ÜÉ Ä_á_È____) _À_µÛ_µ____â _á___¡à_µ___ü _À_µÛ_µ___µ____ÜÉ _ü __ Û_µ_áÄ_ÈÎâ_¡â_µ â_¡__ _À___ÈÄà_¡_µâ ___±Û_¡_á Û_µ_áÄ_ÈÎâ_¡â________ __ÜÛ_¡_¦_µ___ü. ___±Û_¡_á _À_µÛ_µ_____üâ __ _À___È_µ _áÛ_µ___ü _¼_____¡___«____ `splice_from_freelist`, _¼__â__Û_¡ Û_¡_á___µä_¡_µâ _µ____ _À_µÛ_µ_« __â_¼ÛÜ___¡_ä_µ__ Ä___È________ _¼___±_¼____. _Ñ_¡â_µ__ __â_¡â___¼ __â __Ü_á_____¡ ãÄ___¼ _ü_ü _À_µÛ_µ_____üâ __ _À_ü___¼ _____±___«__ÜÉ Ä_á_È____ _¼_____¡___«____ `splice_to_freelist`.> _£ â______ _À___«É___«_¡ _«___¡ _ÀÛ_µ_ü__Ää_µâ___¡: _À____Üö_¡_µâ _±ÜâÛ___«_µ__â___ü_µ _ü Ä_ÀÛ__ä_¡_µâ ___µ___µÛ_¡ _ü _¼___«_¡. _÷ â__, _ü _«ÛÄ_____µ ___±_µ_À_µà_ü___¡_µâ __âÄââ___ü_µ__ _¼_____¡___« _À_µÛ_µ_____¡ _À__âÛ___µ____ÜÉ Ä_á_È____ _À__ __â_«_µ_ÈÎ____â_ü.> _Õ â_µ_¼Ää_µ__ Û_µ_¡_È_ü_á_¡ _ü_ü _ _µã_¡_È_¡-5ëÈ ãÄ___¼ _ü_ü Û_¡_ÀÛ_µ_«_µ_È_µ___ü _À_¡__â_ü _____á__Û_¡ä_¡_â _±Ä_È_µ___¼___µ _á___¡à_µ___ü_µ: _üâ_ü__Ä, _µ_È_ü _À_¡__âÎ Û_¡_ÀÛ_µ_«_µ_È_üâÎ Ä_«_¡_È__Î, _ü _È___¦Î, _µ_È_ü _À_¡__â_ü ___¼_¡_á_¡_È__Î ___µ_«__â_¡â__à____. _Õ _¼__ÛÛ_µ_¼â__ÜÉ _ÀÛ____Û_¡_____¡É _üâ_ü___¡ _____á__Û_¡ä_¡_µâ ___µ___«_¡, _¡ _á___¡à_üâ, ___µ___«_¡ __Ü_À___È___µâ _ü_á_±Üâ__à___¡ _ÀÛ_____µÛ_¼_¡. _¥_È_ü ___¡ â_¡_«_ü_ü Û_¡_ÀÛ_µ_«_µ_È_µ___ü _À_¡__â_ü _À_¡__â_ü ___¼_¡_á_¡_È__Î ___µ_«__â_¡â__à____, â__ ãÄ___¼ _ü _____á__Û_¡ä_¡_µâ _ÀÛ_ü_á___¡_¼ __ö_ü_±_¼_ü `cNoMemory`, __ __â___µâ ___¡ _¼__â__ÛÜ__ Û_¡__â_¡____ __Ü_____«_üâ _«_¡___À _ü _¡___¡Û_ü______ __â_¡___¡___È_ü___¡_µâ _ÀÛ____Û_¡____Ä.> _Õ _ _µã_¡_È_µ-05 ___À_µÛ_¡ _ü_ü Û_¡_ÀÛ_µ_«_µ_È_µ___ü _À_¡__â_ü __Ü_À___È___â ___µ___«_¡ Ä_À_µö____ ÛÓ __ API ___µ _ÀÛ_µ_«Ä____âÛ_µ____ ___ü_¼_¡_¼____ _ÀÛ_____µÛ_¼_ü ___¡ ___µ_«__â_¡â___¼ _À_¡__â_ü. _¥_È_ü ___À_µÛ_¡ _ü _¡_È_È___¼_¡ _ü_ü _À_¡__âÎ __Ü_«_µ_È_üâÎ ___µ _______È_¡, â__ _____¡ _¡___¡ _ÀÛ_µÛÜ___¡_µâ _ÀÛ____Û_¡____Ä  __Ü_____«____ _¡___¡Û_ü__________ _«_¡___À_¡.> __â__â _À___«É___« Û_¡_áÄ______ _À_µÛ_µ___µâ_ü ___¡ _ _µã_¡_È-5ëÈ. __â_¡___¡___È_ü___¡âÎ _È_ü _ÀÛ____Û_¡____Ä __ _¡______ ãÄ___¼ _ü_ü Û_¡_ÀÛ_µ_«_µ_È_µ___ü _ü_È_ü _ü_À___ÈÎ_á_____¡âÎ ___µ_È___¼_¡_ÈÎ__Ü__ _À_µÛ_µÉ___« (_ü_¼_È_à_µ___ü_µ _ü_È_ü `longjmp()`) ÛÓ _È_µ_«Ä_µâ Û_µö_üâÎ._Õå _ _µã_¡_È_µ-05 _ÀÛ_ü __ö_ü_±_¼_µ __â___¦_«_µâ___È_µ___ü _üå ___µ_«__â_¡â_¼_µ _À_¡__â_ü _ÀÛ____Û_¡_____¡ _¡___¡Û_ü______ __â_¡___¡___È_ü___¡_µâ. _ _µã_¡_È-5ëÈ ___µå _____¦_µâ _À___á_____È_üâÎ _µ_±_µ â_¡_¼Ä_ Û___¼__öÎå ÛÓ __å _ÈÄà_¡_µ _¡___¡Û_ü________ _üâÄ_¡ _ü_ü _ÀÛ____Û_¡_____¡ _____¦_µâ _ÀÛ___«___È_¦_¡âÎ __Ü_À___È__âÎ, ___¡_ÀÛ_ü___µÛ, _µ_È_ü _ÀÛ____Û_¡_____¡ _ü___µ_µâ ___µ_¼___ÈÎ_¼__ _«_____µ______ _ü_È_ü ___µ_¼___ÈÎ_¼__ ___üÛâÄ_¡_ÈÎ__ÜÉ ___¡ö_ü__. __¡_ÀÛ_ü___µÛ, _____¦_µâ _±ÜâÎ Û_µ_¡_È_ü_á_____¡___¡ ãÄ___¼ _ü-_À_µ__à___ü _¡, _¼__â__Û_¡ _á_¡_ÀÄ_¼_¡_µâ _«ÛÄ__Ä_ ãÄ___¼ _ü_ _üå _____á__Û_¡ä_¡_µâ _È_ü_±__ Û_µ_áÄ_ÈÎâ_¡â Û_¡_±__âÜ, _È_ü_±__ _ÀÛ_ü_á___¡_¼ __ö_ü_±_¼_ü._Ù__â____Ä __â_¡___¡___È_ü___¡âÎ _ÀÛ____Û_¡____Ä ___µ_ÈÎ_á, __Ä_¦____ _¼_¡_¼ ___ü___ü__Ä__ _«_µ_È_¡âÎ ___µ_È___¼_¡_ÈÎ__Ü__ _À_µÛ_µÉ___« (_ü_¼_È_à_µ___ü_µ _ü_È_ü `longjmp()`). _ _¡_ÀÛ_µ_«_µ_È_µ___ü_µ _À_¡__â_ü _____¦_µâ __Ü_À___È__âÎ _üå _ü_áå _À___ÈÎ_á_____¡â_µ_ÈÎ_¼_üÉ ___¡â_ü____ÜÉ ãÄ___¼ _ü__. _å â_¡__ ______Äâ Û_¡_ÀÛ_µ_«_µ_ÈâÎ Û_µÄÛÜ, âÛ_µ_±Ä_ä_ü_µ _«_¡_ÈÎ___µ__ö_µ____ _______±___¦_«_µ___ü (___¡_ÀÛ_ü___µÛ, _À_¡__âÎ _ü_È_ü __â_¼ÛÜâÜ_µ ã_¡___ÈÜ)._¥_È_ü _ü_À___ÈÎ_á_____¡âÎ _ü_¼_È_à_µ___ü _üå RAII-Û_µÄÛÜ, â__å ___¡å _À_µÛ__Ü__ ___á___È_« _ÀÛ___±_È_µ__ ___µâ. __Û_____µ _ÈÄà_¡ _ü_À___ÈÎ_á_____¡___ü Û_¡_á__ÜÉ DLL å Û_¡_á__Ü___ü Û_¡__â_¡_____¡___ü _Á_ü++._Ù__â____Ä _À__-_ÀÛ_µ_¦___µ__Ä __â_¡_â __Ä_¦__Ü API-ãÄ___¼ _ü_ü ___á_«_¡___ü _È_µ___µ__â____, _¼__â__ÛÜ_µ ______Äâ _____á__Û_¡ä_¡âÎ _ÀÛ_ü_á___¡_¼ Ä_À_µö____â_ü. _å â_¡_¼_¦_µ __Ä_¦____ __ÉÛ_¡___üâÎ `return Û_NO_MEMORY;`. __â_¡â_ü, _ÀÛ_ü_á___¡_¼ Ä_À_µö____â_ü ___µå ___±_á_¡â_µ_ÈÎ____ _«___È_¦_µ__ _±ÜâÎ _¼___«____ _____á__Û_¡â_¡. __Ä___¼ _ü _____¦_µâ _____á__Û_¡ä_¡âÎ `void`, ____ _ÀÛ_ü â____ __å API _____¦_µâ _±ÜâÎ _üå ãÄ___¼ _ü å _ü___µ___µ__ __Û___«_µ `bool alloc_failed()`, _¼__â__Û_¡ _____á__Û_¡ä_¡_µâ `true`, _µ_È_ü ___«_ü__ _ü_áå _ÀÛ_µ_«Ü_«Ää_üÉ __Ü_á________ _À_¡__âÎ __Ü_«_µ_È_üâÎ ___µå ______._¢_¡_¼_ü__ ___±Û_¡_á____, _____¦____ __Ü_«_µ_È_üâÎ _«___µ _À___«_á_¡_«_¡à_ü:* [ ] _ _µ_¡_È_ü_á_¡ _ü API _±_µ_á__â_¼_¡_á________ __Ü_«_µ_È_µ___ü _À_¡__â_ü.* [ ] _Ò_µ___µÛ_¡ _ü _¼___«_¡, Äà_üâÜ___¡_ä_¡ _À___È_µ_«_____¡â_µ_ÈÎ_____µ Û_¡_ÀÛ_µ_«_µ_È_µ___ü_µ Ä_á_È____.
__label__nan	For news and announcements from and about Google
__label__Customer Experience	"Now it's easy to accidentally hit the Delete or Duplicate hotspots in the Canvas page sorter, in part because it's part of the mini slide (clicking on the entire box is designed mostly to switch to that page) and in part because the hotspots are active before they show up, and the user may have initiated the clicking before the appearance of the hotspots would have a chance of canceling the click action (users may frustratingly witness the click + hotspot collision as ""their own failing"")![deletepage](https://user-images.githubusercontent.com/1548516/53482353-4f778000-3a7f-11e9-88f9-87c106d31279.gif)Not sure what the ideal solution would be, though disabling the fade-in transition would help (controls to show up immediately), and it would not be a deep change."
__label__nan	There must be a new contact created on HubSpot.
__label__cloud	c4f4t0r: will approach differ for cloud or premises or hybrid environment?
__label__DevOps, Cloud	"Greetings, I figured I would try to implement this on this [aws-example](https://github.com/aws-samples/aws-amplify-graphql). After installing and running it successfully, I followed the steps in this repo.- Unauthenticated role which is accepted:```json{""Version"": ""2012-10-17"",""Statement"": [{""Effect"": ""Allow"",""Action"": [""appsync:GraphQL""],""Resource"": [""arn:aws:appsync:eu-west-1:073051392232:apis/gucbk3owp5h4dihdqd2qdiqbwa/types/Query/fields/listPictures""]}]}```""Authenticated role"" throws following error```An error occurredYour request has a problem. Please see the following details.The policy failed legacy parsing ```When trying this json:```json{""Version"": ""2012-10-17"",""Statement"": [{""Effect"": ""Allow"",""Action"": [""appsync:GraphQL""],""Resource"": [""arn:aws:appsync:eu-west-1:073051392232:apis/gucbk3owp5h4dihdqd2qdiqbwa/types/Query/fields/listPictures"",""arn:aws:appsync:eu-west-1:073051392232:apis/gucbk3owp5h4dihdqd2qdiqbwa/types/Mutation/fields/createPicture""]}]}```Searched around on stackoverflow and web but cant find what the proper format should be to fix the parsing error.```json{""Version"": ""2012-10-17"",""Statement"": [{""Effect"": ""Allow"",""Action"": [""appsync:GraphQL""],""Resource"": [""arn:aws:appsync:eu-west-1:073051392232:apis/gucbk3owp5h4dihdqd2qdiqbwa/types/Query/fields/listPictures"",""arn:aws:appsync:eu-west-1:073051392232:apis/gucbk3owp5h4dihdqd2qdiqbwa/types/Mutation/fields/createPicture""]}]}```Any guidance would be greatly appreciated. Thank you."
__label__cloud	"<p>Check the Google Cloud Status Dashboard if there are service interruptions for the day that your system encountered the issue.</p><p><a href=""https://status.cloud.google.com/"" rel=""nofollow noreferrer"">https://status.cloud.google.com/</a></p>"
__label__cloud	"If you think there is a bug in Cloud Functions, you should <a href=""https://support.google.com/firebase/contact/support?page=bug_or_feature"" rel=""nofollow noreferrer"">file a bug report</a> for that. But I can assure you that the times on Cloud Functions are not off by four hours. That would be critically bad. What you are seeing is likely just a difference in timezone between your machine and Cloud Functions."
__label__nan	
__label__nan	No petitions, surveys, or crowdfunding
__label__AI	This video is the product of Dessa Engineers, Hashiam Kadhim, Joseph Palermo,and Rayhane Mama. The Engineers used artificial intelligence to recreate Joe RoganÈs voice,generating the most human-like voice synthesis to date. The audio you arelistening to is 100% generated from the artificial intelligence model. Themodel even learned to generate breaths and mouthing sounds where it sees fitin order to make the speech sound most natural. Find out more about thisproject in this blog post (linked below), or check out www.fakejoerogan.com tosee if you can beat our AI model! If you have any other questions please reach out to real.talk@dessa.com Blog: [https://medium.com/@dessa_/real-talk-...](/redirect?redir_token=1wN8IgXCJCvxXsdIj51vF6VvIth8MTU1ODExMDQ4NUAxNTU4MDI0MDg1&q=https%3A%2F%2Fmedium.com%2F%40dessa_%2Freal-talk-speech-synthesis-5dd0897eef7f&v=DWK_iYBl8cA&event=video_description) Learn more about Dessa here:[https://dessa.com/](/redirect?redir_token=1wN8IgXCJCvxXsdIj51vF6VvIth8MTU1ODExMDQ4NUAxNTU4MDI0MDg1&q=https%3A%2F%2Fdessa.com%2F&v=DWK_iYBl8cA&event=video_description) Please note that this project does not suggest that we endorse the views andopinions of Joe Rogan. Joe was selected as a demonstrative model for thepurposes of displaying the capability of this technology.
__label__DevOps	"The puzzle `465-ed5b16ac` from #465 has to be resolved: https://github.com/fidals/stroyprombeton/blob/ae45c60f5dd7dfabfe5c10a4f00c6613cfc2bf7f/stroyprombeton/views/catalog.py#L140-L141The puzzle was created by duker33 on 27-Feb-19. If you have any technical questions, don't ask me, submit new tickets instead. The task will be \""done\"" when the problem is fixed and the text of the puzzle is _removed_ from the source code. Here is more about [PDD](http://www.yegor256.com/2009/03/04/pdd.html) and [about me](http://www.yegor256.com/2017/04/05/pdd-in-action.html)."
__label__cloud	"<p>There is no such thing as ""Firebase functions"". What you're referring to is Cloud Functions with Firebase tools layered on top of it.</p><p>You can connect Cloud Functions (with Firebase or not) to an external database as long as you're on the Blaze payment plan, which allows you to make outgoing connections to some third party service.</p>"
__label__nan	See [here](https://github.com/Luolc/AdaBound).
__label__AI	# Must-read books to learn Artificial Intelligence in 2019We have reviewed the top 5 best Artificial Intelligence books available on theInternet. And to be honest, these books were _really_ hard to find. Betweenthe A.I conspiracy books and the how to make money off A.I books, therewas really wasnÈt much left to choose from. These resources are weighted basedoff trusted community reviews and the quality of the content itself. Becausewhy waste your time on bad content? You wonÈt ever truly understand the fieldof Artificial Intelligence, nor will you be able to even apply it very well.These books will cover topics like Neural Networks, MathematicalOptimizations, Logic, Probability, and Economics  which are all _extremely_useful in todayÈs modern world.### 1\. Artificial Intelligence: A Modern Approach[Artificial Intelligence: A ModernApproach](https://www.amazon.com/gp/product/9332543518/ref=as_li_tl?ie=UTF8&tag=zeroequalsfal-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=9332543518&linkId=d01c6c70480b5bb97712ea8d4f488ded)provides AI algorithm techniques in-detail, from pathfinding to intelligent AIAgent design. If you are looking for one of the best books on A.I, then thisis surely a top pick. There is detailed information on building Agents, graphalgorithms incl. A* Search, and how to navigate in areas of uncertainty. Greatbook with lots of content and examples.### 2\. Deep Learning[DeepLearning](https://www.amazon.com/gp/product/0262035618/ref=as_li_qf_asin_il_tl?ie=UTF8&tag=zeroequalsfal-20&creative=9325&linkCode=as2&creativeASIN=0262035618&linkId=84568d05237b805c578061aed460c18f)is written by a famous ex-Googler, providing a rich and detailed guide intoone of A.IÈs most exciting sub-fields, Machine Learning. In this book youwill learn about Neural Networks and how to construct them for various use-cases. ItÈs been backed by our industry thought-leaders such as Elon Musk whohas commented on how comphresive this book truly is.### 3\. Pattern Recognition and Machine Learning (Information Science andStatistics)[Pattern Recognition and Machine Learning (Information Science andStatistics)](https://www.amazon.com/gp/product/0387310738/ref=as_li_qf_asin_il_tl?ie=UTF8&tag=zeroequalsfal-20&creative=9325&linkCode=as2&creativeASIN=0387310738&linkId=af136812be38f404cda557c135deaabc)is a speciality book on the field of pattern recognition. This is a no bs*book that covers scientific topics such as Bayesian methods to build A.Iagents. It is a truly an outstanding book for itÈs time, and first publishedback in 2006.### 4\. Deep Learning with Python[Deep Learning withPython](https://www.amazon.com/gp/product/1617294438/ref=as_li_qf_asin_il_tl?ie=UTF8&tag=zeroequalsfal-20&creative=9325&linkCode=as2&creativeASIN=1617294438&linkId=e875897693499f3ed3137e2cb2c10493)combines Deep Learning techniques together with the Python programminglanguage. Python is generally the preferred language for building AI models as it is highly recognised by many large companies and it supports someexceptional A.I libraries such as Tensorflow to construct A.I agents. Thisbook will get you up to speed with building A.I using Deep Learning. Priorknowledge of Python may be advised.### 5\. The Elements of Statistical Learning: Data Mining, Inference, andPrediction, Second Edition (Springer Series in Statistics)[The Elements of Statistical Learning: Data Mining, Inference, and Prediction,Second Edition (Springer Series inStatistics)](https://www.amazon.com/gp/product/0387848576/ref=as_li_qf_asin_il_tl?ie=UTF8&tag=zeroequalsfal-20&creative=9325&linkCode=as2&creativeASIN=0387848576&linkId=514d719391cc3e1ddb4c8e1ebefeb8c0)might be one of the best books to gain a solid foundation of statistics, whichreally is the backbone of many A.I based applications. Stats helps to drivethe decision-making process of AI such that smart decisions are made. Thisbook is comphresive and covers Data Mining, Inference, and Prediction  allrelevant and highly applicable today.Thanks for reading!
__label__DevOps	"Love the slides! I thought helpful additions here would be to talk about R types (ints, numerics, strings, etc) and data structured (lists, named lists, etc), and cover the stringr package a bit. also, the concept of ""vectorization"" which is so pervasive in R. sqrt(c(1,2,3,4,5)) etc."
__label__AI	"The importance of Artificial Intelligence is often understated and alsooverstating the same is quite difficult.. to get in-depth with how AI isactually bringing changes in MCA, we must get to the basics of what MedicalBilling and Coding really are.### MEDICAL CODINGMedical coding, if we talk about it at a very basic level -- is something thata coder takes, a written piece if you may, and translates it as accurately aspossible into a coded format such as numeric or alphanumeric code. The piecethat's taken for translation can be something such as a prescription formedication or a doctor's diagnosis or something else medical related. A codefor each and every event is created, these events can be of injuries,diagnosis or medical procedures.Presently, there are about a hundred thousand codes existing that are used formedical procedures, outpatient procedures, and diagnoses. Let us have a lookat a simple example of Medical Coding:Let's say that a patient has walked into a doctorÈs office and he/she iscoughing tirelessly, they have a high production of mucus, and have a dreadedfever. Then a nurse walks up to the patient and asks them their symptoms, oncethe symptoms are noted, she performs some initial tests to get an idea of whatis actually going on, and then comes the doctor who analyzes and concludes thediagnoses saying that the patient is suffering from bronchitis. A medicationis then prescribed to the patient by the doctor.Now comes the interesting part of Modern Healthcare, each and every part ofthe visit is recorded by the clinic be it the doctor or someone in the officewho is authorized to carry out such operations. Then begins the coders jobthat is to translate all of relevant information of the visit into numeric andalphanumeric codes, which are ultimately used in the billing process.The medical coder should be equipped with the knowledge of a few sets andsubsets of code, let's take two of the subsets: International Classificationof Diseases (ICD) these codes correspond to a patientÈs injury or sickness,and Current Procedure Terminology (CPT) that are related to the functions andservices Healthcare providers perform to the patient this can be as performingon them and performing for them.A task included for the Medical Coders is to translate every bit of data orinformation of the patient's visit to the clinic and shape it in the form of acode. There are different codes for different kinds of visits, some codes aremore specific these specific codes can be such as the patients symptoms, thetests performed by the doctor and the diagnosis procedure used by the doctor.The Medical Coders have to keep these guidelines in mind, they are veryimportant and can affect the status of a claim. The coding process concludeswhen the Medical Coder has entered the proper codes into the for or softwareprogram. This is where the job of the Medical Coder ends, now, all of this ispassed on to the Medical Biller.### MEDICAL BILLINGThe Medical Biller more so acts as a middleman between patients, healthcareproviders and the insurance companies. Their job, is on similar lines incontext to the Medical Coder. The Medical Biller translates the codes given bythe Medical Coder into a financial report, they make sure that the HealthcareProvider has been reimbursed appropriately for the services they've provided.Do not be fooled by the simplicity of the term ""Medical Billing"", it may seemthat all the Medical Billers task is to make a bill (Commonly known as a'Claim') for the insurance company by the help of the information provided bythe Medical Coder, the reality of the process is not as simple.Continuing with the previous example, the Medical Biller now looks at thecodes, that consist of information of things such as the kind of visit, thesymptoms, the diagnosis of the doctor, the medication prescribed by the doctorand then creates a Claim. The Claim is then sent and evaluated by theinsurance company, and then returned back. The bill of the patient is thenmade by biller who carefully re-evaluates the returned claim after theinsurance is removed, all of these tasks are performed through a form orsoftware, they're called as [Medical Billing SoftwareSolutions](https://www.osplabs.com/medical-billing-solutions/) and there's alarge number of companies out there creating such solutions.The biller takes a few factors into account such as the insurance plans of thepatient into account while creating the bill, this ensures that an accuratebill is produced. In cases where the patient shows signs refraining the billpayment, the Medical Biller has to take appropriate steps to ensure healthcareprovider is properly compensated.# Let's take a look at how a Traditional Medical Billing & Coding ProcessFlows:Talking about the word 'Traditional' you must have got an idea that itrequires **ALOT** of manual documentation and paper work, the average time fora traditional coding and billing process stretched on to about 5-7 weekswhereas in the modern automated system the process is reduced to as low as 2weeks.Following is a Claim-to-Payment Chase while using a traditional Paper-BasedSystem.1. Patient visits the doctorÈs office.2. Patient check-in, gets treated.3. Doctor or their assistant writes a superbill for the treatment.4. The Medical Coder writes codes for the treatment.5. Medical Billers receive Paper forms who then format the data and forward it to insurance payers.6. Payer generates check and send payment to the provider.Now the point where AI fits into the story is to enhance the efficiency andefficacy of the billing and coding process. _Computer Assisted Coding (CAC) isa technology that works on the concept of Machine Learning (ML) which is abranch of Artificial Intelligence (AI) and Natural Language Processing (NLP),_they provide automated assistance to the rigorous task of identifying andextracting data from the given documents and inserting it into the system."
__label__AI	Learning to drive can be stressful. And if you happen to get paired with animpatient teacher or crazed family member it can be a nightmare. Along withthe stress, you might not learn enough to pass the driverÈs test.In an attempt to make students more at ease and help them learn as efficientlyas possible, one driving school in China is taking out the human factoraltogether. A driving school in Zhenjiang has installed AI coaches in thevehicles. The artificial intelligence, which lives in an interactive screenattached to the dashboard, provides automatic voice navigation and will givedrivers unique commands.For example, the robot voice will remind drivers to turn on their signal asthe car approaches a turn or to slow down if they pick up too much speed.![AI Driving Coach Robot Artificial intelligence -YellRobot](https://yellrobot.com/wp-content/uploads/2019/03/AIDriver2-1024x570.jpeg)credit: Pear Video# Robot Driving Coach Keeps Students CalmThe artificial intelligence will also take control of the car if need be. Ifthe car gets closer than 8 inches to an obstacle, it will automatically stop.Students have remarked how the AI driving coach seems a bit more pleasant thantheir human counterparts.IÈm nervous when a real coach sits beside me. This robot is nicer. Even if Ido something wrong, the robot will continue to encourage me, instead ofblaming me, said one student driver said.The robot can be set to a teaching mode or simulated test mode in which the AIwill let the driver know if they passed or not. The AI driving coach has nothit the busy streets of China yet as it seems limited to a closed course. Somestudents donÈt feel something like this would be effective in a realenvironment.No robot coach would help me step on the brakes if I were accidentally todrive into a ditch, one student commented._Sources:[Pear Video](https://www.pearvideo.com/video_1530905) /[ECNS](http://www.ecns.cn/news/2019-03-19/detail-ifzfmzhu2193194.shtml)_* * *Check out our articles on [AI helping to keep roadssafe](https://yellrobot.com/ai-road-management-system-weathernews/) and Tokyo2020 [Olympics using robots](https://yellrobot.com/robots-exoskeletons-tokyo-2020-olympic-panasonic-toyota/)
__label__DevOps	We ended up with a regression where the targets are always linking every time you execute make. This isn't a big deal on Linux as its fast but on Windows this is terrible.
__label__cloud	Hii. i stored a file in firebase cloud storage and i want to read or download that file. I tried what you said and deployed that cloud function. But i didn&#39;t get any data from that file and i didn&#39;t see any directory what i created in cloud functions folder . where it is created and located?
__label__nan	The goal of /r/tech is to provide a space dedicated to the intelligentdiscussion of innovations and changes to technology in our ever changingworld. We focus on high quality news articles about technology and informativeand thought provoking self posts.
__label__cloud	"<p>See <a href=""https://firebase.google.com/docs/hosting/functions"" rel=""nofollow noreferrer"">https://firebase.google.com/docs/hosting/functions</a> .</p><blockquote><p>Cloud Functions for Firebase lets you automatically run backend code in response to HTTPS requests. Your code is stored in Google's cloud and runs in a managed environment. There's no need to manage and scale your own servers.</p><p>For example use cases and samples for Cloud Functions integrated with Firebase Hosting, visit our serverless overview.</p></blockquote>"
__label__nan	No petitions, surveys, or crowdfunding
__label__cloud	"<p>i am trying to publish data to Cloud Pub Sub .Data is in JSON format and is being kept in my local folder. I am not using Cloud Storage and trying to read the pubsub message directly through cloud function. Tested the flow with manually passing messages and the data is getting inserted into Bigquery tables also. Only place i got stuck is, how will i pass a .txt file JSON dataset to Cloud PubSub</p><p>Sample data{""ID"":6,""NAME"":""Komal"",""AGE"":22,""ADDRESS"":""Assam"",""SALARY"":20000}</p><p>Can any one give me a hint!!</p><p>I could see various options using cloud storage and all, here i am reading the changed data from DB table,insert those records into 1 dummy table and converting the data from that table to JSON format and writing to a .txt file. From here if i could publish the data to pubsub , entire flow will get completed</p><p>If i manually pass like below , the data will get inserted</p><p>gcloud pubsub topics publish pubsubtopic1 --message {""ID"":6,""NAME"":""Komal"",""AGE"":22,""ADDRESS"":""Assam"",""SALARY"":20000}</p><p>Thanks In Advance</p>"
__label__DevOps	I am on a Mac and MuJoCo by itself and every other part of the demo code in the readme works except these lines of code and their following errors:**mj.set(m, :opt, :timestep, -0.002)**ERROR: MethodError: no method matching update_ptr(::Base.RefValue{mjModel}, ::UInt64, ::Float64)Closest candidates are:update_ptr(::Ptr, ::Integer, ::Float64) at /Users/powers/.julia/packages/MuJoCo/GRTFJ/src/mjextra.jl:152update_ptr(::Ptr, ::Integer, ::Integer) at /Users/powers/.julia/packages/MuJoCo/GRTFJ/src/mjextra.jl:149update_ptr(::Ptr, ::Integer, ::StaticArrays.SArray{Tuple{S},T,1,S} where T where S) at /Users/powers/.julia/packages/MuJoCo/GRTFJ/src/mjextra.jl:155Stacktrace:[1] set(::jlModel, ::Symbol, ::Symbol, ::Float64) at /Users/powers/.julia/packages/MuJoCo/GRTFJ/src/mjextra.jl:204[2] top-level scope at none:0**m.m[].opt.timestep = 0.002**ERROR: setfield! immutable struct of type mjOption cannot be changedStacktrace:[1] setproperty!(::mjOption, ::Symbol, ::Float64) at ./sysimg.jl:19[2] top-level scope at none:0**d.qpos[:] = rand(nq)**ERROR: UndefVarError: nq not definedStacktrace:[1] top-level scope at none:0
__label__nan	
__label__DevOps	https://github.com/python/python-docs-fr/issues/602see issue above and black. So we can add it to `python-docs-fr` travis
__label__nan	- [ ] ¾Óø¾Î_á KeyÛâ- [ ] ¾Óø¾Îö ¼Çö ¾__ ShareÛâ- [ ] ¾Óø¾Î¬â__ BookÛâ
__label__Frontend tools	http://blog.yangyong.io/2019/02/27/html/textarea/ JavaScript¾÷øüÐ¥Îü_ÂÂ¼Î´__ãø_¬Û_Ù÷
__label__nan	## Paste the link of the GitHub organisation below and submithttps://github.com/flexport---###### Please subscribe to this thread to get notified when a new repository is created
__label__nan	
__label__AI	Bullying has always been a problem in schools. Over the last few years, itÈsfinally getting taken seriously and given proper attention.Research has shown that when adults respond quickly, bullying behavior can bestopped over time. Unfortunately, teachers donÈt always see the signs untilitÈs too late. One school district in Japan is attempting to give teachers andofficials [a powerfulally](http://mainichi.jp/english/articles/20190323/p2g/00m/0na/063000c) incombating the problem.## Otsu Partners with Hitachi to Analyze 9000 Cases of BullyingThe western Japanese city of Otsu recently signed an accord with[Hitachi](http://www.hitachi.us/) to collaborate on a project. They plan touse artificial intelligence and machine learning to help detect signs andpatterns of aggression.AI will analyze 9000 cases of bullying that occurred in local elementary andjunior high schools over the past 6 years. It will look at things such asgrade, gender, timing, location, number of students involved and academicrecords.The goal of the project is to help teachers detect characteristics and earlywarning signs. Often minor issues between students can lead to biggerproblems. If signs are recognized early enough, teachers can squash bullyingbefore it occurs.Local schools are expected act firmly against (bullying) without solely beingdependent on teachersÈ experience, by having AI theoretically analyze pastdata. said Otsu Mayor Naomi Koshi.### Otsu Mobilized by TragedyThe school board expects the analysis to be completed by October. Otsusuffered tragedy in 2011 when a 13 year-old boy committed suicide. After a twoyear investigation, it was determined to have been caused by bullying. Sincethen, the city has required each school to report bullying cases within 24hours.ItÈs always good to see artificial intelligence being used for good. If AI canhelp stop even one case of bullying then the project would be a success. Welook forward to more schools using technology to combat aggression and helpstudents.* * *Check out our articles on [AI making policydecisions](https://yellrobot.com/artificial-intelligence-politics-decisions-europe-ai/) and a [gourmet coffee robot.](https://yellrobot.com/briggo-coffee-robot-coffee-haus/)
__label__DevOps	I can see there is toolchain for aarch64 arm device, so we can build in RPiB3 with 64-bit OS. How about raspbian os which has armv7l. How can I build in such device. Thanks.
__label__nan	No petitions, surveys, or crowdfunding
__label__AI	The approach is related to traditional simulation, but with criticaldifferences. A simulation is essentially assumption-driven, Schawinski said.The approach is to say, I think I know what the underlying physical laws arethat give rise to everything that I see in the system.È So I have a recipe forstar formation, I have a recipe for how dark matter behaves, and so on. I putall of my hypotheses in there, and I let the simulation run. And then I ask:Does that look like reality? What heÈs done with generative modeling, hesaid, is in some sense, exactly the opposite of a simulation. We donÈt knowanything; we donÈt want to assume anything. We want the data itself to tell uswhat might be going on.The apparent success of generative modeling in a study like this obviouslydoesnÈt mean that astronomers and graduate students have been made redundant but it appears to represent a shift in the degree to which learning aboutastrophysical objects and processes can be achieved by an artificial systemthat has little more at its electronic fingertips than a vast pool of data.ItÈs not fully automated science  but it demonstrates that weÈre capable ofat least in part building the tools that make the process of scienceautomatic, Schawinski said.Generative modeling is clearly powerful, but whether it truly represents a newapproach to science is open to debate. For [DavidHogg](https://cosmo.nyu.edu/hogg/), a cosmologist at New York University andthe Flatiron Institute (which, like _Quanta_ , is funded by the SimonsFoundation), the technique is impressive but ultimately just a verysophisticated way of extracting patterns from data  which is what astronomershave been doing for centuries. In other words, itÈs an advanced form ofobservation plus analysis. HoggÈs own work, like SchawinskiÈs, leans heavilyon AI; heÈs been using neural networks to [classifystars](https://arxiv.org/pdf/1711.08793.pdf) according to their spectra and to[infer other physical attributes](https://arxiv.org/pdf/1603.03040.pdf) ofstars using data-driven models. But he sees his work, as well as SchawinskiÈs,as tried-and-true science. I donÈt think itÈs a third way, he said recently.I just think we as a community are becoming far more sophisticated about howwe use the data. In particular, we are getting much better at comparing datato data. But in my view, my work is still squarely in the observational mode.## Hardworking AssistantsWhether theyÈre conceptually novel or not, itÈs clear that AI and neuralnetworks have come to play a critical role in contemporary astronomy andphysics research. At the Heidelberg Institute for Theoretical Studies, thephysicist [KaiPolsterer](https://www.iau.org/administration/membership/individual/16830/)heads the astroinformatics group  a team of researchers focused on new, data-centered methods of doing astrophysics. Recently, theyÈve been using amachine-learning algorithm to [extract redshift information from galaxy datasets](https://www.aanda.org/articles/aa/pdf/2018/01/aa31326-17.pdf), apreviously arduous task.Polsterer sees these new AI-based systems as hardworking assistants that cancomb through data for hours on end without getting bored or complaining aboutthe working conditions. These systems can do all the tedious grunt work, hesaid, leaving you to do the cool, interesting science on your own.But theyÈre not perfect. In particular, Polsterer cautions, the algorithms canonly do what theyÈve been trained to do. The system is agnostic regardingthe input. Give it a galaxy, and the software can estimate its redshift andits age  but feed that same system a selfie, or a picture of a rotting fish,and it will output a (very wrong) age for that, too. In the end, oversight bya human scientist remains essential, he said. It comes back to you, theresearcher. YouÈre the one in charge of doing the interpretation.For his part, Nord, at Fermilab, cautions that itÈs crucial that neuralnetworks deliver not only results, but also error bars to go along with them,as every undergraduate is trained to do. In science, if you make a measurementand donÈt report an estimate of the associated error, no one will take theresults seriously, he said.Like many AI researchers, Nord is also concerned about the impenetrability ofresults produced by neural networks; often, a system delivers an answerwithout offering a clear picture of how that result was obtained.Yet not everyone feels that a lack of transparency is necessarily a problem.[Lenka Zdeborov](http://artax.karlin.mff.cuni.cz/~zdebl9am/), a researcher atthe Institute of Theoretical Physics at CEA Saclay in France, points out thathuman intuitions are often equally impenetrable. You look at a photograph andinstantly recognize a cat  but you donÈt know how you know, she said. Yourown brain is in some sense a black box.ItÈs not only astrophysicists and cosmologists who are migrating toward AI-fueled, data-driven science. Quantum physicists like [RogerMelko](https://uwaterloo.ca/physics-astronomy/people-profiles/roger-melko) ofthe Perimeter Institute for Theoretical Physics and the University of Waterlooin Ontario have used neural networks to solve some of the toughest and mostimportant problems in that field, such as [how to represent the mathematicalwave function](https://arxiv.org/pdf/1812.09329.pdf) describing a many-particle system. AI is essential because of what Melko calls the exponentialcurse of dimensionality. That is, the possibilities for the form of a wavefunction grow exponentially with the number of particles in the system itdescribes. The difficulty is similar to trying to work out the best move in agame like chess or Go: You try to peer ahead to the next move, imagining whatyour opponent will play, and then choose the best response, but with eachmove, the number of possibilities proliferates.Of course, AI systems have mastered both of these games  chess, decades ago,and Go in 2016, when an AI system called[AlphaGo](https://www.theguardian.com/technology/2016/mar/15/googles-alphago-seals-4-1-victory-over-grandmaster-lee-sedol) defeated a top human player.They are similarly suited to problems in quantum physics, Melko says.## The Mind of the MachineWhether Schawinski is right in claiming that heÈs found a third way of doingscience, or whether, as Hogg says, itÈs merely traditional observation anddata analysis on steroids, itÈs clear AI is changing the flavor ofscientific discovery, and itÈs certainly accelerating it. How far will the AIrevolution go in science?Occasionally, grand claims are made regarding the achievements of a robo-scientist. A decade ago, an AI robot chemist named Adam investigated thegenome of bakerÈs yeast and worked out which genes are responsible for makingcertain amino acids. (Adam did this by observing strains of yeast that hadcertain genes missing, and comparing the results to the behavior of strainsthat had the genes.) _Wired_ Ès headline read, [Robot Makes ScientificDiscovery All by Itself](https://www.wired.com/2009/04/robotscientist/).More recently, Lee Cronin, a chemist at the University of Glasgow, has beenusing a robot [to randomly mixchemicals](https://www.wired.co.uk/article/robot-chemist-life-on-earth), tosee what sorts of new compounds are formed. Monitoring the reactions in real-time with a mass spectrometer, a nuclear magnetic resonance machine, and aninfrared spectrometer, the system eventually learned to predict whichcombinations would be the most reactive. Even if it doesnÈt lead to furtherdiscoveries, Cronin has said, the robotic system could allow chemists to speedup their research by about 90 percent.Last year, another team of scientists at ETH Zurich used neural networks to[deduce physical laws](https://arxiv.org/abs/1807.10300) from sets of data.Their system, a sort of robo-Kepler, rediscovered the heliocentric model ofthe solar system from records of the position of the sun and Mars in the sky,as seen from Earth, and figured out the law of conservation of momentum byobserving colliding balls. Since physical laws can often be expressed in morethan one way, the researchers wonder if the system might offer new ways perhaps simpler ways  of thinking about known laws.These are all examples of AI kick-starting the process of scientificdiscovery, though in every case, we can debate just how revolutionary the newapproach is. Perhaps most controversial is the question of how muchinformation can be gleaned from data alone  a pressing question in the age ofstupendously large (and growing) piles of it. In _The Book of Why_ (2018), thecomputer scientist Judea Pearl and the science writer Dana Mackenzie assertthat data are profoundly dumb. Questions about causality can never beanswered from data alone, they write. Anytime you see a paper or a studythat analyzes the data in a model-free way, you can be certain that the outputof the study will merely summarize, and perhaps transform, but not interpretthe data. Schawinski sympathizes with PearlÈs position, but he described theidea of working with data alone as a bit of a straw man. HeÈs neverclaimed to deduce cause and effect that way, he said. IÈm merely saying wecan do more with data than we often conventionally do.Another oft-heard argument is that science requires creativity, and that  atleast so far  we have no idea how to program that into a machine. (Simplytrying everything, like CroninÈs robo-chemist, doesnÈt seem especiallycreative.) Coming up with a theory, with reasoning, I think demandscreativity, Polsterer said. Every time you need creativity, you will need ahuman. And where does creativity come from? Polsterer suspects it is relatedto boredom  something that, he says, a machine cannot experience. To becreative, you have to dislike being bored. And I donÈt think a computer willever feel bored. On the other hand, words like creative and inspired haveoften been used to describe programs like Deep Blue and AlphaGo. And thestruggle to describe what goes on inside the mind of a machine is mirroredby the difficulty we have in probing our own thought processes.Schawinski recently left academia for the private sector; he now runs astartup called Modulos which employs a number of ETH scientists and, accordingto its website, works in the eye of the storm of developments in AI andmachine learning. Whatever obstacles may lie between current AI technologyand full-fledged artificial minds, he and other experts feel that machines arepoised to do more and more of the work of human scientists. Whether there is alimit remains to be seen.Will it be possible, in the foreseeable future, to build a machine that candiscover physics or mathematics that the brightest humans alive are not ableto do on their own, using biological hardware? Schawinski wonders. Will thefuture of science eventually necessarily be driven by machines that operate ona level that we can never reach? I donÈt know. ItÈs a good question.
__label__nan	"<p><code>firebase-functions</code> version 16.3.0, <a href=""https://firebase.google.com/support/release-notes/android"" rel=""nofollow noreferrer"">released 15 Mar 2019</a>, adds the ability to <a href=""https://firebase.google.com/docs/reference/android/com/google/firebase/functions/HttpsCallableReference"" rel=""nofollow noreferrer"">configure the timeout</a>.</p>"
__label__Containers	This way a secure connection to kopano_ical can be made quite trivial to setup.
__label__DevOps, Customer Experience	Add a modifier class to `.pf-c-page__main-section` that removes padding from the main section, to allow cards to sit on the edges of the viewport. This is necessary for the data table and data list demos.
__label__nan	There must be a new contact created on HubSpot.
__label__nan	This is basically a note to self:https://docs.microsoft.com/en-us/sysinternals/downloads/pstools
__label__AI	"<p>I want to train a language model using NLTK in python but i got into several problems.first of all i don't know why my words turn into just characters as i write something like this :</p><pre><code> s = ""Natural-language processing (NLP) is an area of computer science "" \""and artificial intelligence concerned with the interactions "" \""between computers and human (natural) languages.""s = s.lower();paddedLine = pad_both_ends(word_tokenize(s),n=2);train, vocab = padded_everygram_pipeline(2, paddedLine)print(list(vocab))lm = MLE(2);lm.fit(train,vocab)</code></pre><p>and the printed vocab is something like this that is clearly not correct(i don't want to work with characters!),this is part of output.:</p><pre><code>&lt;s&gt;', '&lt;', 's', '&gt;', '&lt;/s&gt;', '&lt;s&gt;', 'n', 'a', 't', 'u', 'r', 'a', 'l', '-', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', '&lt;/s&gt;', '&lt;s&gt;', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', '&lt;/s&gt;', '&lt;s&gt;', '(', '&lt;/s&gt;', '&lt;s&gt;', 'n', 'l', 'p', '&lt;/s&gt;', '&lt;s&gt;', ')', '&lt;/s&gt;'</code></pre><p>why my input turns into characters?i did this work in another way but with no luck :</p><pre><code>paddedLine = pad_both_ends(word_tokenize(s),n=2);#train, vocab = padded_everygram_pipeline(2, tokens)#train = everygrams(paddedLine,max_len = 2);train = ngrams(paddedLine,2);vocab = Vocabulary(paddedLine,unk_cutoff = 1);print(list(train))lm = MLE(2);lm.fit(train,vocab)</code></pre><p>when i run this code my train is absolute nothing,empty! it shows me ""[]"" !!wired thing is when i comment at this line from above code:</p><pre><code>vocab = Vocabulary(paddedLine,unk_cutoff = 1);</code></pre><p>now my train data is ok and something like this that is correct :</p><pre><code>[('&lt;s&gt;', 'natural-language'), ('natural-language', 'processing'), ('processing', '('), ('(', 'nlp'), ('nlp', ')'), (')', 'is'), ('is', 'an'), ('an', 'area'), ('area', 'of'), ('of', 'computer'), ('computer', 'science'), ('science', 'and'), ('and', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'concerned'), ('concerned', 'with'), ('with', 'the'), ('the', 'interactions'), ('interactions', 'between'), ('between', 'computers'), ('computers', 'and'), ('and', 'human'), ('human', '('), ('(', 'natural'), ('natural', ')'), (')', 'languages'), ('languages', '.'), ('.', '&lt;/s&gt;')]</code></pre><p>whats wrong with it? by the way i have to say that i'm not an expert in python or NLTK and its my first experience.next question is how can i use kneser-ney smoothing or add-one smoothing on training languge model? and am i doing language model training the right way?my trainig data is simple :</p><pre><code>""Natural-language processing (NLP) is an area of computer science "" \""and artificial intelligence concerned with the interactions "" \""between computers and human (natural) languages.""</code></pre><p>thanks.</p>"
__label__DevOps, Customer Experience	HidebugDefaultTargetPlatformOverride = TargetPlatform.fuchsia; // for desktop embedderwhy you make target fuchsia?and is fuchsia apps will work in the future as cross-platform apps from day one?
__label__AI	Artificial Intelligence (AI) is often confused with automation, yet the twoare fundamentally different. The key difference is that [ArtificialIntelligence](https://www.iafrikan.com/tag/artificial-intelligence) mimicshuman intelligence decisions and actions, while automation focuses onstreamlining repetitive, instructive tasks.Automation has been around for some time and is probably so integrated intomost business operations that itÈs not obvious  for example, the auto-generation of marketing e-mails and SMSs to customers and even customerstatements for specific periods. Automation saves time and money spent onmonotonous, voluminous tasks and gives employees an opportunity to applythemselves to more complex processes.## Understanding Machine Learning and Deep LearningAI deals with technologies, systems or even processes that competently mimichow human beings make decisions, react to new information, speak, hear, aswell as understand language. It helps to understand [MachineLearning](https://www.iafrikan.com/tag/machine-learning) (ML) as a subset ofAI. ML enables systems and processes to learn from data, identify patterns andrecommend decisions without human involvement.Deep learning on the other hand is defined as a subset of ML where artificialneural networks  algorithms built around the neural structure of the humanbrain  learn from data. The same way human beings learn from day-to-dayevents over time, a [deep learning algorithm executes functionsrepeatedly](https://www.iafrikan.com/2018/03/17/whats-real-and-whats-hype-in-artificial-intelligence-and-machine-learning/) and continuously learns andadjusts itself to improve accuracy. We call them deep learning algorithmsbecause the neural networks have various (deep) layers that enable learning ofcomplex patterns in large amounts of data.Take [FacebookÈs facial recognition applicationDeepFace](https://research.fb.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/) as an example.Facebook uses deep learning to analyse every photo I have ever been tagged into arrive at a set of features of my face, called a template. The algorithmdoes the same for millions of other Facebook users based on their unique setof features. LetÈs say I post a picture of myself on Facebook with a group ofpeople, it will recommend I tag myself when the model is confident that it isme based on a probability score.È Facebook says DeepFace has a 97% successrate in recognising whether two images are of the same person or not compared to 96% for humans.## Is AI replacing human jobs?On the contrary, according to a report by global IT consulting firm Gartner,[AI is estimated to create around 2.3 million opportunities by the year2020](https://www.gartner.com/en/newsroom/press-releases/2017-12-13-gartner-says-by-2020-artificial-intelligence-will-create-more-jobs-than-it-eliminates).YouÈll find a pool of talented people behind every project. Each use caserequires a ML team to drive it. Uber, for example, created a whole range ofjobs to teach machines how to understand customer demand, traffic and safety.ItÈs no different in our business. There is huge potential for jobs in thefuture  all it takes is a [willingness to adapt to work alongsidemachines](https://www.iafrikan.com/2019/04/17/technology-doesnt-kill-job-opportunities-egypts-minister-of-ict/).* * *Cover image credit: Franki Chamaki/Unsplash
__label__Customer Experience	"The idea is to make a ""git pull"" if possible, maybe once a day"
__label__AI	1. 1.Turing, A. M. Computing machinery and intelligence. _Mind_ **49** , 433460(1950).2. 2.Jordan, M. I. & Mitchell, T. M. Machine learning: trends, perspectives, andprospects. _Science_ **349** , 255260 (2015).3. 3.Mitchell, T. M. _Machine Learning_ (McGraw-Hill Science/Engineering/Math,Boston, Mass, USA, 1997).4. 4.Peek, N., Combi, C., Marin, R. & Bellazzi, R. Thirty years of artificialintelligence in medicine (AIME) conferences: a review of research themes._Artif. Intell. Med._ **65** , 6173 (2015).5. 5.Yu, K. H., Beam, A. L. & Kohane, I. S. Artificial intelligence in healthcare._Nat. Biomed. Eng._ **2** , 719731 (2018).6. 6.Lynch, C. J. & Liston, C. New machine-learning technologies for computer-aideddiagnosis. _Nat. Med._ **24** , 13041305 (2018).7. 7.Wong, D. & Yip, S. Machine learning classifies cancer. _Nature_ **555** ,446447 (2018).8. 8.Zhang, W., Chien, J., Yong, J. & Kuang, R. Network-based machine learning andgraph theory algorithms for precision oncology. _npj Precis. Oncol._ **1** ,25 (2017).9. 9.Ching, T. et al. Opportunities and obstacles for deep learning in biology andmedicine. _J. R. Soc. Interface_ **15** , 20170387 (2018).10. 10.Richter, A. N. & Khoshgoftaar, T. M. A review of statistical and machinelearning methods for modeling cancer risk using structured clinical data._Artif. Intell. Med._ **90** , 114 (2018).11. 11.Esteva, A. et al. Dermatologist-level classification of skin cancer with deepneural networks. _Nature_ **542** , 115118 (2017).12. 12.LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. _Nature_ **521** , 436444(2015).13. 13.Goodfellow, I., Bengio, Y. & Courville, A. _Deep Learning_ (MIT Press,Cambridge, Mass, USA, 2016).14. 14.Coudray, N. et al. Classification and mutation prediction from non-small celllung cancer histopathology images using deep learning. _Nat. Med._ **24** ,15591567 (2018).15. 15.Ehteshami Bejnordi, B. et al. Diagnostic assessment of deep learningalgorithms for detection of lymph node metastases in women with breast cancer._JAMA_ **318** , 21992210 (2017).16. 16.Rawat, W. & Wang, Z. Deep convolutional neural networks for imageclassification: a comprehensive review. _Neural Comput._ **29** , 23522449(2017).17. 17.Bailey, M. H. et al. Comprehensive characterization of cancer driver genes andmutations. _Cell_ **173** , 371385 (2018).18. 18.Ghahramani, Z. Probabilistic machine learning and artificial intelligence._Nature_ **521** , 452459 (2015).19. 19.Touw, W. G. et al. Data mining in the Life Sciences with Random Forest: a walkin the park or lost in the jungle? _Brief. Bioinform._ **14** , 315326(2013).20. 20.Azuaje, F. Computational models for predicting drug responses in cancerresearch. _Brief. Bioinform._ **18** , 820829 (2017).21. 21.Zhao, L., Lee, V. H. F., Ng, M. K., Yan, H. & Bijlsma, M. F. Molecularsubtyping of cancer: current status and moving toward clinical applications._Brief. Bioinform._ <https://doi.org/10.1093/bib/bby026> (2018).22. 22.Karczewski, K. J. & Snyder, M. P. Integrative omics for health and disease._Nat. Rev. Genet._ **19** , 229310 (2018).23. 23.Li, Y., Wu, F. X. & Ngom, A. A review on machine learning principles formulti-view biological data integration. _Brief. Bioinform._ **19** , 325340(2018).24. 24.Ramazzotti, D., Lal, A., Wang, B., Batzoglou, S. & Sidow, A. Multi-omic tumordata reveal diversity of molecular mechanisms that correlate with survival.Preprint at <https://www.biorxiv.org/content/early/2018/10/14/267245> (2018).25. 25.Kim, D. et al. Knowledge boosting: a graph-based integration approach withmulti-omics data and genomic knowledge for cancer clinical outcome prediction._J. Am. Med. Inform. Assoc._ **22** , 109120 (2015).26. 26.Klughammer, J. et al. The DNA methylation landscape of glioblastoma diseaseprogression shows extensive heterogeneity in time and space. _Nat. Med._**24** , 16111624 (2018).27. 27.Yu, K. H. et al. Association of omics features with histopathology patterns inlung adenocarcinoma. _Cell Syst._ **5** , 620627 (2017).28. 28.Gevaert, O. et al. Glioblastoma multiforme: exploratory radiogenomic analysisby using quantitative image features. _Radiology_ **273** , 168174 (2014).29. 29.Disselhorst, J. A. et al. Linking imaging to omics utilizing image-guidedtissue extraction. _Proc. Natl. Acad. Sci. U.S.A._ **115** , E2980E2987(2018).30. 30.Pan, S. J. & Yang, Q. A survey on transfer learning. _IEEE Trans. Knowl. DataEng._ **22** , 13451359 (2010).31. 31.Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. & Wojna, Z. Rethinking theinception architecture for computer vision. Preprint at<https://arxiv.org/abs/1512.00567> (2015).32. 32.Sevakula, R. K., Singh, V., Verma, N. K., Kumar, C. & Cui, Y. Transferlearning for molecular cancer classification using deep neural networks._IEEE/ACM Trans. Comput. Biol. Bioinform._<https://doi.org/10.1109/TCBB.2018.2822803> (2018).33. 33.Turki, T. W., Wei, Z. & Wang, J. T. L. Transfer learning approaches to improvedrug sensitivity prediction in multiple myeloma patients. _IEEE Access_ **5**, 73817393 (2017).34. 34.Tan, M. Prediction of anti-cancer drug response by kernelized multi-tasklearning. _Artif. Intell. Med._ **73** , 7077 (2016).35. 35.Shaikhina, T. & Khovanova, N. A. Handling limited datasets with neuralnetworks in medical applications: a small-data approach. _Artif. Intell. Med._**75** , 5163 (2017).36. 36.Choi, C. et al. RETAIN: an interpretable predictive model for healthcare usingreverse time attention mechanism. Preprint at<https://arxiv.org/abs/1608.05745> (2016).37. 37.Lahav, O., Mastronarde, N. & van der Schaar, M. What is interpretable? Usingmachine learning to design interpretable decision-support systems. Preprint at<https://arxiv.org/abs/1811.10799> (2018).38. 38.Alaa, A. M. & van der Schaar, M. Forecasting individualized diseasetrajectories using interpretable deep learning. Preprint at<https://arxiv.org/abs/1810.10489> (2018).39. 39.Castelvecchi, D. Can we open the black box of AI? _Nature_ **538** , 2023(2016).40. 40.Lundberg, S. M. et al. Explainable machine-learning predictions for theprevention of hypoxaemia during surgery. _Nat. Biomed. Eng._ **2** , 749760(2018).41. 41.Li, O., Liu, H., Chen, C. & Rudin, C. Deep learning for case-based reasoningthrough prototypes: a neural network that explains its predictions. Preprintat <https://arxiv.org/abs/1710.04806> (2017).42. 42.Chen, C., Li, O., Barnett, A., Su, J. & Rudin, C. This looks like that: deeplearning for interpretable image recognition. Preprint at<https://arxiv.org/abs/1806.10574> (2018).43. 43.Ancona, M., Ceolini, E., _ztireli, C. & Gross, M. Towards better understandingof gradient-based attribution methods for deep neural networks. Preprint at<https://arxiv.org/abs/1711.06104> (2018).44. 44.Fabris, F., Doherty, A., Palmer, D., de Magalhaes, J. P. & Freitas, A. A. Anew approach for interpreting Random Forest models and its application to thebiology of ageing. _Bioinformatics_ **34** , 24492456 (2018).45. 45.Basu, S., Kumbier, K., Brown, J. B. & Yu, B. Iterative random forests todiscover predictive and stable high-order interactions. _Proc. Natl. Acad.Sci. U.S.A._ **115** , 19431948 (2018).46. 46.Yu, M. K. et al. Visible machine learning for biomedicine. _Cell_ **173** ,15621565 (2018).47. 47.Yauney, G. & Shah, P. Reinforcement learning with action-derived rewards forchemotherapy and clinical trial dosing regimen selection. _Proc. Mach. Learn.Res._ **85** , 161226 (2018).48. 48.Ali, I. et al. Lung nodule detection via deep reinforcement learning. _Front.Oncol._ **8** , 108 (2018).49. 49.Padmanabhan, R., Meskin, N. & Haddad, W. M. Reinforcement learning-basedcontrol of drug dosing for cancer chemotherapy treatment. _Math. Biosci._**293** , 1120 (2017).50. 50.Tseng, H. H. et al. Deep reinforcement learning for automated radiationadaptation in lung cancer. _Med. Phys._ **44** , 66906705 (2017).51. 51.Mahmud, M., Kaiser, M. S., Hussain, A. & Vassanelli, S. Applications of deeplearning and reinforcement learning to biological data. _IEEE Trans. NeuralNetw. Learn. Syst._ **29** , 20632079 (2018).52. 52.Girardi, D. et al. Interactive knowledge discovery with the doctor-in-the-loop: a practical example of cerebral aneurysms research. _Brain Inform._**3** , 133143 (2016).53. 53.Pearl, J. _Causality: Models, Reasoning and Inference_ (Cambridge UniversityPress, Cambridge, England, 2000).54. 54.Yoon, J., Jordon, J. & Van der Schaar, M. GANITE: estimation of individualizedtreatment effects using generative adversarial nets. In _InternationalConference on Learning Representations._<https://openreview.net/forum?id=ByKWUeWA> (2018).55. 55.Alaa, A. M. & Van der Schaar, M. AutoPrognosis: automated clinical prognosticmodeling via Bayesian optimization with structured Kernel learning. Preprintat <https://arxiv.org/abs/1802.07207> (2018).
__label__DevOps, Mobile services	This is something I noticed personally on a site while working on AMP integration, and was just reported at https://wordpress.org/support/topic/php-fatal-error-enabling-paired-mode/ as well.Basically, it's possible to call `is_amp_endpoint()` after `parse_query` but without there being a global `$wp_query` (e.g. because of some plugin `doing_it_wrong`).This will lead to a fatal error in this part of the code:https://github.com/ampproject/amp-wp/blob/286aef5d61e838d4b4b5797acd583a5e0e947f90/includes/amp-helper-functions.php#L273-L277Where `get_query_var()` will fail to access `$wp_query->get()`.I feel like we can just return `false` in when `$wp_query` is not set, perhaps even with an additional `_doing_it_wrong()` to say that there's no global query.
__label__cloud	"<p>I checked out ""Google Cloud Platform Marketplace"", most solutions are built to ""launch on compute engine"".</p><p>Are there any ""Google Cloud Platform Marketplace Solutions"" built to ""launch on app engine""?</p><p>A good percentage of my projects are built and processed on app engine. If I can find these app engine solutions from ""Google Cloud Platform Marketplace"", that will save me a lot of development time.</p>"
__label__nan	AbhÌ_ngig von der Implementierung der Transaktionen fÌ_r BestandsÌ_nderungen
__label__cloud	<p>When we realize the data lake with GCP Cloud storage, and data processing with Cloud services such as Dataproc, Dataflow How can we generated data lineage report in GCP. Thanks.</p>
__label__DevOps	"We will support Java inner class.Example is below.- Test.java```javaimport OuterTest.*;class Test{public static void main(String[] args){System.out.println(OuterTest.helloWorld());}}```- OuterTest.java```javaclass OuterTest{public static string helloWorld(){return ""Hello World!"";}}```"
__label__AI	ItÈs easy to get lost amidst all the uncertainty and speculation, but when wedo, we may fail to see whatÈs happening right in front of us right now. AI isalready creating new forms of employment. In fact, researchers at Accenturehave identified several new categories of jobs spurred by AI. This research isfeatured in the _MIT Sloan Management Review_ article The Jobs ThatArtificial Intelligence Will Create. IÈm joined by authors H. James Wilsonand Paul Daugherty for a look at the findings from their first round ofresearch and what they have learned since about the new roles that AI iscreating in the organization. Jim and Paul, welcome, and thanks for taking thetime to talk about the work you and your colleagues are doing to help usunderstand AIÈs impact on employment.**Paul Daugherty:** Yeah, we started this about two-and-a-half years ago whenJim and I were looking at the advance of AI and the current state of a lot ofthe discussions around AI. And we became concerned about the type of dialoguethat was happening. As you say, there certainly is a massive impact on the waywork is done, brought on by AI. But in our early experience, we saw a lot ofpromise for AI to change jobs and create jobs and make more human jobs  ormake jobs more human  in many steps. So Jim and I launched this researchproject to look at 1,500 organizations and how they were using AI and how itwas impacting their business, their workforce, and the things that they did inthe company. And the finding was that contrary to what a lot of people think,we believe AI will create a lot of novel new jobs. It will certainly eliminatesome jobs, but we believe that the net effect will be creating a lot of jobs and jobs that are good jobs that leverage our human capability in differentways. Broadly speaking, we came up with three categories of jobs that we call_trainer_ , _explainer_ , and _sustainer_  three categories of new jobs whereweÈre using our human capability in different ways to allow AI to have thepositive impact on the way we work, the way we live, and overall a positiveimpact on outcomes.**Paul Michelman:** Thanks, Paul. LetÈs walk through each of these categories,beginning with trainer.**James Wilson:** So we initially did that research of about 1,500 companies,and we didnÈt initially see these three job categories, but when we started todig down into the research, when we started to do follow-up case studies,thatÈs where we really started to see these jobs surface  managers that wewere interviewing talking about writing fundamentally new job descriptions.And we actually saw recurring job titles [and] job categories that they werewriting for. One of those job categories is the trainer role. And these arethe people that are quite often doing the data science. TheyÈre doing themachine learning engineering. TheyÈre the ones that are actively building theAI systems. One of the things that we see is that even within the samecompany, there can be a lot of variety within a particular job category, likea trainer job. So, for instance, Tesla: You can see that the carmaker isrecruiting line managers with experience in robotics, and robot engineers andcomputer vision researchers, and deep learning scientists and machine learningsystems experts. So really rich variety  even within that one trainercategory within a single company.**Paul Michelman:** And Jim, trainers are exclusively technology experts?**James Wilson:** No, not necessarily. And we can talk some more about that.You know, itÈs important to have functional experts on your team, as well. Itmight be that you have a person with a marketing background or an operationsbackground on your team helping identify and solve problems that the technicalexperts  for instance, the data scientists  will then go in and solve for.**Paul Daugherty:** And just to add on, one specific type of job we see herein the trainer category are the AI personality trainers  somebody who canbehaviorally train the chatbots and intelligent virtual agents that so manycompanies are deploying right now. Companies deploy those solutions tointeract (voice-driven interaction) with their consumers and such. WhattheyÈre realizing is that AI becomes the brand. And so you need to train it tobehave in the right way, to operate the right way, to have the right answers,the right tone, etc. And thatÈs a nontechnical type of job thatÈs needed toshape that type of behavior and [to] work with the engineers to get thatbehavior implemented in the right way in the solutions.**Paul Michelman:** Great. LetÈs move on to the second category, which youlabel explainers.**Paul Daugherty:** Explainers is one that I think is getting to the fact thatAI is embedded in very complex systems and business processes. And so thereÈsan issue both of explaining AI itself and how itÈs working, but more broadlyexplaining the kinds of outcomes that are being generated by the systems thatare being developed. For example, if you think about a self-driving car, ithas a lot of AI embedded in it, but thereÈs lots of other driving systems andthings included. So when you think about autonomous vehicles and whatÈshappening, what weÈre seeing is companies creating roles [for] people tounderstand the overall context of the system  the environmental conditions,the road conditions, lots of things in addition to the AI itself and how itwas behaving, so that they can understand and tune the systems to operate moreeffectively.... Understanding that impact is the [type of job] weÈre seeing inthe explainer category.**James Wilson:** In some cases, these explainer roles are actually beingencouraged through regulation. So this year by some estimates there were about75,000 new explainer roles being created related to the GDPRÈs right to[explanation]. And these are analysts in banks, for instance, and in customerservice centers and that sort of thing, [who] answer customersÈ questionsabout an algorithmic decision.**Paul Michelman:** So are explainers always an interface between theorganization and the public? Or are they also interfacing within parts of theorganization?**James Wilson:** They quite often are interfacing with parts of theorganization as well. So, for instance, in health care weÈre seeing a lot ofearly evidence that explainers are working with physicians in explaining whyan AI system is making a particular recommendation and whether then the doctorcan go on and make a medical recommendation to a patient as a result. Theyoften are working in health care settings, making interpretations and sharinginsights with medical professionals, not necessarily patients or customers.**Paul Michelman:** So letÈs move to the third category: sustainers.**Paul Daugherty:** This is really speaking to the roles that are needed tomanage AI (the use of AI) and to make sure that it not only behaves right atthe outset, but it continues to behave properly to produce the desiredoutcomes over time, because the technology changes, the data changes, thesituation changes, the business changes. And sustainer roles are people whoreally understand the outcomes that need to be driven to make sure that thatoutcome and that impact is sustained.**James Wilson:** They also spend a good deal of their day thinking aboutunintended consequences from AI systems and how those end up being received bythe public. So, for instance, surge pricing. Is a surge pricing model going tobe something that is sustainable for a company? That was an issue, obviously,that some of the firms like Uber and Lyft had to deal with initially. How doyou come up with a surge-pricing model thatÈs algorithm-driven but also issustainable? Things like biased algorithms, discriminatory facial recognitionsystems  these are things that [the] first wave of trainers didnÈtnecessarily think about, but now sustainers think about whether theseunanticipated, unintended consequences are something that can be managed. Ormaybe they might even recommend that an AI system has to be taken out ofoperation until the company figures out how to get it right.**Paul Michelman:** WhatÈs an example of a title that a sustainer might havein the organization?**Paul Daugherty:** I think sustainers can manifest themselves in a number ofways. WeÈre seeing this often as augmenting the team or the work thatÈs beingdone in different situations. For example, in manufacturing or factory typesof situations where theyÈre using collaborative robots and different types oftechnology that need to be continually configured and rearranged to meet thedynamic needs of the supply chain and what theyÈre producing  sustainer rolesin that sense would be the technician whoÈs reorganizing and managing theinterface between the robots and the production process thatÈs beingperformed. So those are the types of roles that we see there.**James Wilson:** You know, just driving up and down the streets of SanFrancisco, youÈre going to pass a number of autonomous vehicles. But ofcourse, sitting behind that robo-car is an AI safety trainer. And so you see alot of those roles in autonomous vehicle situations. In general, any companythatÈs building robotic systems is going to be hiring these AI safety or AIcompliance officers that really make sure at a basic level that the systemsthat theyÈre deploying are safe in the public.**Paul Michelman:** You did this research, originally, two years ago. And Iguess in terms of the longevity of management ideas, two years is really notthat long a period of time. But in the world of AI, we almost should betalking about dog years, I think  two years seems like a long time. So IÈmwondering: When youÈre looking at the market today, when youÈre looking atemployment trends today, would you stick to these three categories? Have theyevolved? How has your thinking shifted, if at all?**Paul Daugherty:** Yeah, I think thereÈs a little bit of both. IÈll talkfirst about what we see with the categories we identified. If you look attrainers, explainers, and sustainers, I think we see more evidence every dayof how these roles are growing and increasing. For example, if you look at jobpostings, which we were researching a little while ago, you can find_explainer_ in job titles now  Algorithm Explainability Engineer andFinancial Services Explainability Specialist and things like that  the needto explain the algorithms and the AI. WeÈre seeing this accelerate, I think,as you said, in this dog-year type of fashion. WeÈve also seen some compellingexamples from some of the early entrants of why you need these roles. I thinkFacebook is an instructive story. What theyÈve done, following all the focuson them around Cambridge Analytica, is theyÈve created tens of thousands ofnew jobs to add humans in to manage the algorithms and produce the resultsthat people really want, in a more responsible fashion. And those aresustainer jobs  itÈs people added in. I think FacebookÈs comment was alongthe lines of: WeÈve concluded algorithms canÈt manage the algorithms, we needpeople to manage the algorithms. And those arenÈt isolated incidents. I thinkthose are examples of the roles that all companies are going to need as theydeploy the technology.**James Wilson:** Yeah, our article focused on unprecedented new jobcategories where people are out there developing and responsibly managing AIsystems. But while AI is certainly creating new jobs, itÈs also changing oldjobs by augmenting them. And we didnÈt get into that much in that initialarticle. For example, at one bioscience company that weÈve been looking at itÈs based out here in the Bay Area  scientists use robotic lab equipment tohelp on certain experimental tasks. The robotic helpers precisely squirtliquids and they plate cells and they count microbe colonies in a way thataugments and accelerates scientific work. And as a result of this robotaugmentation, scientists are now able to complete about 400 times moreexperiments each week. So if you think about that, a scientist now has thepotential to make a hundred yearsÈ worth of scientific discovery in a singleyear through AI augmentation. But you know the lab scientistÈs job content hasreally changed quite a bit. She now does things that are quite a bit differentthan she was doing before and has different ways of doing them. And we didnÈtget into that topic as much. We were much more focused on the job creation,not the job content change.**Paul Michelman:** When weÈre looking at the three fully new categories ofjobs, how equally and evenly distributed are these roles going to be? Arethere particular industries or types of organizations for which these rolesare going to emerge earlier? Are there other organizations that should takekind of a sit-back-and-wait approach?**James Wilson:** I would make two points here. The first is that companiesreally need all three roles. For instance, a few years ago many of the mostadvanced AI firms  the major technology companies, for instance  focusedexclusively on staffing AI trainers. But now theyÈre playing catch up. So youreally do need to have all three. But I think one insight here is that the AItalent war is quite a bit different and broader than a lot of people initiallythought. My second point is that the distribution of the roles is going tovary quite a bit by industry and customer and regulatory context.**Paul Michelman:** When weÈre looking at these new categories, it would seemthat one of the fundamental challenges organizations face is that these arejobs that no one has done before. No one has ever trained to be an AI trainer.How do we solve for that?**Paul Daugherty:** ThatÈs one of the biggest challenges that I think we haveto face as we look at how do we prepare people for these new roles and how dobusinesses and organizations prepare for these new roles. WeÈve done somefollow-on research on this, and we think thereÈs three things that we reallyneed to focus on to get this right. One is focusing more on experientiallearning. If you look at traditional training, it would show that peopleforget 80% of what they learn within about a day of learning it fromtraditional training methods. So how do you get people engaged in the learningprocess in the experiential way? We think apprenticeships are very important hands-on learning, learning injected at different points in the process. Forexample, weÈve done an interesting training and learning approach with a largeaircraft manufacturer, where we used AI and mixed-reality technology to equipworkers with a mixed-reality headset that helps them understand the job theywere doing and do higher-skilled jobs faster by providing them guidance alongthe way. And thatÈs an example of using technology plus experiential learningto advance people skills into these new categories.A second thing we found is important is shifting the burden from just theperson needing to learn to looking [at] the responsibility [that] differentinstitutions  businesses, etc.  have for the training. One thing we firmlybelieve is that every organization needs to look at learning as a corecompetency in a really new and fresh way. And you need to think about learningplatforms from lifelong learning as a core part of what you do. Because toyour point, you canÈt go hire people for some of these roles, you may need tobuild people to do them. For example, we worked with an oil company on a newdrilling technology that uses visualization and AI and gaming engines tocreate a whole different way for a technician to operate a drill (oildrilling, operating miles underground). So where are you going to hire thegaming engine, visualization-inspired driller? YouÈre not going to find peopleon the market with those skills. YouÈre going to have to take your currenttechnicians and develop these new digital skills in them, which is why webelieve that these learning platforms are going to be a critical component forcompanies. ItÈs going to be differentiating for those who can get it right.And then finally, from an overall societal and multi-stakeholder perspective,we need to look at how we enable vulnerable people in the population [who] arealready maybe separated by a digital divide  who donÈt have the rightbaseline skills to operate in this environment  and do more to make sure thateverybodyÈs got the base of skills that [they] need to participate in thesejobs.**Paul Michelman:** So this is really interesting. On the one hand, a focus onon-the-job learning, experiential learning, certainly promises or would seemto promise a shorter time frame and maybe more stickiness to get peopletrained up for these new roles. And yet, thatÈs still a major organizationalundertaking  maybe not as great as relying on academia to fill the void,which will take decades  but still these jobs need to be done. They may notbe fully at scale, but as you guys have noted, theyÈre very much real andhappening right now. So as we look at these three categories, where shouldthat first crop of people come from?**James Wilson:** Well, I think one thing that we can do today is to make iteasier for people to become trainers, explainers, and sustainers by basicallylowering the barrier to building or improving an AI system  what Paul and Icall _AI democratization_. WeÈre already beginning to see point-and-click AItraining tools out there. And many of the cloud AI services providers, forinstance, are quite easy to use. If you have a data set, you can just uploadthe data set to one of these services and then start playing around with thedata. So I think the complement to what Paul was just talking about, which wasraising the skill level, is also at the same time to lower the barrier tousing these systems. I think thatÈs a really important thing. And itÈs oftenan untapped opportunity, but weÈre beginning to see more and more companiesmigrating toward that model as well.**Paul Michelman:** In terms of global impact for these new categories of jobsin particular  and I realize this is going to be a difficult question toanswer in particulars  so general trends would be fine, but I think a lot ofpeople would like some help in sizing the opportunity that your researchsuggests, especially as we think about potential job loss at the hands of AI,machine learning, and automation. Are the new jobs weÈre discussing here arelative drop in the bucket for the highly specialized few or well-trainedfew? What is this going to look like at scale?**Paul Daugherty:** These jobs certainly are a drop in the bucket, but youhave to put it in context. We think this is a major impact  these jobs are amajor impact going forward on employment and opportunity for people. However,just to start, there will be a lot of disruption in the labor force, and therewill be categories of jobs that are at risk for automation. But you have tolook at the broad spectrum of how thatÈll happen. And from the research weÈvedone, if you look across categories of jobs, if you look at the content ofwork, thereÈs about 10% of work generally that we found through our researchis human-only (only humans can do). ThereÈs about 35% of work that isautomatable  that part of the work is automatable by machines, algorithms,etc. And the rest of the work  which is the majority of it  is reallyaugmentable, which means you can improve the way humans do it, but itÈslargely going to need to be done by humans. And I think that the contextaround these new jobs is [that] most of the jobs become transformed indifferent ways. And how do we use AI and other technology to transform thejobs to prepare people for those changed jobs? So thatÈs a big impact, and Iwould say almost every job will change as a result of the technology. Many newjobs will be created and some will be eliminated.One good data point, having just come from a G7 meeting recently: Canadaannounced that through their investment theyÈre making, they expect a $16billion economic increase in output as a result of the investments theyÈremaking in AI. ThatÈs significant output. They talk about 16,000 jobs theyÈrecreating through the focus on AI. And we see similar types of impacts andresults around the world and larger impacts in terms of GDP increase (economicoutput increase) by countries. And thatÈs where the opportunity is  in kindof envisioning how do we prepare people for these new types of jobs that willbe created?**James Wilson:** Yeah, just building on PaulÈs point, I think you can get agood quantitative sense of the size of opportunity by looking at businessleadersÈ investment expectations, especially around growth. In our research,for instance, we found that firms that invest in their AI workforce at thesame rate as top-performing businesses in their sector are going to grow bothrevenues but also their workforce.**Paul Daugherty:** ThereÈs another impact on jobs that I think we need tothink about, which is the fact that itÈs hard to anticipate where the new jobsare coming from and what the new jobs will look like. ThatÈs why we try to beprescriptive and talk about trainers and explainers and sustainers. Onehistorical observation IÈd offer is that if you look back at prior technologywaves weÈve had  20 years ago, people wouldnÈt have anticipated that weÈdhave large categories of people employed in things like search engineoptimizers, web designers, eBay retail merchants, etc. In a similar fashion,weÈre already seeing this creation of the new jobs going forward, and theyÈrethe unanticipated, new things that we need to continue to be creative aboutand look for as time goes on.**Paul Michelman:** So whatÈs next in your research?**James Wilson:** In our research, we see that about 69% of executives believethat their industry is going to be completely transformed between now and 2022as a result of AI. But we continue to try to understand not only the jobs thatare going to be created, but also the skills that are going to help thistransformation  that are going to enable this transformation. And I thinkthis is an important area for our research. PaulÈs already set it up verynicely. A lot of our findings thus far have been surprising to us. Forinstance, you might think that STEM skills are the be-all and end-all for theage of AI. But our research is showing that four distinctively soft skills arebecoming much more valuable as we begin collaborating with smart machines andusing smart machines: These are complex reasoning, creativity,social/emotional intelligence, and certain forms of sensory perception_. Sointerestingly, one thing that weÈre tracking now is how skills are becomingsofter. And what does that look like on an AI team?**Paul Daugherty:** Yeah, I think going further on those human skills. Becauseone question we get a lot is exactly that, which is: OK, Paul and Jim, we getyou, we believe what youÈre laying out here. What do I do tomorrow? What do Ido next month to start preparing my people and my workforce? Getting to thatnext level of specificity  the human skills and how we get people ready  Ithink is really important. ThereÈs a couple other fronts weÈve launched. Oneis on responsible AI, which we hinted at in the original article, but itÈsreally become more important, which is: How do we make sure we get the rightoutcomes from AI? Speaking of things like transparency and explainability,which one of our job categories addresses; thinking about bias, which is anissue that many have run into when they apply AI  creating biased outcomesrather than inclusive outcomes; thinking about accountability; thinking abouttrustworthiness and issues like that. So weÈre doing a lot of further work onthat. In fact, we have a new article in _MIT SMR_ on fairness and approachesto fairness with AI and some work weÈve done in that area. These are going tobe really important issues for businesses and organizations to grasp and tomake sure that as we have increasing numbers of people working in AI and morepowerful solutions delivered with the AI, how do we make sure we deliver theright outcomes in all cases?**Paul Michelman:** Terrific. Paul Daugherty, Jim Wilson, thank you both verymuch.**Paul Daugherty:** Thank you, Paul.**James Wilson:** Thank you, Paul.
__label__Customer experience	When Xdata is more, is PNChart can scroll?
__label__nan	"<p>Preciso de ajuda com minhas regras de seguran_a do Firestore, pois n£o est£o permitindo updates ou deletes como eu queria que fizessem.</p><p>Tenho 2 cole_µes: Ofertas e Usurios;Cada usurio tem seu idUsurio e um papel(leitor, administrador, editor)Cada oferta tem seu IdOferta e o IdUsuario(do usurio que a criou).</p><p>Quero permitir que apenas quem criou a oferta possa fazer updates ou deletes nela.Isso funciona bem na func£o Proprietario();Quero ainda que os usurios cujo campo Papel seja: administrador ou editor possam fazer updates ou deletes nas ofertas que n£o foram criadas por eles.</p><p>Essa © a parte em que n£o funcionam as regras</p><p>Algu©m pode me ajudar? Eu li 2 vezes tudo que achei no firebase sobre as regras, mas n£o consigo criar uma que funcione.Eu entendi que eu precisaria de um get(/databases</p><pre><code>service cloud.firestore {match /databases/{database}/documents {match /ofertas/{oferta}{allow read: if logado();allow create: if logado();allow update: if proprietario();allow delete: if proprietario() || isAdmin();}match /usuarios/{usuario}{function isAdmin(){return resource.data.papel == ""administrador"";}allow read: if logado();allow create: if logado();allow update: if request.auth.uid == usuario;}function logado(){return request.auth!=null;}function proprietario(){return request.auth.uid == resource.data.idUsuario;}function temAcesso(){return resource.data.tipo in get(/databases/$(database)/documents/usuarios/$(tipo)).data.tipo;}}}</code></pre>"
__label__nan	<p>You can inject and use <code>SpanCustomizer</code> interface to customize the current span (add tags / logs etc.) or inject and use <code>Tracer</code> to retrieve the current span ad manipulate it.</p>
__label__AI	29th May 2019#Leveraging Web Development with Artificial IntelligenceWeb Development is growing dramatically and so is the use of ArtificialIntelligence. TodayÈs web development is focused mainly on enhancing userexperience and AI is, thus, the perfect match for it.Artificial Intelligence (AI) is the ability of a computer or a machine tothink and work just like humans. AI is based on algorithms which use computingpower to solve specific problems faster and better than humans can. AI allowsthe machines to analyze, plan, learn and adapt. AI relies on the data that isprovided to it and interprets this data to create new perspectives and learnnew objectives. It then leverages this data to provide better and moreaccurate results in the future.Let us now analyze how Artificial Intelligence and Machine Learning can beused in Web Development and provide you with an exceptional advantage overyour competitors:![](https://i1.wp.com/www.atyantik.com/wp-content/uploads/2019/05/60847875_2340618609596223_6147071032143380480_n.png?fit=640%2C328&ssl=1)1. **User Experience**  AI and ML can be used to enhance the experience of your website or app visitors. These visitors are often potential customers and they are more likely to purchase a product if they are provided with easy-to-use interface which enables them to find specific products faster.2. **Improved Predictive Response**  traditional chatbots were boring, too predictive and sometimes even useless. But the modern AI-powered chatbots are super-intelligent. They know how to quickly and accurately respond to exact customer queries and needs making the whole experience delightful for the consumers.3. **Personalized Content**  Machine Learning is able to predict the intentions of the users based on their previous activities and can hence be used to show content to the audience that is best tailored to their needs. Users are shown with music, videos, tv shows, and even products that they are most likely to interact with.4. **Voice-based Search**  Voice Recognition and interpretation is one of the prodigies of Machine Learning and has revolutionized the way users interact with applications. Integrating web development allows customers to shop faster and smarter.5. **Unique Store Experience**  AI can be used by retailers to gain customer data and get insights on customer behaviour. This data can then be used to offer personalized and customized shopping recommendations, products, deals and much more to the users. This helps boost your sales as your customers will more likely buy products from your website rather than from a competitor.#### Case Study of Amazon and Netflix implementing AI and ML in theirRecommendation EnginesHave you ever wondered why the product you see on Amazon is exactly what youwere looking for? Well, Amazon **** has been using AI techniques to tailor theproducts and content it recommends to its customers. Their recommendationsystem uses **goods-based recommendation ** which means that users arerecommended based on their previous interactions and purchases and **buddy-based recommendation ** which means that users are recommended products andcontent based on what their Facebook or Instagram friends like.![Netflix Logo](https://i0.wp.com/www.atyantik.com/wp-content/uploads/2019/05/netflix.png?resize=253%2C142&ssl=1)**Netflix** , on the other hand, revamped their recommendation algorithms andhoned them for visual impressions. Their AI and ML algorithms are optimized toprovide users with the image that they are most likely to respond to. Theyfeed implicit (based on user behavior) and explicit (based on user activity)data to their Machine Learning algorithms to help it figure out the mostrelevant content for each individual user. It is constantly collecting datafrom its 100 million subscribers to make its AI and ML algorithms better andbetter every day.
__label__nan	
__label__Customer Experience	There are conventions and approaches that are not clear. They could be solved with a more intuitive UI, but for the time being would be good to document them.
__label__DevOps	PHP7 introduces [new error handling](http://php.net/manual/en/language.errors.php7.php), which the current version of the Whoops error handler [doesn't understand](https://github.com/filp/whoops/issues/341). Whoops 2 fixes this and other problems as a result of moving to PHP7.
__label__AI	Article URL: <https://www.i-programmer.info/news/105-artificial-intelligence/12810-saps-creating-trustworthy-and-ethical-artificial-intelligence.html>Comments URL: <https://news.ycombinator.com/item?id=20031515>Points: 1# Comments: 0
__label__DevOps, Containers	## Error in Faveo Community**Symfony\Component\Debug\Exception\FatalErrorException** in **vendor/laravel/framework/src/Illuminate/Container/Container.php:698**Maximum execution time of 30 seconds exceeded[View on Bugsnag](https://app.bugsnag.com/ladybird-web-solution-pvt-ltd/faveo-community/errors/5c767c5105bbe70018a1702d?event_id=5c767c510035ab9097410000&i=gh&m=ci)## Stacktracevendor/laravel/framework/src/Illuminate/Container/Container.php:698 - [main][View full stacktrace](https://app.bugsnag.com/ladybird-web-solution-pvt-ltd/faveo-community/errors/5c767c5105bbe70018a1702d?event_id=5c767c510035ab9097410000&i=gh&m=ci)*Created automatically via Bugsnag*
__label__Analysis	The metrics validation should be extended to support any kind of metric by allowing custom Prometheus queries in the canary analysis spec.
__label__Customer Experience	If you add a big piece of text which you try to split in multiple paragraphs by hitting the return key, then it adds the ssml closing paragraph tag at the end.
__label__cloud	<p>Model export feature is currently not supported in Cloud AutoML Vision.</p>
__label__DevOps	## Error in Faveo Community**Symfony\Component\Debug\Exception\FatalThrowableError** in **GET /gcdhelpdesk/public/ticket/tooltip**Call to a member function purify() on null[View on Bugsnag](https://app.bugsnag.com/ladybird-web-solution-pvt-ltd/faveo-community/errors/5c768484f0d68100194589ed?event_id=5c76848400358f7be3780000&i=gh&m=ci)## Stacktrace\app\Http\Controllers\Agent\helpdesk\TicketController.php:2403 - App\Http\Controllers\Agent\helpdesk\TicketController::getTooltip\app\Http\Middleware\Authenticate.php:54 - App\Http\Middleware\Authenticate::handle\app\Http\Middleware\CheckUpdate.php:35 - App\Http\Middleware\CheckUpdate::handle\app\Http\Middleware\LanguageMiddleware.php:33 - App\Http\Middleware\LanguageMiddleware::handle\app\Http\Middleware\VerifyCsrfToken.php:33 - App\Http\Middleware\VerifyCsrfToken::handle[View full stacktrace](https://app.bugsnag.com/ladybird-web-solution-pvt-ltd/faveo-community/errors/5c768484f0d68100194589ed?event_id=5c76848400358f7be3780000&i=gh&m=ci)*Created automatically via Bugsnag*
__label__Customer Experience	it should possible to show more infos on the delivery email like:- customer email and other infos- deliveryContext stuff
__label__Customer Experience	We should consider whether it would be useful for Janeway to collect and report information on co-reviewers (additional people who help a referee perform peer review).
__label__AI	Microsoft unveiled a [curriculum](https://docs.microsoft.com/en-us/learn/paths/ai-business-school-government/) in its Artificial IntelligenceBusiness School this week that is specifically tailored for governmentdecision-makers.The fact is that government workers across the boardand especially decision-makersdonÈt necessarily have that familiarity or depth on AI, AnthonySalcito, MicrosoftÈs vice president for government, said in a[statement](https://blogs.microsoft.com/ai-for-business/2019/05/27/government-ai-school/?utm_source=ai-blog&utm_campaign=1735). This new learning path is away to get them introduced to the concept and to understand why itÈs importantin the context of government work.The tech giant launched its AI Business School in March to help executives andother business leaders better understand how to implement the technologythrough a free online master class. Now that more than 140,000 people havegained practical guidance from the AI course material to date, the company isoffering a new path that is designed especially for government agencies.Through various course modules, government insiders can learn about thecomponents that comprise a strong AI strategy, principles to guide responsibleAI adoption, lessons on fostering an AI-ready culture and other insightsaround using the tech to better serve their constituents.The course content also includes a case study demonstrating how a city inFinland has integrated AI to more efficiently serve citizens and a demo thatshowcases how the government can utilize intelligent bots to help constituentsaccess needed resources, among other materials and a video lecture.Microsoft is also expressly gearing the curriculum to governments of all sizesand Salcito said the new learning path is motivated by MicrosoftÈs keypriority, which is to help the government serve its constituents throughcloud services.We believe this course is valuable for government decision-makers at alllevels  from small municipalities to large cities, Salcito said. The beautyof artificial intelligence technologies is their scalability.
__label__cloud	This is my entire code on cloud function to convert the uploaded audio file from cloud storage bucket to text using speech api.... the code throws error saying that invalid arguments on calling speech api....
